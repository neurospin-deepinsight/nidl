<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="nidl.datasets.OpenBHB" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://neurospin-deepinsight.github.io/modules/generated/nidl.datasets.OpenBHB.html" />
<meta property="og:site_name" content="Nidl" />
<meta property="og:description" content="Examples using nidl.datasets.OpenBHB: Model probing callback of embedding estimators Presentation of the OpenBHB dataset Self-Supervised Learning with Barlow Twins Weakly Supervised Contrastive Lea..." />
<meta property="og:image" content="https://neurospin-deepinsight.github.io/_images/sphx_glr_plot_model_probing_thumb.png" />
<meta property="og:image:alt" content="" />
<meta name="description" content="Examples using nidl.datasets.OpenBHB: Model probing callback of embedding estimators Presentation of the OpenBHB dataset Self-Supervised Learning with Barlow Twins Weakly Supervised Contrastive Lea..." />
<link rel="search" title="Search" href="../../search.html"><link rel="next" title="nidl.callbacks: Available callbacks" href="../callbacks.html"><link rel="prev" title="nidl.datasets.ImageDataFrameDataset" href="nidl.datasets.ImageDataFrameDataset.html">
        <link rel="prefetch" href="../../_static/nidl-transparent.png" as="image">

    <link rel="shortcut icon" href="../../_static/favicon.ico"><!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>nidl.datasets.OpenBHB - Nidl</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=2da93098" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=749372d1" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/fontawesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/solid.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/brands.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: black;
  --admonition-font-size: 100%;
  --admonition-title-font-size: 100%;
  --color-announcement-background: #FBB360;
  --color-announcement-text: #111418;
  --color-admonition-title--note: #448aff;
  --color-admonition-title-background--note: #448aff10;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-announcement-background: #935610;
  --color-announcement-text: #FFFFFF;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-announcement-background: #935610;
  --color-announcement-text: #FFFFFF;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>

<div class="announcement">
  <aside class="announcement-content">
     <p>This is the development documentation of nidl (0.0.1)  
  </aside>
</div>

<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Nidl</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/nidl-transparent.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Nidl</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../auto_examples/index.html">Examples</a><input aria-label="Toggle navigation of Examples" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_model_probing.html">Model probing callback of embedding estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_openbhb.html">Presentation of the OpenBHB dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/simclr_stl10.html">Self-Supervised Contrastive Learning with SimCLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_barlowtwins_openbhb.html">Self-Supervised Learning with Barlow Twins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_yaware_openbhb.html">Weakly Supervised Contrastive Learning with y-Aware</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../user_guide.html">User guide</a><input aria-label="Toggle navigation of User guide" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html#what-is-nidl">2. What is <code class="docutils literal notranslate"><span class="pre">nidl</span></code>?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html#using-nidl-for-the-first-time">3. Using <code class="docutils literal notranslate"><span class="pre">nidl</span></code> for the first time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html#applications-to-neuroimaging">4. Applications to Neuroimaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../supervised_learning/index.html">5. Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../self_supervised_learning/index.html">6. Self Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoencoders/index.html">7. Auto Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_probing.html">8. Model Probing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_augmentation/index.html">9. Data Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pretrained_models.html">10. Pretrained Models</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../architectures/index.html">11. Architectures</a><input aria-label="Toggle navigation of 11. Architectures" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/volume.html">11.1. Volume</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/surface.html">11.2. Surface</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../open_datasets.html">12. Open Datasets</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../index.html">API References</a><input aria-label="Toggle navigation of API References" checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../estimators.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.estimators</span></code>: Available estimators</a><input aria-label="Toggle navigation of nidl.estimators: Available estimators" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.BaseEstimator.html">nidl.estimators.BaseEstimator</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ClassifierMixin.html">nidl.estimators.ClassifierMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ClusterMixin.html">nidl.estimators.ClusterMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.RegressorMixin.html">nidl.estimators.RegressorMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.TransformerMixin.html">nidl.estimators.TransformerMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.SimCLR.html">nidl.estimators.ssl.SimCLR</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.YAwareContrastiveLearning.html">nidl.estimators.ssl.YAwareContrastiveLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.BarlowTwins.html">nidl.estimators.ssl.BarlowTwins</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.InfoNCE.html">nidl.losses.InfoNCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.YAwareInfoNCE.html">nidl.losses.YAwareInfoNCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.BarlowTwinsLoss.html">nidl.losses.BarlowTwinsLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.ProjectionHead.html">nidl.estimators.ssl.utils.ProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.SimCLRProjectionHead.html">nidl.estimators.ssl.utils.SimCLRProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.YAwareProjectionHead.html">nidl.estimators.ssl.utils.YAwareProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.BarlowTwinsProjectionHead.html">nidl.estimators.ssl.utils.BarlowTwinsProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.autoencoders.VAE.html">nidl.estimators.autoencoders.VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.BetaVAELoss.html">nidl.losses.BetaVAELoss</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../architectures.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.volume.backbones</span></code>: Available backbones</a><input aria-label="Toggle navigation of nidl.volume.backbones: Available backbones" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.utils.Weights.html">nidl.utils.Weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.AlexNet.html">nidl.volume.backbones.AlexNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.DenseNet.html">nidl.volume.backbones.DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.ResNet.html">nidl.volume.backbones.ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.ResNetTruncated.html">nidl.volume.backbones.ResNetTruncated</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.densenet121.html">nidl.volume.backbones.densenet121</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.resnet18_trunc.html">nidl.volume.backbones.resnet18_trunc</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.resnet50.html">nidl.volume.backbones.resnet50</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.resnet50_trunc.html">nidl.volume.backbones.resnet50_trunc</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../augmentation.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.transforms</span></code>: Available augmentations</a><input aria-label="Toggle navigation of nidl.transforms: Available augmentations" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.Transform.html">nidl.transforms.Transform</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.Identity.html">nidl.transforms.Identity</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.MultiViewsTransform.html">nidl.transforms.MultiViewsTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.VolumeTransform.html">nidl.transforms.VolumeTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomGaussianBlur.html">nidl.volume.transforms.augmentation.RandomGaussianBlur</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomGaussianNoise.html">nidl.volume.transforms.augmentation.RandomGaussianNoise</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomErasing.html">nidl.volume.transforms.augmentation.RandomErasing</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomFlip.html">nidl.volume.transforms.augmentation.RandomFlip</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomResizedCrop.html">nidl.volume.transforms.augmentation.RandomResizedCrop</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomRotation.html">nidl.volume.transforms.augmentation.RandomRotation</a></li>
</ul>
</li>
<li class="toctree-l2 current has-children"><a class="reference internal" href="../datasets.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.datasets</span></code>: Available datasets</a><input aria-label="Toggle navigation of nidl.datasets: Available datasets" checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="nidl.datasets.BaseImageDataset.html">nidl.datasets.BaseImageDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.datasets.BaseNumpyDataset.html">nidl.datasets.BaseNumpyDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.datasets.ImageDataFrameDataset.html">nidl.datasets.ImageDataFrameDataset</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">nidl.datasets.OpenBHB</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../callbacks.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.callbacks</span></code>: Available callbacks</a><input aria-label="Toggle navigation of nidl.callbacks: Available callbacks" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.BatchTypingCallback.html">nidl.callbacks.BatchTypingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.BatchTypingCallback.html">nidl.callbacks.BatchTypingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.ClassificationProbingCallback.html">nidl.callbacks.ClassificationProbingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.RegressionProbingCallback.html">nidl.callbacks.RegressionProbingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.MultitaskModelProbing.html">nidl.callbacks.MultitaskModelProbing</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.ModelProbing.html">nidl.callbacks.ModelProbing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../development.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ci.html">Continuous integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../maintenance.html">Maintenance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../whats_new.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../authors.html">Team</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../versions.html">Versions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl">GitHub Repository</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/neurospin-deepinsight/nidl/blob/main/doc/modules/generated/nidl.datasets.OpenBHB.rst?plain=true" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/neurospin-deepinsight/nidl/edit/main/doc/modules/generated/nidl.datasets.OpenBHB.rst" rel="edit" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="admonition note">
<p class="admonition-title">Note</p>
<p>This page is a reference documentation. It only explains the class
signature, and not how to use it. Please refer to the
<a class="reference internal" href="../../user_guide.html#user-guide"><span class="std std-ref">user guide</span></a> for the big picture.</p>
</div>
<section id="nidl-datasets-openbhb">
<h1>nidl.datasets.OpenBHB<a class="headerlink" href="#nidl-datasets-openbhb" title="Link to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nidl.datasets.</span></span><span class="sig-name descname"><span class="pre">OpenBHB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modality</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'vbm'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'age'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streaming</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transforms</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_transforms</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L10"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.Dataset" title="(in PyTorch vmain (2.10.0a0+git4c963a6 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a></p>
<p>OpenBHB dataset <a class="reference internal" href="#r32a5d1ff9dee-1" id="id1">[1]</a>.</p>
<p>The Open Big Healthy Brains (OpenBHB) dataset is a large multi-site brain
MRI dataset consisting of 3227 training samples and 757 validation samples.
It aggregates T1-weighted (T1w) MRI scans from 10 public datasets:</p>
<ul class="simple">
<li><p>IXI</p></li>
<li><p>ABIDE I</p></li>
<li><p>ABIDE II</p></li>
<li><p>CoRR</p></li>
<li><p>GSP</p></li>
<li><p>Localizer</p></li>
<li><p>MPI-Leipzig</p></li>
<li><p>NAR</p></li>
<li><p>NPC</p></li>
<li><p>RBP</p></li>
</ul>
<p>These scans were acquired across 93 centers worldwide (North America,
Europe, and China). Only healthy controls aged between 6 and 88 years
are included, with balanced representation of males and females.</p>
<p>All T1w MRI scans have been uniformly preprocessed using CAT12 (SPM),
FreeSurfer, and Quasi-Raw (in-house minimal preprocessing). Both
Voxel-Based Morphometry (VBM) and Surface-Based Morphometry (SBM) features
are available.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The entire OpenBHB takes ~350GB of disk. We recommend enabling
<cite>streaming=True</cite> if you intend to use only a small portion of the
dataset.</p>
</div>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>root</strong><span class="classifier">str</span></dt><dd><p>Path to the root data directory where the dataset is stored.</p>
</dd>
<dt><strong>modality</strong><span class="classifier">str or tuple of str</span></dt><dd><p>Which modality to load for each brain image. If a tuple (multimodal
OpenBHB), a dictionary is returned from <cite>__getitem__</cite> with modality
names as keys and corresponding NumPy arrays as values.</p>
<p>Available modalities:</p>
<ul class="simple">
<li><p>“vbm”: Whole-brain voxel-based morphometry 3D T1w image,
shape <cite>(121, 145, 121)</cite></p></li>
<li><p>“quasiraw”: Whole-brain T1w image with minimal preprocessing,
shape <cite>(182, 218, 182)</cite></p></li>
<li><p>“vbm_roi”: Gray matter volume per region (Neuromorphometrics atlas,
142 regions by hemisphere), shape <cite>(1, 284)</cite></p></li>
<li><p>“fs_desikan_roi”: FreeSurfer surface-based features computed on the
Desikan atlas (34 regions by hemisphere), shape <cite>(7, 68)</cite></p></li>
<li><p>“fs_destrieux_roi”: FreeSurfer surface-based features computed on the
Destrieux atlas (74 regions by hemisphere), shape <cite>(7, 148)</cite></p></li>
<li><p>“fs_xhemi”: FreeSurfer surface-based features (curvature, sulcal
depth, cortical thickness) computed on the <cite>fsaverage7</cite> mesh
(163842 vertices by hemisphere), shape <cite>(8, 163842)</cite></p></li>
</ul>
</dd>
<dt><strong>target</strong><span class="classifier">{‘age’, ‘sex’, ‘site’}, list of str, or None</span></dt><dd><p>Target(s) to return with each image. If string, returns the target as
float (for ‘age’), int (for ‘site’) or string (for ‘sex’). If <cite>target</cite>
is a list of strings, returns multiple targets as dictionary:
{&lt;target&gt;: &lt;value&gt;}. If None, no target is returned.</p>
</dd>
<dt><strong>split</strong><span class="classifier">{‘train’, ‘val’, ‘internal_val’, ‘external_val’}</span></dt><dd><p>Dataset split to use. The ‘val’ split is the union of:</p>
<ul class="simple">
<li><p>‘internal_val’: Images acquired with the same MRI scanner as training
data (in-domain)</p></li>
<li><p>‘external_val’: Images acquired with different MRI scanners
(out-of-domain)</p></li>
</ul>
</dd>
<dt><strong>streaming</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, data are downloaded lazily from Hugging Face on demand (when
accessed via <cite>__getitem__</cite>). If False, the entire split is downloaded
at initialization for the requested modality.</p>
</dd>
<dt><strong>max_workers: int, default=1</strong></dt><dd><p>Number of concurrent threads to download files on the Hugging Face,
1 thread = 1 file download.
Warning: setting <cite>max_workers</cite> &gt; 1 can raise Hugging Face 429 errors
(too many requests). We recommend keeping this value low.</p>
</dd>
<dt><strong>transforms</strong><span class="classifier">callable or None, default=None</span></dt><dd><p>A function/transform that takes in a brain image and returns a
transformed version. Input depends on <cite>modality</cite> and can be a 3D image,
1D vector, or dict.</p>
</dd>
<dt><strong>target_transforms</strong><span class="classifier">callable or None, default=None</span></dt><dd><p>A function/transform applied to the target(s).</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The data are downloaded exclusively from the <a class="reference external" href="https://huggingface.co/datasets/benoit-dufumier/openBHB">OpenBHB repository</a> in the
HuggingFace either on-the-fly (lazy download) or during initialization
(immediate download) if there are not already there.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r32a5d1ff9dee-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Dufumier, B., Grigis, A., Victor, J., Ambroise, C., Frouin, V. &amp;
Duchesnay, E. (2022). OpenBHB: a Large-Scale Multi-Site Brain MRI
Data-set for Age Prediction and Debiasing.
NeuroImage, 254, 119121. <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2022.119637">https://doi.org/10.1016/j.neuroimage.2022.119637</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<p>Load the VBM modality from the training split and get the age target:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">OpenBHB</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">root</span><span class="o">=</span><span class="s1">&#39;data/openbhb&#39;</span><span class="p">,</span> <span class="n">modality</span><span class="o">=</span><span class="s1">&#39;vbm&#39;</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s1">&#39;age&#39;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span><span class="p">,</span> <span class="n">age</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1, 121, 145, 121)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">age</span><span class="p">)</span>
<span class="go">34.0</span>
</pre></div>
</div>
<p>Load multiple modalities and multiple targets:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">OpenBHB</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;data/openbhb&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">modality</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;vbm&#39;</span><span class="p">,</span> <span class="s1">&#39;quasiraw&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">target</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">,</span> <span class="s1">&#39;site&#39;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">split</span><span class="o">=</span><span class="s1">&#39;val&#39;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;vbm&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1, 121, 145, 121)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;quasiraw&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1, 182, 218, 182)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="go">{&#39;age&#39;: 19.0, &#39;sex&#39;: &#39;female&#39;, &#39;site&#39;: 0}</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.REPO_ID">
<span class="sig-name descname"><span class="pre">REPO_ID</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'benoit-dufumier/openBHB'</span></em><a class="headerlink" href="#nidl.datasets.OpenBHB.REPO_ID" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.REVISION">
<span class="sig-name descname"><span class="pre">REVISION</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'8508cda68fea74f217926acbf46ee5863f8879d1'</span></em><a class="headerlink" href="#nidl.datasets.OpenBHB.REVISION" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modality</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'vbm'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'age'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">streaming</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transforms</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_transforms</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L154"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.__init__" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.download_dataset_split">
<span class="sig-name descname"><span class="pre">download_dataset_split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modality</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">incremental</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L203"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.download_dataset_split" title="Link to this definition">¶</a></dt>
<dd><p>Fetch a split of the dataset from Hugging Face if not present.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>split: {‘train’, ‘val’, ‘internal_val’, ‘external_val’}</strong></dt><dd><p>Split to download if not present.</p>
</dd>
<dt><strong>modality</strong><span class="classifier">tuple of str</span></dt><dd><p>Modalities to download (“vbm”, “vbm_roi”, “quasiraw”, “fs_xhemi”,
“fs_desikan_roi” or “fs_destrieux_roi”)</p>
</dd>
<dt><strong>samples: list of dict</strong></dt><dd><p>List of paths to the data in the current split. This should have
been generated by <cite>make_dataset</cite>.</p>
</dd>
<dt><strong>incremental: bool, default=True</strong></dt><dd><p>If True, only missing files in the data split are downloaded.
Otherwise, all data in the split are downloaded and local data
are eventually replaced.</p>
</dd>
<dt><strong>max_workers: int, default=8</strong></dt><dd><p>Number of concurrent threads to download files (1 thread = 1 file
download).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.download_file">
<span class="sig-name descname"><span class="pre">download_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L184"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.download_file" title="Link to this definition">¶</a></dt>
<dd><p>Download a single file from the OpenBHB repository on the HF.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_cat12_template">
<span class="sig-name descname"><span class="pre">get_cat12_template</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L463"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_cat12_template" title="Link to this definition">¶</a></dt>
<dd><p>Get the CAT12 gray matter tissue probability map as NIfTI image.</p>
<p>This method retrieves the CAT12 gray matter (GM) tissue probability map
(TPM) registered to MNI152 space.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>nii</strong><span class="classifier">nibabel.Nifti1Image, shape (121, 145, 121)</span></dt><dd><p>A 3D NIfTI image containing the CAT12 gray matter TPM.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference external" href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.load" title="(in NiBabel v5.4.0.dev1+g3b1c7b37)"><code class="xref py py-func docutils literal notranslate"><span class="pre">nibabel.nifti1.load</span></code></a></dt><dd><p>Function used to load the NIfTI image.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The template file is expected at:
<cite>&lt;root&gt;/resource/cat12vbm_space-MNI152_desc-gm_TPM.nii.gz</cite>. If the file
is not available locally, it will be downloaded from the Hugging Face.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_fs_labels">
<span class="sig-name descname"><span class="pre">get_fs_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atlas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'destrieux'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L528"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_fs_labels" title="Link to this definition">¶</a></dt>
<dd><p>Get region names on the given atlas where “fs_destrieux_roi” (for
“destrieux” atlas) or “fs_desikan_roi” (for “desikan” atlas) features
have been computed in OpenBHB.</p>
<p>The names are extracted from the resource file.</p>
<p>First 74 (resp. 38) regions are from the left hemisphere, last 74
(resp. 38) are from the right hemisphere for the Destrieux
(resp. Desikan).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>symmetric: bool</strong></dt><dd><p>If True, removes “lh-” and “rh-” from labels indicating right and
left hemisphere. Final length is divided by two.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>labels: list of string</dt><dd><p>List of region names on the given atlas.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The resource file is expected at: <cite>&lt;root&gt;/resource/resources.json</cite>. If
it is not present locally, it is automatically downloaded from the
Hugging Face.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_fs_roi_feature_names">
<span class="sig-name descname"><span class="pre">get_fs_roi_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L591"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_fs_roi_feature_names" title="Link to this definition">¶</a></dt>
<dd><p>Get the 7 feature names corresponding to “fs_destrieux_roi” and
“fs_desikan_roi” data.</p>
<p>The feature names are extracted from the resource file.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt>features: list of string</dt><dd><p>List of 7 feature names corresponding to “fs_destrieux_roi” and
“fs_desikan_roi” data in OpenBHB.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The resource file is expected at: <cite>&lt;root&gt;/resource/resources.json</cite>. If
it is not present locally, it is automatically downloaded from the
Hugging Face.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_fs_xhemi_feature_names">
<span class="sig-name descname"><span class="pre">get_fs_xhemi_feature_names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L613"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_fs_xhemi_feature_names" title="Link to this definition">¶</a></dt>
<dd><p>Get the 8 feature names corresponding to “fs_xhemi” data.</p>
<p>The feature names are extracted from the resource file. If it is not
present locally, it is automatically downloaded from the Hugging Face.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt>features: list of string</dt><dd><p>List of 8 feature names corresponding to “fs_xhemi”.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The resource file is expected at: <cite>&lt;root&gt;/resource/resources.json</cite></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_neuromorphometrics_atlas">
<span class="sig-name descname"><span class="pre">get_neuromorphometrics_atlas</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L424"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_neuromorphometrics_atlas" title="Link to this definition">¶</a></dt>
<dd><p>Get the Neuromorphometrics gray matter atlas and its region names.</p>
<p>This method loads the Neuromorphometrics gray matter atlas as a NIfTI
image, along with the associated region names (abbreviations).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt>dict</dt><dd><p>A dictionary containing:</p>
<ul class="simple">
<li><p><cite>data</cite> : <a class="reference external" href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image" title="(in NiBabel v5.4.0.dev1+g3b1c7b37)"><code class="xref py py-class docutils literal notranslate"><span class="pre">nibabel.nifti1.Nifti1Image</span></code></a>, the atlas image.</p></li>
<li><p><cite>labels</cite> : list of region names (string) corresponding to
integer labels in the atlas.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference external" href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.load" title="(in NiBabel v5.4.0.dev1+g3b1c7b37)"><code class="xref py py-func docutils literal notranslate"><span class="pre">nibabel.nifti1.load</span></code></a></dt><dd><p>Function used to load the NIfTI image.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>Expects the following files under the resource directory:</p>
<ul class="simple">
<li><p><cite>&lt;root&gt;/resource/neuromorphometrics.nii</cite> : NIfTI atlas file</p></li>
<li><p><cite>&lt;root&gt;/resource/neuromorphometrics.csv</cite> : CSV with region names.</p></li>
</ul>
<p>If the files are not found locally, they will be downloaded from the
Hugging Face.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_or_download_file">
<span class="sig-name descname"><span class="pre">get_or_download_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L289"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_or_download_file" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_participants">
<span class="sig-name descname"><span class="pre">get_participants</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L295"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_participants" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_quasiraw_template">
<span class="sig-name descname"><span class="pre">get_quasiraw_template</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L495"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_quasiraw_template" title="Link to this definition">¶</a></dt>
<dd><p>Get the quasi-raw MNI152 brain template as a NIfTI image.</p>
<p>This method retrieves the quasi-raw T1-weighted brain template in
MNI152 space.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt>nii: nibabel.Nifti1Image, shape (182, 218, 182)</dt><dd><p>A 3D NIfTI image containing the quasi-raw MNI152 brain template.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference external" href="https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.load" title="(in NiBabel v5.4.0.dev1+g3b1c7b37)"><code class="xref py py-func docutils literal notranslate"><span class="pre">nibabel.nifti1.load</span></code></a></dt><dd><p>Function used to load the NIfTI image.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The template file is expected at:
<cite>&lt;root&gt;/resource/quasiraw_space-MNI152_desc-brain_T1w.nii.gz</cite>. If the
file is not present locally, it is automatically downloaded from the
Hugging Face.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_resource">
<span class="sig-name descname"><span class="pre">get_resource</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L299"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_resource" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.get_vbm_roi_labels">
<span class="sig-name descname"><span class="pre">get_vbm_roi_labels</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L569"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.get_vbm_roi_labels" title="Link to this definition">¶</a></dt>
<dd><p>Get region names on the Neuromorphometrics atlas where “vbm_roi”
features are computed in OpenBHB.</p>
<p>The names are extracted from the resource file. If it is not present
locally, it is automatically downloaded from the Hugging Face.</p>
<p>First 142 features are GM volumes, last 142 are CSF volumes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt>labels: list of string</dt><dd><p>List of region names on the Neuromorphometrics atlas where
“vbm_roi” features have been computed.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The resource file is expected at: <cite>&lt;root&gt;/resource/resources.json</cite></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.datasets.OpenBHB.make_dataset">
<span class="sig-name descname"><span class="pre">make_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">split</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl/blob/12d038d/nidl/datasets/openbhb.py#L303"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.datasets.OpenBHB.make_dataset" title="Link to this definition">¶</a></dt>
<dd><p>Generate a list of sample file paths and their corresponding targets
for a given dataset split.</p>
<p>This method constructs file paths for each participant listed in
<cite>participants.tsv</cite> according to the specified <cite>split</cite>. It supports
both unimodal and multimodal configurations, depending on the
<cite>modality</cite> attribute.</p>
<p>Each returned sample is a tuple of the form:</p>
<ul class="simple">
<li><p>({id: path}, target): if <cite>modality</cite> is a single string</p></li>
<li><p>({id: tuple of path}, target): if <cite>modality</cite> is a tuple of strings</p></li>
</ul>
<p>If <cite>target</cite> is None, the sample tuple excludes the target and only
contains the path or tuple of paths.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>split: {‘train’, ‘val’, ‘internal_val’, ‘external_val’}</strong></dt><dd><p>Which participants to include in the dataset.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>samples: list of tuple</dt><dd><p>List of samples in the form ({id: path}, target), where:</p>
<ul class="simple">
<li><p><cite>id</cite> is the participant’s ID.</p></li>
<li><p><cite>path</cite> is either a single file path (str) or a tuple of file
paths (if multiple modalities are used).</p></li>
<li><p><cite>target</cite> is the associated value from the participants metadata
(e.g., age, sex, site), or None if <cite>target</cite> is not set.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Expects the following file under the root directory:
<cite>&lt;root&gt;/participants.tsv</cite>. If not present, it is downloaded
automatically from the Hugging Face.</p>
</dd></dl>

</dd></dl>

<section id="examples-using-nidl-datasets-openbhb">
<h2>Examples using <code class="docutils literal notranslate"><span class="pre">nidl.datasets.OpenBHB</span></code><a class="headerlink" href="#examples-using-nidl-datasets-openbhb" title="Link to this heading">¶</a></h2>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This notebook will show you how to investigate the data representation given by an embedding estimator during training  (such as SimCLR, y-Aware Contrastive Learning or Barlow Twins) using the notion of &quot;probing&quot;. A standard machine learning model (e.g. linear or SVM) is trained and evaluated on the data embedding for a given task as the model is being fitted. It allows the user to understand what concepts are learned by the model."><img alt="" src="../../_images/sphx_glr_plot_model_probing_thumb.png" />
<p><a class="reference internal" href="../../auto_examples/plot_model_probing.html#sphx-glr-auto-examples-plot-model-probing-py"><span class="std std-ref">Model probing callback of embedding estimators</span></a></p>
  <div class="sphx-glr-thumbnail-title">Model probing callback of embedding estimators</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This notebook introduces the OpenBHB dataset [1]_, a large-scale, multi-site brain MRI dataset. It is designed to perform benchmarking of machine learning and deep learning models on neuroimaging data."><img alt="" src="../../_images/sphx_glr_plot_openbhb_thumb.png" />
<p><a class="reference internal" href="../../auto_examples/plot_openbhb.html#sphx-glr-auto-examples-plot-openbhb-py"><span class="std std-ref">Presentation of the OpenBHB dataset</span></a></p>
  <div class="sphx-glr-thumbnail-title">Presentation of the OpenBHB dataset</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This tutorial will show you how to fit and evaluate a Barlow Twins model [1]_ on the OpenBHB dataset using NIDL."><img alt="" src="../../_images/sphx_glr_plot_barlowtwins_openbhb_thumb.png" />
<p><a class="reference internal" href="../../auto_examples/plot_barlowtwins_openbhb.html#sphx-glr-auto-examples-plot-barlowtwins-openbhb-py"><span class="std std-ref">Self-Supervised Learning with Barlow Twins</span></a></p>
  <div class="sphx-glr-thumbnail-title">Self-Supervised Learning with Barlow Twins</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This tutorial will show you how to fit and evaluate a y-Aware Contrastive Learning model on the OpenBHB dataset using NIDL. As in the original paper [1]_, we will use age as a weak label to guide the contrastive learning process. The model will be trained to bring representations of samples with similar ages closer together in the feature space, while pushing apart samples with dissimilar ages. This approach leverages the age information to enhance the quality of the learned representations, making them more relevant for downstream tasks such as age prediction or disease classification."><img alt="" src="../../_images/sphx_glr_plot_yaware_openbhb_thumb.png" />
<p><a class="reference internal" href="../../auto_examples/plot_yaware_openbhb.html#sphx-glr-auto-examples-plot-yaware-openbhb-py"><span class="std std-ref">Weakly Supervised Contrastive Learning with y-Aware</span></a></p>
  <div class="sphx-glr-thumbnail-title">Weakly Supervised Contrastive Learning with y-Aware</div>
</div></div><div style='clear:both'></div></section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../callbacks.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.callbacks</span></code>: Available callbacks</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="nidl.datasets.ImageDataFrameDataset.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">nidl.datasets.ImageDataFrameDataset</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; The nidl developers
- Code and documentation distributed under CeCILL-B license.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link fa-brands fa-solid fa-github fa-2x" href="https://github.com/neurospin-deepinsight/nidl" aria-label="GitHub"></a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">nidl.datasets.OpenBHB</a><ul>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB"><code class="docutils literal notranslate"><span class="pre">OpenBHB</span></code></a><ul>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.REPO_ID"><code class="docutils literal notranslate"><span class="pre">OpenBHB.REPO_ID</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.REVISION"><code class="docutils literal notranslate"><span class="pre">OpenBHB.REVISION</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.__init__"><code class="docutils literal notranslate"><span class="pre">OpenBHB.__init__</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.download_dataset_split"><code class="docutils literal notranslate"><span class="pre">OpenBHB.download_dataset_split</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.download_file"><code class="docutils literal notranslate"><span class="pre">OpenBHB.download_file</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_cat12_template"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_cat12_template</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_fs_labels"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_fs_labels</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_fs_roi_feature_names"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_fs_roi_feature_names</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_fs_xhemi_feature_names"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_fs_xhemi_feature_names</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_neuromorphometrics_atlas"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_neuromorphometrics_atlas</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_or_download_file"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_or_download_file</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_participants"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_participants</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_quasiraw_template"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_quasiraw_template</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_resource"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_resource</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.get_vbm_roi_labels"><code class="docutils literal notranslate"><span class="pre">OpenBHB.get_vbm_roi_labels</span></code></a></li>
<li><a class="reference internal" href="#nidl.datasets.OpenBHB.make_dataset"><code class="docutils literal notranslate"><span class="pre">OpenBHB.make_dataset</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#examples-using-nidl-datasets-openbhb">Examples using <code class="docutils literal notranslate"><span class="pre">nidl.datasets.OpenBHB</span></code></a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=4ea706d9"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    </body>
</html>