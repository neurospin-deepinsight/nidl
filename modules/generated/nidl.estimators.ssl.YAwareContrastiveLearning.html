<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="nidl.estimators.ssl.YAwareContrastiveLearning" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://neurospin-deepinsight.github.io/modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.html" />
<meta property="og:site_name" content="Nidl" />
<meta property="og:description" content="Examples using nidl.estimators.ssl.YAwareContrastiveLearning: Model probing callback of embedding estimators Weakly Supervised Contrastive Learning with y-Aware" />
<meta property="og:image" content="https://neurospin-deepinsight.github.io/_images/sphx_glr_plot_model_probing_thumb.png" />
<meta property="og:image:alt" content="" />
<meta name="description" content="Examples using nidl.estimators.ssl.YAwareContrastiveLearning: Model probing callback of embedding estimators Weakly Supervised Contrastive Learning with y-Aware" />
<link rel="search" title="Search" href="../../search.html"><link rel="next" title="nidl.estimators.ssl.BarlowTwins" href="nidl.estimators.ssl.BarlowTwins.html"><link rel="prev" title="nidl.estimators.ssl.SimCLR" href="nidl.estimators.ssl.SimCLR.html">
        <link rel="prefetch" href="../../_static/nidl-transparent.png" as="image">

    <link rel="shortcut icon" href="../../_static/favicon.ico"><!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>nidl.estimators.ssl.YAwareContrastiveLearning - Nidl</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=2da93098" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=749372d1" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/fontawesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/solid.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/brands.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: black;
  --admonition-font-size: 100%;
  --admonition-title-font-size: 100%;
  --color-announcement-background: #FBB360;
  --color-announcement-text: #111418;
  --color-admonition-title--note: #448aff;
  --color-admonition-title-background--note: #448aff10;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-announcement-background: #935610;
  --color-announcement-text: #FFFFFF;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-announcement-background: #935610;
  --color-announcement-text: #FFFFFF;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>

<div class="announcement">
  <aside class="announcement-content">
     <p>This is the development documentation of nidl (0.0.1.dev)  
  </aside>
</div>

<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Nidl</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/nidl-transparent.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Nidl</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../auto_examples/index.html">Examples</a><input aria-label="Toggle navigation of Examples" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_model_probing.html">Model probing callback of embedding estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_openbhb.html">Presentation of the OpenBHB dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/simclr_stl10.html">Self-Supervised Contrastive Learning with SimCLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_barlowtwins_openbhb.html">Self-Supervised Learning with Barlow Twins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../auto_examples/plot_yaware_openbhb.html">Weakly Supervised Contrastive Learning with y-Aware</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../user_guide.html">User guide</a><input aria-label="Toggle navigation of User guide" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html#what-is-nidl">2. What is <code class="docutils literal notranslate"><span class="pre">nidl</span></code>?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html#using-nidl-for-the-first-time">3. Using <code class="docutils literal notranslate"><span class="pre">nidl</span></code> for the first time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../introduction.html#applications-to-neuroimaging">4. Applications to Neuroimaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../supervised_learning/index.html">5. Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../self_supervised_learning/index.html">6. Self Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoencoders/index.html">7. Auto Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../model_probing.html">8. Model Probing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_augmentation/index.html">9. Data Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pretrained_models.html">10. Pretrained Models</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../architectures/index.html">11. Architectures</a><input aria-label="Toggle navigation of 11. Architectures" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/volume.html">11.1. Volume</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/surface.html">11.2. Surface</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../open_datasets.html">12. Open Datasets</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="../index.html">API References</a><input aria-label="Toggle navigation of API References" checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2 current has-children"><a class="reference internal" href="../estimators.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.estimators</span></code>: Available estimators</a><input aria-label="Toggle navigation of nidl.estimators: Available estimators" checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.BaseEstimator.html">nidl.estimators.BaseEstimator</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ClassifierMixin.html">nidl.estimators.ClassifierMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ClusterMixin.html">nidl.estimators.ClusterMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.RegressorMixin.html">nidl.estimators.RegressorMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.TransformerMixin.html">nidl.estimators.TransformerMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.SimCLR.html">nidl.estimators.ssl.SimCLR</a></li>
<li class="toctree-l3 current current-page"><a class="current reference internal" href="#">nidl.estimators.ssl.YAwareContrastiveLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.BarlowTwins.html">nidl.estimators.ssl.BarlowTwins</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.InfoNCE.html">nidl.losses.InfoNCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.YAwareInfoNCE.html">nidl.losses.YAwareInfoNCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.BarlowTwinsLoss.html">nidl.losses.BarlowTwinsLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.ProjectionHead.html">nidl.estimators.ssl.utils.ProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.SimCLRProjectionHead.html">nidl.estimators.ssl.utils.SimCLRProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.YAwareProjectionHead.html">nidl.estimators.ssl.utils.YAwareProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.ssl.utils.BarlowTwinsProjectionHead.html">nidl.estimators.ssl.utils.BarlowTwinsProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.estimators.autoencoders.VAE.html">nidl.estimators.autoencoders.VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.losses.BetaVAELoss.html">nidl.losses.BetaVAELoss</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../architectures.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.volume.backbones</span></code>: Available backbones</a><input aria-label="Toggle navigation of nidl.volume.backbones: Available backbones" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.utils.Weights.html">nidl.utils.Weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.AlexNet.html">nidl.volume.backbones.AlexNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.DenseNet.html">nidl.volume.backbones.DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.ResNet.html">nidl.volume.backbones.ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.ResNetTruncated.html">nidl.volume.backbones.ResNetTruncated</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.densenet121.html">nidl.volume.backbones.densenet121</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.resnet18_trunc.html">nidl.volume.backbones.resnet18_trunc</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.resnet50.html">nidl.volume.backbones.resnet50</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.backbones.resnet50_trunc.html">nidl.volume.backbones.resnet50_trunc</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../augmentation.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.transforms</span></code>: Available augmentations</a><input aria-label="Toggle navigation of nidl.transforms: Available augmentations" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.Transform.html">nidl.transforms.Transform</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.Identity.html">nidl.transforms.Identity</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.MultiViewsTransform.html">nidl.transforms.MultiViewsTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.transforms.VolumeTransform.html">nidl.transforms.VolumeTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomGaussianBlur.html">nidl.volume.transforms.augmentation.RandomGaussianBlur</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomGaussianNoise.html">nidl.volume.transforms.augmentation.RandomGaussianNoise</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomErasing.html">nidl.volume.transforms.augmentation.RandomErasing</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomFlip.html">nidl.volume.transforms.augmentation.RandomFlip</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomResizedCrop.html">nidl.volume.transforms.augmentation.RandomResizedCrop</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.volume.transforms.augmentation.RandomRotation.html">nidl.volume.transforms.augmentation.RandomRotation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../datasets.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.datasets</span></code>: Available datasets</a><input aria-label="Toggle navigation of nidl.datasets: Available datasets" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.datasets.BaseImageDataset.html">nidl.datasets.BaseImageDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.datasets.BaseNumpyDataset.html">nidl.datasets.BaseNumpyDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.datasets.ImageDataFrameDataset.html">nidl.datasets.ImageDataFrameDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.datasets.OpenBHB.html">nidl.datasets.OpenBHB</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../callbacks.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.callbacks</span></code>: Available callbacks</a><input aria-label="Toggle navigation of nidl.callbacks: Available callbacks" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.BatchTypingCallback.html">nidl.callbacks.BatchTypingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.BatchTypingCallback.html">nidl.callbacks.BatchTypingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.ClassificationProbingCallback.html">nidl.callbacks.ClassificationProbingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.RegressionProbingCallback.html">nidl.callbacks.RegressionProbingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.MultitaskModelProbing.html">nidl.callbacks.MultitaskModelProbing</a></li>
<li class="toctree-l3"><a class="reference internal" href="nidl.callbacks.ModelProbing.html">nidl.callbacks.ModelProbing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../development.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ci.html">Continuous integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../maintenance.html">Maintenance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../whats_new.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../authors.html">Team</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../versions.html">Versions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl">GitHub Repository</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/neurospin-deepinsight/nidl/blob/main/doc/modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.rst?plain=true" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/neurospin-deepinsight/nidl/edit/main/doc/modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.rst" rel="edit" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="admonition note">
<p class="admonition-title">Note</p>
<p>This page is a reference documentation. It only explains the class
signature, and not how to use it. Please refer to the
<a class="reference internal" href="../../user_guide.html#user-guide"><span class="std std-ref">user guide</span></a> for the big picture.</p>
</div>
<section id="nidl-estimators-ssl-yawarecontrastivelearning">
<h1>nidl.estimators.ssl.YAwareContrastiveLearning<a class="headerlink" href="#nidl-estimators-ssl-yawarecontrastivelearning" title="Link to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nidl.estimators.ssl.</span></span><span class="sig-name descname"><span class="pre">YAwareContrastiveLearning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">projection_head=&lt;class</span> <span class="pre">'nidl.estimators.ssl.utils.projection_heads.YAwareProjectionHead'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">projection_head_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel='gaussian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bandwidth=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer='adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate=0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L24"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nidl.estimators.TransformerMixin.html#nidl.estimators.TransformerMixin" title="nidl.estimators.base.TransformerMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerMixin</span></code></a>, <a class="reference internal" href="nidl.estimators.BaseEstimator.html#nidl.estimators.BaseEstimator" title="nidl.estimators.base.BaseEstimator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEstimator</span></code></a></p>
<p>y-Aware Contrastive Learning <a class="reference internal" href="#r8ad1d13554f2-1" id="id1">[1]</a>.</p>
<p>y-Aware Contrastive Learning is a self-supervised learning framework for
learning visual representations with auxiliary variables. It leverages
contrastive learning by maximizing the agreement between differently
augmented views of images with similar auxiliary variables while minimizing
agreement between different images. The framework consists of:</p>
<ol class="arabic simple">
<li><p>Data Augmentation - Generates two augmented views of an image.</p></li>
<li><p>Kernel - Similarity function between auxiliary variables.</p></li>
<li><p>Encoder (Backbone Network) - Maps images to feature embeddings
(e.g., 3D-ResNet).</p></li>
<li><p>Projection Head - Maps features to a latent space for contrastive
loss optimization.</p></li>
<li><p>Contrastive Loss (y-Aware) - Encourages augmented views of
i) the same image and ii) images with close auxiliary variables
to be closer while pushing dissimilar ones apart.</p></li>
</ol>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>encoder: nn.Module or class</strong></dt><dd><p>Which deep architecture to use for encoding the input.
A PyTorch <cite>torch.nn.Module</cite> is expected.
In general, the uninstantiated class should be passed, although
instantiated modules will also work.</p>
</dd>
<dt><strong>encoder_kwargs: dict or None, default=None</strong></dt><dd><p>Options for building the encoder (depends on each architecture).
Examples:</p>
<ul class="simple">
<li><p>encoder=torchvision.ops.MLP, encoder_kwargs={“in_channels”: 10,
“hidden_channels”: [4, 3, 2]} builds an MLP with 3 hidden layers,
input dim 10, output dim 2.</p></li>
<li><p>encoder=nidl.volume.backbones.resnet3d.resnet18,
encoder_kwargs={“n_embedding”: 10} builds a ResNet-18 model with
10 output dimension.</p></li>
</ul>
<p>Ignored if <cite>encoder</cite> is instantiated.</p>
</dd>
<dt><strong>projection_head: nn.Module or class or None, default=YAwareProjectionHead</strong></dt><dd><p>Which projection head to use for the model. If None, no projection head
is used and the encoder output is directly used for loss computation.
Otherwise, a <cite>~torch.nn.Module</cite> is expected. In general,
the uninstantiated class should be passed, although instantiated
modules will also work. By default, a 2-layer MLP with ReLU activation,
2048-d hidden units, and 128-d output dimensions is used.</p>
</dd>
<dt><strong>projection_head_kwargs: dict or None, default=None</strong></dt><dd><p>Arguments for building the projection head. By default, input dimension
is 2048-d and output dimension is 128-d. These can be changed by
passing a dictionary with keys ‘input_dim’ and ‘output_dim’.
‘input_dim’ must be equal to the encoder’s output dimension.
Ignored if <cite>projection_head</cite> is instantiated.</p>
</dd>
<dt><strong>temperature: float, default=0.1</strong></dt><dd><p>Temperature value in y-Aware InfoNCE loss. Small values imply more
uniformity between samples’ embeddings, whereas high values impose
clustered embedding more sensitive to augmentations.</p>
</dd>
<dt><strong>kernel: {‘gaussian’, ‘epanechnikov’, ‘exponential’, ‘linear’, ‘cosine’},         default=”gaussian”</strong></dt><dd><p>Kernel used as a similarity function between auxiliary variables.</p>
</dd>
<dt><strong>bandwidth</strong><span class="classifier">Union[float, List[float], array, KernelMetric],         default=1.0</span></dt><dd><p>The method used to calculate the bandwidth (“sigma” in [1]) between
auxiliary variables:</p>
<ul class="simple">
<li><p>If <cite>bandwidth</cite> is a scalar, it sets the bandwidth to a diagnonal
matrix with equal values.</p></li>
<li><p>If <cite>bandwidth</cite> is a 1d array, it sets the bandwidth to a
diagonal matrix and it must be of size equal to the number of
features in <cite>y</cite>.</p></li>
<li><p>If <cite>bandwidth</cite> is a 2d array, it must be of shape
<cite>(n_features, n_features)</cite> where <cite>n_features</cite> is the number of
features in <cite>y</cite>.</p></li>
<li><p>If <cite>bandwidth</cite> is <cite>KernelMetric</cite>, it uses the <cite>pairwise</cite>
method to compute the similarity matrix between auxiliary variables.</p></li>
</ul>
</dd>
<dt><strong>optimizer: {‘sgd’, ‘adam’, ‘adamW’} or torch.optim.Optimizer or type,         default=”adam”</strong></dt><dd><p>Optimizer for training the model. Can be:</p>
<ul>
<li><p>A string:</p>
<blockquote>
<div><ul class="simple">
<li><p>‘sgd’: Stochastic Gradient Descent (with optional momentum).</p></li>
<li><p>‘adam’: First-order gradient-based optimizer (default).</p></li>
<li><p>‘adamW’: Adam with decoupled weight decay regularization
(see “Decoupled Weight Decay Regularization”, Loshchilov and
Hutter, ICLR 2019).</p></li>
</ul>
</div></blockquote>
</li>
<li><p>An instance or subclass of <code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code>.</p></li>
</ul>
</dd>
<dt><strong>optimizer_kwargs: dict or None, default=None</strong></dt><dd><p>Arguments for the optimizer (‘adam’ by default). By default:
{‘betas’: (0.9, 0.99), ‘weight_decay’: 5e-05} where ‘betas’ are the
exponential decay rates for first and second moment estimates.</p>
<p>Ignored if <cite>optimizer</cite> is instantiated.</p>
</dd>
<dt><strong>learning_rate: float, default=1e-4</strong></dt><dd><p>Initial learning rate.</p>
</dd>
<dt><strong>lr_scheduler: LRSchedulerPLType or class or None, default=None</strong></dt><dd><p>Learning rate scheduler to use.</p>
</dd>
<dt><strong>lr_scheduler_kwargs: dict or None, default=None</strong></dt><dd><p>Additional keyword arguments for the scheduler.</p>
<p>Ignored if <cite>lr_scheduler</cite> is instantiated.</p>
</dd>
<dt><strong>**kwargs: dict, optional</strong></dt><dd><p>Additional keyword arguments for the BaseEstimator class, such as
<cite>max_epochs</cite>, <cite>max_steps</cite>, <cite>num_sanity_val_steps</cite>,
<cite>check_val_every_n_epoch</cite>, <cite>callbacks</cite>, etc.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>encoder: torch.nn.Module</strong></dt><dd><p>Deep neural network mapping input data to low-dimensional vectors.</p>
</dd>
<dt><strong>projection_head: torch.nn.Module</strong></dt><dd><p>Maps encoder output to latent space for contrastive loss optimization.</p>
</dd>
<dt><strong>loss: yAwareInfoNCE</strong></dt><dd><p>The yAwareInfoNCE loss function used for training.</p>
</dd>
<dt><strong>optimizer: torch.optim.Optimizer</strong></dt><dd><p>Optimizer used for training.</p>
</dd>
<dt><strong>lr_scheduler: LRSchedulerPLType or None</strong></dt><dd><p>Learning rate scheduler used for training.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r8ad1d13554f2-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Dufumier, B., et al., “Contrastive learning with continuous proxy
meta-data for 3D MRI classification.” MICCAI, 2021.
<a class="reference external" href="https://arxiv.org/abs/2106.08808">https://arxiv.org/abs/2106.08808</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">projection_head=&lt;class</span> <span class="pre">'nidl.estimators.ssl.utils.projection_heads.YAwareProjectionHead'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">projection_head_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel='gaussian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bandwidth=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer='adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate=0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L150"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning.__init__" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning.all_gather_and_flatten">
<span class="sig-name descname"><span class="pre">all_gather_and_flatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L424"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning.all_gather_and_flatten" title="Link to this definition">¶</a></dt>
<dd><p>Gathers the tensor from all devices and flattens batch dimension.</p>
<p>This is useful when gathering tensors without adding extra dimensions.
It handles some edge cases, such as when using a single GPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>tensor: torch.Tensor or None</strong></dt><dd><p>The tensor to gather. If None, it is returned as is.</p>
</dd>
<dt><strong>**kwargs: dict</strong></dt><dd><p>Additional keyword arguments for <cite>self.all_gather</cite>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>tensor: torch.Tensor</dt><dd><p>The gathered and flattened tensor.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L366"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning.configure_optimizers" title="Link to this definition">¶</a></dt>
<dd><p>Instantiate the required optimizer and setup the scheduler.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning.parse_batch">
<span class="sig-name descname"><span class="pre">parse_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L319"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning.parse_batch" title="Link to this definition">¶</a></dt>
<dd><p>Parses the batch to extract the two views and the auxiliary
variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch: Any</strong></dt><dd><p>Parse a batch input and return V1, V2, and y.
The batch can be either:</p>
<ul class="simple">
<li><p>(V1, V2): two views of the same sample.</p></li>
<li><p>((V1, V2), y): two views and an auxiliary label or variable.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>V1</strong><span class="classifier">torch.Tensor</span></dt><dd><p>First view of the input.</p>
</dd>
<dt><strong>V2</strong><span class="classifier">torch.Tensor</span></dt><dd><p>Second view of the input.</p>
</dd>
<dt><strong>y</strong><span class="classifier">Optional[torch.Tensor]</span></dt><dd><p>Auxiliary label or variable, if present. Otherwise, None.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L204"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning.training_step" title="Link to this definition">¶</a></dt>
<dd><p>Perform one training step and computes training loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch: Any</strong></dt><dd><p>A batch of data that has been generated from train_dataloader.
It can be a pair of <cite>torch.Tensor</cite> (V1, V2) or a pair
((V1, V2), y) where V1 and V2 are the two views of the same sample
and y is the auxiliary variable.</p>
</dd>
<dt><strong>batch_idx: int</strong></dt><dd><p>The index of the current batch (ignored).</p>
</dd>
<dt><strong>dataloader_idx: int, default=0</strong></dt><dd><p>The index of the dataloader (ignored).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>loss: Tensor</dt><dd><p>Training loss computed on this batch of data.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning.transform_step">
<span class="sig-name descname"><span class="pre">transform_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L290"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning.transform_step" title="Link to this definition">¶</a></dt>
<dd><p>Encode the input data into the latent space.</p>
<p>Importantly, we do not apply the projection head here since it is
not part of the final model at inference time (only used for training).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch: torch.Tensor</strong></dt><dd><p>A batch of data that has been generated from <cite>test_dataloader</cite>.
This is given as is to the encoder.</p>
</dd>
<dt><strong>batch_idx: int</strong></dt><dd><p>The index of the current batch (ignored).</p>
</dd>
<dt><strong>dataloader_idx: int, default=0</strong></dt><dd><p>The index of the dataloader (ignored).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>features: torch.Tensor</dt><dd><p>The encoded features returned by the encoder.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nidl.estimators.ssl.YAwareContrastiveLearning.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/neuropsin-deepinsight/nidl/blob/816b775/nidl/estimators/ssl/yaware.py#L250"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nidl.estimators.ssl.YAwareContrastiveLearning.validation_step" title="Link to this definition">¶</a></dt>
<dd><p>Perform one validation step and computes validation loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch: Any</strong></dt><dd><p>A batch of data that has been generated from val_dataloader.
It can be a pair of <cite>torch.Tensor</cite> (V1, V2) or a pair
((V1, V2), y) where V1 and V2 are the two views of the same
sample and y is the auxiliary variable.</p>
</dd>
<dt><strong>batch_idx: int</strong></dt><dd><p>The index of the current batch (ignored).</p>
</dd>
<dt><strong>dataloader_idx: int, default=0</strong></dt><dd><p>The index of the dataloader (ignored).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<section id="examples-using-nidl-estimators-ssl-yawarecontrastivelearning">
<h2>Examples using <code class="docutils literal notranslate"><span class="pre">nidl.estimators.ssl.YAwareContrastiveLearning</span></code><a class="headerlink" href="#examples-using-nidl-estimators-ssl-yawarecontrastivelearning" title="Link to this heading">¶</a></h2>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This notebook will show you how to investigate the data representation given by an embedding estimator during training  (such as SimCLR, y-Aware Contrastive Learning or Barlow Twins) using the notion of &quot;probing&quot;. A standard machine learning model (e.g. linear or SVM) is trained and evaluated on the data embedding for a given task as the model is being fitted. It allows the user to understand what concepts are learned by the model."><img alt="" src="../../_images/sphx_glr_plot_model_probing_thumb.png" />
<p><a class="reference internal" href="../../auto_examples/plot_model_probing.html#sphx-glr-auto-examples-plot-model-probing-py"><span class="std std-ref">Model probing callback of embedding estimators</span></a></p>
  <div class="sphx-glr-thumbnail-title">Model probing callback of embedding estimators</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This tutorial will show you how to fit and evaluate a y-Aware Contrastive Learning model on the OpenBHB dataset using NIDL. As in the original paper [1]_, we will use age as a weak label to guide the contrastive learning process. The model will be trained to bring representations of samples with similar ages closer together in the feature space, while pushing apart samples with dissimilar ages. This approach leverages the age information to enhance the quality of the learned representations, making them more relevant for downstream tasks such as age prediction or disease classification."><img alt="" src="../../_images/sphx_glr_plot_yaware_openbhb_thumb.png" />
<p><a class="reference internal" href="../../auto_examples/plot_yaware_openbhb.html#sphx-glr-auto-examples-plot-yaware-openbhb-py"><span class="std std-ref">Weakly Supervised Contrastive Learning with y-Aware</span></a></p>
  <div class="sphx-glr-thumbnail-title">Weakly Supervised Contrastive Learning with y-Aware</div>
</div></div><div style='clear:both'></div></section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="nidl.estimators.ssl.BarlowTwins.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">nidl.estimators.ssl.BarlowTwins</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="nidl.estimators.ssl.SimCLR.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">nidl.estimators.ssl.SimCLR</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; The nidl developers
- Code and documentation distributed under CeCILL-B license.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link fa-brands fa-solid fa-github fa-2x" href="https://github.com/neurospin-deepinsight/nidl" aria-label="GitHub"></a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">nidl.estimators.ssl.YAwareContrastiveLearning</a><ul>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning</span></code></a><ul>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning.__init__"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning.__init__</span></code></a></li>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning.all_gather_and_flatten"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning.all_gather_and_flatten</span></code></a></li>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning.configure_optimizers"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning.configure_optimizers</span></code></a></li>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning.parse_batch"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning.parse_batch</span></code></a></li>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning.training_step"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning.training_step</span></code></a></li>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning.transform_step"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning.transform_step</span></code></a></li>
<li><a class="reference internal" href="#nidl.estimators.ssl.YAwareContrastiveLearning.validation_step"><code class="docutils literal notranslate"><span class="pre">YAwareContrastiveLearning.validation_step</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#examples-using-nidl-estimators-ssl-yawarecontrastivelearning">Examples using <code class="docutils literal notranslate"><span class="pre">nidl.estimators.ssl.YAwareContrastiveLearning</span></code></a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=4ea706d9"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    </body>
</html>