
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_yaware_openbhb.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_yaware_openbhb.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_yaware_openbhb.py:


Weakly Supervised Contrastive Learning with y-Aware
===================================================

This tutorial will show you how to fit and evaluate a y-Aware Contrastive
Learning model on the OpenBHB dataset using NIDL. As in the original paper
[1]_, we will use age as a weak label to guide the contrastive learning
process. The model will be trained to bring representations of samples with
similar ages closer together in the feature space, while pushing apart samples
with dissimilar ages. This approach leverages the age information to enhance
the quality of the learned representations, making them more relevant for
downstream tasks such as age prediction or disease classification.

We will follow these steps using the NIDL library:

1. Load the OpenBHB dataset.
2. Define the data augmentations for weakly-supervised training.
3. Define the y-Aware Contrastive Learning model.
4. Train the model using the weak labels.
5. Visualize the model's embedding using MDS and evaluate its
   performance on age prediction using linear regression and KNN.

As for the neuroimaging data, we will investigate two input representations:

- Voxel-based morphometry (VBM) maps, which are preprocessed gray matter
  density maps.
- Surface-based morphometry (SBM) maps, which are cortical thickness, mean
  curvature, gray matter volume and surface area maps projected onto a
  standard surface template.

  Both representations are available in the OpenBHB dataset. To make the
  training faster and reduce the memory footprint, we will consider regions
  of interest (ROIs) instead of the whole brain. For VBM, we will
  use the mean gray matter density averaged within each ROI of the
  Neuromorphometrics atlas (284 regions). For SBM, we will use the cortical
  thickness, mean curvature, gray matter volume and surface area averaged
  within each ROI of the Desikan-Killiany atlas (68 regions).

  The y-Aware Contrastive Learning model will be trained individually on both
  representations and we will compare their performance on age prediction.

.. [1] Dufumier et al., Contrastive learning with continuous proxy meta-data
       for 3d mri classification, MICCAI 2021.

Setup
-----

This notebook requires some packages besides nidl. Let's first start with
importing our standard libraries below:

.. GENERATED FROM PYTHON SOURCE LINES 51-66

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import torchvision.transforms as transforms
    from sklearn.linear_model import LinearRegression
    from sklearn.manifold import MDS
    from sklearn.metrics import mean_absolute_error, r2_score
    from sklearn.neighbors import KNeighborsRegressor
    from torch.utils.data import DataLoader
    from torchvision.ops import MLP

    from nidl.datasets import OpenBHB
    from nidl.estimators.ssl import YAwareContrastiveLearning
    from nidl.transforms import MultiViewsTransform








.. GENERATED FROM PYTHON SOURCE LINES 67-68

We define some global parameters that will be used throughout the notebook:

.. GENERATED FROM PYTHON SOURCE LINES 68-74

.. code-block:: Python

    data_dir = "/tmp/openbhb"
    batch_size = 128
    num_workers = 10
    latent_size = 32









.. GENERATED FROM PYTHON SOURCE LINES 75-82

OpenBHB datasets and data augmentations for Contrastive Learning
----------------------------------------------------------------

We will use the OpenBHB dataset for pre-training the models. We will focus
on the VBM ROI representation and the SBM ROI representation for this
tutorial. Since they are tabular data, we will use random masking and
adding Gaussian noise as data augmentation in contrastive learning.

.. GENERATED FROM PYTHON SOURCE LINES 82-97

.. code-block:: Python


    # Hyperparameters for data augmentations
    mask_prob = 0.8
    noise_std = 0.5
    contrast_transforms = transforms.Compose(
        [
            lambda x: x.flatten(),
            lambda x: (np.random.rand(*x.shape) > mask_prob).astype(np.float32)
            * x,  # random masking
            lambda x: x
            + (
                (np.random.rand() > 0.5) * np.random.randn(*x.shape) * noise_std
            ).astype(np.float32),  # random Gaussian noise
        ]
    )







.. GENERATED FROM PYTHON SOURCE LINES 98-100

We first create the SSL dataloaders with VBM modality and age as weak label.
We use the previous contrastive transforms for data augmentation.

.. GENERATED FROM PYTHON SOURCE LINES 100-127

.. code-block:: Python


    dataloader_ssl_vbm = DataLoader(
        OpenBHB(
            data_dir,
            modality="vbm_roi",
            target="age",
            streaming=False,
            transforms=MultiViewsTransform(contrast_transforms, n_views=2),
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=True,
    )
    dataloader_ssl_vbm_test = DataLoader(
        OpenBHB(
            data_dir,
            modality="vbm_roi",
            target="age",
            split="val",
            streaming=False,
            transforms=MultiViewsTransform(contrast_transforms, n_views=2),
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 128-131

Then, we create the SSL dataloaders with SBM modality on the Desikan-Killiany
atlas and age as weak label. We only extract some surface features and we use
the same contrastive transforms as for VBM.

.. GENERATED FROM PYTHON SOURCE LINES 131-175

.. code-block:: Python


    # Extract only surface area, GM volume, cortical thickness, mean curvature for
    # SBM maps
    sbm_channels = [0, 1, 2, 5]


    def sbm_transform(x):
        return x[sbm_channels].flatten()


    def vbm_transform(x):
        return x.flatten()


    dataloader_ssl_sbm = DataLoader(
        OpenBHB(
            data_dir,
            modality="fs_desikan_roi",
            target="age",
            streaming=False,
            transforms=MultiViewsTransform(
                transforms.Compose([sbm_transform, contrast_transforms]), n_views=2
            ),
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=True,
    )
    dataloader_ssl_sbm_test = DataLoader(
        OpenBHB(
            data_dir,
            modality="fs_desikan_roi",
            target="age",
            split="val",
            streaming=False,
            transforms=MultiViewsTransform(
                transforms.Compose([sbm_transform, contrast_transforms]), n_views=2
            ),
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 176-178

Finally, we create the dataloaders for evaluating the learned representations
on age prediction. We don't apply any data augmentation here.

.. GENERATED FROM PYTHON SOURCE LINES 178-237

.. code-block:: Python


    dataloader_vbm_train = DataLoader(
        OpenBHB(
            data_dir,
            modality="vbm_roi",
            target="age",
            split="train",
            transforms=vbm_transform,
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )

    dataloader_vbm_test = DataLoader(
        OpenBHB(
            data_dir,
            modality="vbm_roi",
            target="age",
            split="val",
            transforms=vbm_transform,
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )

    dataloader_sbm_train = DataLoader(
        OpenBHB(
            data_dir,
            modality="fs_desikan_roi",
            target="age",
            split="train",
            transforms=sbm_transform,
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )
    dataloader_sbm_test = DataLoader(
        OpenBHB(
            data_dir,
            modality="fs_desikan_roi",
            target="age",
            split="val",
            transforms=sbm_transform,
        ),
        batch_size=batch_size,
        num_workers=num_workers,
        shuffle=False,
    )

    # Small hack to avoid returning the target in the dataloaders since we aim
    # at transforming these datasets without their targets.
    dataloader_vbm_train.dataset.target = None
    dataloader_vbm_test.dataset.target = None
    dataloader_sbm_train.dataset.target = None
    dataloader_sbm_test.dataset.target = None





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(




.. GENERATED FROM PYTHON SOURCE LINES 238-244

Training of y-Aware Contrastive Learning models
-----------------------------------------------

We can now instantiate and train two y-Aware Contrastive Learning models (one
for VBM and another for SBM). We use the age as weak label to impose similar
representations of samples with close age for both models.

.. GENERATED FROM PYTHON SOURCE LINES 246-250

Since we work with tabular data, we can use a simple MLP as encoder. For
VBM data, the input dimension is 284 and we compress the data to a 32-d
vector. SBM data is flattened to a 272-d vector (68 regions * 4 features)
and we also compress it to a 32-d vector.

.. GENERATED FROM PYTHON SOURCE LINES 250-254

.. code-block:: Python


    vbm_encoder = MLP(in_channels=284, hidden_channels=[64, latent_size])
    sbm_encoder = MLP(in_channels=272, hidden_channels=[64, latent_size])








.. GENERATED FROM PYTHON SOURCE LINES 255-258

We limit the training to 10 epochs for the sake of time and we use a small
bandwidth for the Gaussian kernel in the y-Aware model compared to the
variance of the age in OpenBHB (sigma=4 vs std(age)=15).

.. GENERATED FROM PYTHON SOURCE LINES 258-290

.. code-block:: Python


    sigma = 4
    vbm_model = YAwareContrastiveLearning(
        encoder=vbm_encoder,
        projection_head_kwargs={
            "input_dim": latent_size,
            "hidden_dim": 2 * latent_size,
            "output_dim": latent_size,
        },
        bandwidth=sigma**2,
        random_state=42,
        max_epochs=10,
        temperature=0.1,
        learning_rate=1e-5,
        enable_checkpointing=False,
    )

    sbm_model = YAwareContrastiveLearning(
        encoder=sbm_encoder,
        projection_head_kwargs={
            "input_dim": latent_size,
            "hidden_dim": 2 * latent_size,
            "output_dim": latent_size,
        },
        bandwidth=sigma**2,
        random_state=42,
        max_epochs=10,
        temperature=0.1,
        learning_rate=1e-5,
        enable_checkpointing=False,
    )








.. GENERATED FROM PYTHON SOURCE LINES 291-292

We train both models on their respective dataloaders.

.. GENERATED FROM PYTHON SOURCE LINES 292-302

.. code-block:: Python

    vbm_model.fit(
        dataloader_ssl_vbm,
        dataloader_ssl_vbm_test,
    )

    sbm_model.fit(
        dataloader_ssl_sbm,
        dataloader_ssl_sbm_test,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Sanity Checking: |          | 0/? [00:00<?, ?it/s]    Sanity Checking: |          | 0/? [00:00<?, ?it/s]    Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]    Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 11.98it/s]    Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 15.13it/s]                                                                               /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
    Training: |          | 0/? [00:00<?, ?it/s]    Training: |          | 0/? [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/26 [00:00<?, ?it/s]    Epoch 0:   4%|▍         | 1/26 [00:00<00:23,  1.05it/s]    Epoch 0:   4%|▍         | 1/26 [00:00<00:23,  1.05it/s, v_num=8, loss/train=11.70]    Epoch 0:   8%|▊         | 2/26 [00:01<00:16,  1.46it/s, v_num=8, loss/train=11.70]    Epoch 0:   8%|▊         | 2/26 [00:01<00:16,  1.46it/s, v_num=8, loss/train=11.80]    Epoch 0:  12%|█▏        | 3/26 [00:01<00:11,  2.08it/s, v_num=8, loss/train=11.80]    Epoch 0:  12%|█▏        | 3/26 [00:01<00:11,  2.07it/s, v_num=8, loss/train=11.70]    Epoch 0:  15%|█▌        | 4/26 [00:01<00:08,  2.59it/s, v_num=8, loss/train=11.70]    Epoch 0:  15%|█▌        | 4/26 [00:01<00:08,  2.58it/s, v_num=8, loss/train=11.60]    Epoch 0:  19%|█▉        | 5/26 [00:01<00:06,  3.11it/s, v_num=8, loss/train=11.60]    Epoch 0:  19%|█▉        | 5/26 [00:01<00:06,  3.11it/s, v_num=8, loss/train=11.70]    Epoch 0:  23%|██▎       | 6/26 [00:01<00:05,  3.67it/s, v_num=8, loss/train=11.70]    Epoch 0:  23%|██▎       | 6/26 [00:01<00:05,  3.67it/s, v_num=8, loss/train=11.60]    Epoch 0:  27%|██▋       | 7/26 [00:01<00:04,  4.20it/s, v_num=8, loss/train=11.60]    Epoch 0:  27%|██▋       | 7/26 [00:01<00:04,  4.20it/s, v_num=8, loss/train=11.60]    Epoch 0:  31%|███       | 8/26 [00:01<00:03,  4.76it/s, v_num=8, loss/train=11.60]    Epoch 0:  31%|███       | 8/26 [00:01<00:03,  4.76it/s, v_num=8, loss/train=11.70]    Epoch 0:  35%|███▍      | 9/26 [00:01<00:03,  5.32it/s, v_num=8, loss/train=11.70]    Epoch 0:  35%|███▍      | 9/26 [00:01<00:03,  5.32it/s, v_num=8, loss/train=11.70]    Epoch 0:  38%|███▊      | 10/26 [00:01<00:02,  5.78it/s, v_num=8, loss/train=11.70]    Epoch 0:  38%|███▊      | 10/26 [00:01<00:02,  5.77it/s, v_num=8, loss/train=11.60]    Epoch 0:  42%|████▏     | 11/26 [00:01<00:02,  6.31it/s, v_num=8, loss/train=11.60]    Epoch 0:  42%|████▏     | 11/26 [00:01<00:02,  6.31it/s, v_num=8, loss/train=11.60]    Epoch 0:  46%|████▌     | 12/26 [00:01<00:02,  6.86it/s, v_num=8, loss/train=11.60]    Epoch 0:  46%|████▌     | 12/26 [00:01<00:02,  6.85it/s, v_num=8, loss/train=11.60]    Epoch 0:  50%|█████     | 13/26 [00:01<00:01,  7.39it/s, v_num=8, loss/train=11.60]    Epoch 0:  50%|█████     | 13/26 [00:01<00:01,  7.39it/s, v_num=8, loss/train=11.70]    Epoch 0:  54%|█████▍    | 14/26 [00:01<00:01,  7.93it/s, v_num=8, loss/train=11.70]    Epoch 0:  54%|█████▍    | 14/26 [00:01<00:01,  7.92it/s, v_num=8, loss/train=11.60]    Epoch 0:  58%|█████▊    | 15/26 [00:01<00:01,  8.44it/s, v_num=8, loss/train=11.60]    Epoch 0:  58%|█████▊    | 15/26 [00:01<00:01,  8.44it/s, v_num=8, loss/train=11.70]    Epoch 0:  62%|██████▏   | 16/26 [00:01<00:01,  8.96it/s, v_num=8, loss/train=11.70]    Epoch 0:  62%|██████▏   | 16/26 [00:01<00:01,  8.96it/s, v_num=8, loss/train=11.60]    Epoch 0:  65%|██████▌   | 17/26 [00:01<00:00,  9.46it/s, v_num=8, loss/train=11.60]    Epoch 0:  65%|██████▌   | 17/26 [00:01<00:00,  9.46it/s, v_num=8, loss/train=11.70]    Epoch 0:  69%|██████▉   | 18/26 [00:01<00:00,  9.98it/s, v_num=8, loss/train=11.70]    Epoch 0:  69%|██████▉   | 18/26 [00:01<00:00,  9.97it/s, v_num=8, loss/train=11.70]    Epoch 0:  73%|███████▎  | 19/26 [00:01<00:00, 10.49it/s, v_num=8, loss/train=11.70]    Epoch 0:  73%|███████▎  | 19/26 [00:01<00:00, 10.49it/s, v_num=8, loss/train=11.60]    Epoch 0:  77%|███████▋  | 20/26 [00:01<00:00, 10.94it/s, v_num=8, loss/train=11.60]    Epoch 0:  77%|███████▋  | 20/26 [00:01<00:00, 10.94it/s, v_num=8, loss/train=11.60]    Epoch 0:  81%|████████  | 21/26 [00:01<00:00, 11.44it/s, v_num=8, loss/train=11.60]    Epoch 0:  81%|████████  | 21/26 [00:01<00:00, 11.44it/s, v_num=8, loss/train=11.60]    Epoch 0:  85%|████████▍ | 22/26 [00:01<00:00, 11.93it/s, v_num=8, loss/train=11.60]    Epoch 0:  85%|████████▍ | 22/26 [00:01<00:00, 11.93it/s, v_num=8, loss/train=11.70]    Epoch 0:  88%|████████▊ | 23/26 [00:01<00:00, 12.42it/s, v_num=8, loss/train=11.70]    Epoch 0:  88%|████████▊ | 23/26 [00:01<00:00, 12.42it/s, v_num=8, loss/train=11.70]    Epoch 0:  92%|█████████▏| 24/26 [00:01<00:00, 12.89it/s, v_num=8, loss/train=11.70]    Epoch 0:  92%|█████████▏| 24/26 [00:01<00:00, 12.88it/s, v_num=8, loss/train=11.70]    Epoch 0:  96%|█████████▌| 25/26 [00:01<00:00, 13.37it/s, v_num=8, loss/train=11.70]    Epoch 0:  96%|█████████▌| 25/26 [00:01<00:00, 13.36it/s, v_num=8, loss/train=11.60]    Epoch 0: 100%|██████████| 26/26 [00:01<00:00, 13.85it/s, v_num=8, loss/train=11.60]    Epoch 0: 100%|██████████| 26/26 [00:01<00:00, 13.85it/s, v_num=8, loss/train=8.410]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 64.85it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 94.11it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 111.38it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 122.94it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 131.43it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 126.91it/s]
                                                                               Epoch 0: 100%|██████████| 26/26 [00:02<00:00,  9.53it/s, v_num=8, loss/train=8.410, loss/val=11.60]    Epoch 0: 100%|██████████| 26/26 [00:02<00:00,  9.53it/s, v_num=8, loss/train=8.410, loss/val=11.60]    Epoch 0:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.410, loss/val=11.60]             Epoch 1:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.410, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 1:   4%|▍         | 1/26 [00:01<00:47,  0.52it/s, v_num=8, loss/train=8.410, loss/val=11.60]    Epoch 1:   4%|▍         | 1/26 [00:01<00:48,  0.52it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:   8%|▊         | 2/26 [00:02<00:24,  0.98it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:   8%|▊         | 2/26 [00:02<00:24,  0.98it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  12%|█▏        | 3/26 [00:02<00:15,  1.46it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  12%|█▏        | 3/26 [00:02<00:15,  1.45it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  15%|█▌        | 4/26 [00:02<00:11,  1.90it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  15%|█▌        | 4/26 [00:02<00:11,  1.90it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  19%|█▉        | 5/26 [00:02<00:09,  2.28it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  19%|█▉        | 5/26 [00:02<00:09,  2.28it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  23%|██▎       | 6/26 [00:02<00:07,  2.71it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  23%|██▎       | 6/26 [00:02<00:07,  2.71it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  27%|██▋       | 7/26 [00:02<00:06,  3.13it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  27%|██▋       | 7/26 [00:02<00:06,  3.13it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  31%|███       | 8/26 [00:02<00:05,  3.56it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  31%|███       | 8/26 [00:02<00:05,  3.56it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  35%|███▍      | 9/26 [00:02<00:04,  3.98it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  35%|███▍      | 9/26 [00:02<00:04,  3.98it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  38%|███▊      | 10/26 [00:02<00:03,  4.41it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  38%|███▊      | 10/26 [00:02<00:03,  4.41it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  42%|████▏     | 11/26 [00:02<00:03,  4.81it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  42%|████▏     | 11/26 [00:02<00:03,  4.81it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  46%|████▌     | 12/26 [00:02<00:02,  5.22it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  46%|████▌     | 12/26 [00:02<00:02,  5.22it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  50%|█████     | 13/26 [00:02<00:02,  5.63it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  50%|█████     | 13/26 [00:02<00:02,  5.63it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  54%|█████▍    | 14/26 [00:02<00:01,  6.05it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  54%|█████▍    | 14/26 [00:02<00:01,  6.04it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  58%|█████▊    | 15/26 [00:02<00:01,  6.46it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  58%|█████▊    | 15/26 [00:02<00:01,  6.45it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  62%|██████▏   | 16/26 [00:02<00:01,  6.86it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  62%|██████▏   | 16/26 [00:02<00:01,  6.86it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  65%|██████▌   | 17/26 [00:02<00:01,  7.27it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  65%|██████▌   | 17/26 [00:02<00:01,  7.27it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  69%|██████▉   | 18/26 [00:02<00:01,  7.66it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  69%|██████▉   | 18/26 [00:02<00:01,  7.66it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  73%|███████▎  | 19/26 [00:02<00:00,  8.06it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  73%|███████▎  | 19/26 [00:02<00:00,  8.06it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  77%|███████▋  | 20/26 [00:02<00:00,  8.46it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  77%|███████▋  | 20/26 [00:02<00:00,  8.46it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  81%|████████  | 21/26 [00:02<00:00,  8.86it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  81%|████████  | 21/26 [00:02<00:00,  8.86it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  85%|████████▍ | 22/26 [00:02<00:00,  9.24it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  85%|████████▍ | 22/26 [00:02<00:00,  9.24it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  88%|████████▊ | 23/26 [00:02<00:00,  9.61it/s, v_num=8, loss/train=11.60, loss/val=11.60]    Epoch 1:  88%|████████▊ | 23/26 [00:02<00:00,  9.60it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  92%|█████████▏| 24/26 [00:02<00:00,  9.99it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  92%|█████████▏| 24/26 [00:02<00:00,  9.99it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  96%|█████████▌| 25/26 [00:02<00:00, 10.37it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1:  96%|█████████▌| 25/26 [00:02<00:00, 10.37it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1: 100%|██████████| 26/26 [00:02<00:00, 10.76it/s, v_num=8, loss/train=11.50, loss/val=11.60]    Epoch 1: 100%|██████████| 26/26 [00:02<00:00, 10.76it/s, v_num=8, loss/train=8.470, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 38.90it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 30.37it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 41.46it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 36.26it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 43.42it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 48.85it/s]
                                                                              Epoch 1: 100%|██████████| 26/26 [00:03<00:00,  7.86it/s, v_num=8, loss/train=8.470, loss/val=11.50]    Epoch 1: 100%|██████████| 26/26 [00:03<00:00,  7.85it/s, v_num=8, loss/train=8.470, loss/val=11.50]    Epoch 1:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.470, loss/val=11.50]             Epoch 2:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.470, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 2:   4%|▍         | 1/26 [00:01<00:43,  0.58it/s, v_num=8, loss/train=8.470, loss/val=11.50]    Epoch 2:   4%|▍         | 1/26 [00:01<00:43,  0.58it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:   8%|▊         | 2/26 [00:01<00:23,  1.04it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:   8%|▊         | 2/26 [00:01<00:23,  1.03it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:  12%|█▏        | 3/26 [00:02<00:15,  1.48it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:  12%|█▏        | 3/26 [00:02<00:15,  1.48it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:  15%|█▌        | 4/26 [00:02<00:11,  1.93it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:  15%|█▌        | 4/26 [00:02<00:11,  1.93it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  19%|█▉        | 5/26 [00:02<00:08,  2.38it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  19%|█▉        | 5/26 [00:02<00:08,  2.38it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  23%|██▎       | 6/26 [00:02<00:07,  2.82it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  23%|██▎       | 6/26 [00:02<00:07,  2.82it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  27%|██▋       | 7/26 [00:02<00:05,  3.25it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  27%|██▋       | 7/26 [00:02<00:05,  3.24it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  31%|███       | 8/26 [00:02<00:04,  3.68it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  31%|███       | 8/26 [00:02<00:04,  3.67it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  35%|███▍      | 9/26 [00:02<00:04,  4.05it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  35%|███▍      | 9/26 [00:02<00:04,  4.05it/s, v_num=8, loss/train=11.40, loss/val=11.50]    Epoch 2:  38%|███▊      | 10/26 [00:02<00:03,  4.45it/s, v_num=8, loss/train=11.40, loss/val=11.50]    Epoch 2:  38%|███▊      | 10/26 [00:02<00:03,  4.45it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  42%|████▏     | 11/26 [00:02<00:03,  4.86it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  42%|████▏     | 11/26 [00:02<00:03,  4.86it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  46%|████▌     | 12/26 [00:02<00:02,  5.25it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  46%|████▌     | 12/26 [00:02<00:02,  5.25it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  50%|█████     | 13/26 [00:02<00:02,  5.66it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  50%|█████     | 13/26 [00:02<00:02,  5.66it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  54%|█████▍    | 14/26 [00:02<00:01,  6.07it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  54%|█████▍    | 14/26 [00:02<00:01,  6.07it/s, v_num=8, loss/train=11.40, loss/val=11.50]    Epoch 2:  58%|█████▊    | 15/26 [00:02<00:01,  6.48it/s, v_num=8, loss/train=11.40, loss/val=11.50]    Epoch 2:  58%|█████▊    | 15/26 [00:02<00:01,  6.48it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:  62%|██████▏   | 16/26 [00:02<00:01,  6.89it/s, v_num=8, loss/train=11.60, loss/val=11.50]    Epoch 2:  62%|██████▏   | 16/26 [00:02<00:01,  6.89it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  65%|██████▌   | 17/26 [00:02<00:01,  7.29it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  65%|██████▌   | 17/26 [00:02<00:01,  7.29it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  69%|██████▉   | 18/26 [00:02<00:01,  7.70it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  69%|██████▉   | 18/26 [00:02<00:01,  7.70it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  73%|███████▎  | 19/26 [00:02<00:00,  8.10it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  73%|███████▎  | 19/26 [00:02<00:00,  8.10it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  77%|███████▋  | 20/26 [00:02<00:00,  8.50it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  77%|███████▋  | 20/26 [00:02<00:00,  8.50it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  81%|████████  | 21/26 [00:02<00:00,  8.89it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  81%|████████  | 21/26 [00:02<00:00,  8.88it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  85%|████████▍ | 22/26 [00:02<00:00,  9.26it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  85%|████████▍ | 22/26 [00:02<00:00,  9.25it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  88%|████████▊ | 23/26 [00:02<00:00,  9.64it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  88%|████████▊ | 23/26 [00:02<00:00,  9.64it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  92%|█████████▏| 24/26 [00:02<00:00, 10.02it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  92%|█████████▏| 24/26 [00:02<00:00, 10.02it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  96%|█████████▌| 25/26 [00:02<00:00, 10.40it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2:  96%|█████████▌| 25/26 [00:02<00:00, 10.40it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2: 100%|██████████| 26/26 [00:02<00:00, 10.79it/s, v_num=8, loss/train=11.50, loss/val=11.50]    Epoch 2: 100%|██████████| 26/26 [00:02<00:00, 10.79it/s, v_num=8, loss/train=8.350, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 48.28it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 20.15it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 24.59it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 31.71it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 38.41it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 43.74it/s]
                                                                              Epoch 2: 100%|██████████| 26/26 [00:03<00:00,  8.00it/s, v_num=8, loss/train=8.350, loss/val=11.40]    Epoch 2: 100%|██████████| 26/26 [00:03<00:00,  8.00it/s, v_num=8, loss/train=8.350, loss/val=11.40]    Epoch 2:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.350, loss/val=11.40]             Epoch 3:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.350, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 3:   4%|▍         | 1/26 [00:01<00:44,  0.56it/s, v_num=8, loss/train=8.350, loss/val=11.40]    Epoch 3:   4%|▍         | 1/26 [00:01<00:44,  0.56it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:   8%|▊         | 2/26 [00:01<00:23,  1.02it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:   8%|▊         | 2/26 [00:01<00:23,  1.02it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  12%|█▏        | 3/26 [00:02<00:16,  1.42it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  12%|█▏        | 3/26 [00:02<00:16,  1.42it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  15%|█▌        | 4/26 [00:02<00:11,  1.84it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  15%|█▌        | 4/26 [00:02<00:11,  1.84it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  19%|█▉        | 5/26 [00:02<00:09,  2.27it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  19%|█▉        | 5/26 [00:02<00:09,  2.27it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  23%|██▎       | 6/26 [00:02<00:07,  2.71it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  23%|██▎       | 6/26 [00:02<00:07,  2.71it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  27%|██▋       | 7/26 [00:02<00:06,  3.13it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  27%|██▋       | 7/26 [00:02<00:06,  3.13it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  31%|███       | 8/26 [00:02<00:05,  3.55it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  31%|███       | 8/26 [00:02<00:05,  3.55it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  35%|███▍      | 9/26 [00:02<00:04,  3.96it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  35%|███▍      | 9/26 [00:02<00:04,  3.96it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  38%|███▊      | 10/26 [00:02<00:03,  4.36it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  38%|███▊      | 10/26 [00:02<00:03,  4.36it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  42%|████▏     | 11/26 [00:02<00:03,  4.76it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  42%|████▏     | 11/26 [00:02<00:03,  4.76it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  46%|████▌     | 12/26 [00:02<00:02,  5.14it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  46%|████▌     | 12/26 [00:02<00:02,  5.14it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  50%|█████     | 13/26 [00:02<00:02,  5.51it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  50%|█████     | 13/26 [00:02<00:02,  5.51it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  54%|█████▍    | 14/26 [00:02<00:02,  5.91it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  54%|█████▍    | 14/26 [00:02<00:02,  5.91it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  58%|█████▊    | 15/26 [00:02<00:01,  6.31it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  58%|█████▊    | 15/26 [00:02<00:01,  6.31it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  62%|██████▏   | 16/26 [00:02<00:01,  6.71it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  62%|██████▏   | 16/26 [00:02<00:01,  6.71it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  65%|██████▌   | 17/26 [00:02<00:01,  7.11it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  65%|██████▌   | 17/26 [00:02<00:01,  7.10it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  69%|██████▉   | 18/26 [00:02<00:01,  7.50it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  69%|██████▉   | 18/26 [00:02<00:01,  7.50it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  73%|███████▎  | 19/26 [00:02<00:00,  7.89it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  73%|███████▎  | 19/26 [00:02<00:00,  7.89it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  77%|███████▋  | 20/26 [00:02<00:00,  8.28it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  77%|███████▋  | 20/26 [00:02<00:00,  8.27it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  81%|████████  | 21/26 [00:02<00:00,  8.65it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  81%|████████  | 21/26 [00:02<00:00,  8.65it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  85%|████████▍ | 22/26 [00:02<00:00,  9.03it/s, v_num=8, loss/train=11.50, loss/val=11.40]    Epoch 3:  85%|████████▍ | 22/26 [00:02<00:00,  9.03it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  88%|████████▊ | 23/26 [00:02<00:00,  9.39it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  88%|████████▊ | 23/26 [00:02<00:00,  9.39it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  92%|█████████▏| 24/26 [00:02<00:00,  9.76it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  92%|█████████▏| 24/26 [00:02<00:00,  9.76it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  96%|█████████▌| 25/26 [00:02<00:00, 10.13it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3:  96%|█████████▌| 25/26 [00:02<00:00, 10.13it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3: 100%|██████████| 26/26 [00:02<00:00, 10.51it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 3: 100%|██████████| 26/26 [00:02<00:00, 10.51it/s, v_num=8, loss/train=8.260, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 78.09it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 112.47it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 134.41it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 149.44it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 160.28it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 154.76it/s]
                                                                               Epoch 3: 100%|██████████| 26/26 [00:03<00:00,  7.75it/s, v_num=8, loss/train=8.260, loss/val=11.40]    Epoch 3: 100%|██████████| 26/26 [00:03<00:00,  7.75it/s, v_num=8, loss/train=8.260, loss/val=11.40]    Epoch 3:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.260, loss/val=11.40]             Epoch 4:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.260, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 4:   4%|▍         | 1/26 [00:02<00:51,  0.49it/s, v_num=8, loss/train=8.260, loss/val=11.40]    Epoch 4:   4%|▍         | 1/26 [00:02<00:51,  0.48it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:   8%|▊         | 2/26 [00:02<00:25,  0.96it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:   8%|▊         | 2/26 [00:02<00:25,  0.96it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  12%|█▏        | 3/26 [00:02<00:16,  1.42it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  12%|█▏        | 3/26 [00:02<00:16,  1.42it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  15%|█▌        | 4/26 [00:02<00:11,  1.87it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  15%|█▌        | 4/26 [00:02<00:11,  1.87it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  19%|█▉        | 5/26 [00:02<00:09,  2.25it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  19%|█▉        | 5/26 [00:02<00:09,  2.25it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  23%|██▎       | 6/26 [00:02<00:07,  2.64it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  23%|██▎       | 6/26 [00:02<00:07,  2.64it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  27%|██▋       | 7/26 [00:02<00:06,  3.04it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  27%|██▋       | 7/26 [00:02<00:06,  3.03it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  31%|███       | 8/26 [00:02<00:05,  3.45it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  31%|███       | 8/26 [00:02<00:05,  3.45it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  35%|███▍      | 9/26 [00:02<00:04,  3.86it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  35%|███▍      | 9/26 [00:02<00:04,  3.86it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  38%|███▊      | 10/26 [00:02<00:03,  4.27it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  38%|███▊      | 10/26 [00:02<00:03,  4.27it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  42%|████▏     | 11/26 [00:02<00:03,  4.66it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  42%|████▏     | 11/26 [00:02<00:03,  4.66it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  46%|████▌     | 12/26 [00:02<00:02,  5.07it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  46%|████▌     | 12/26 [00:02<00:02,  5.06it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  50%|█████     | 13/26 [00:02<00:02,  5.47it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  50%|█████     | 13/26 [00:02<00:02,  5.46it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  54%|█████▍    | 14/26 [00:02<00:02,  5.85it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  54%|█████▍    | 14/26 [00:02<00:02,  5.85it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  58%|█████▊    | 15/26 [00:02<00:01,  6.24it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  58%|█████▊    | 15/26 [00:02<00:01,  6.24it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  62%|██████▏   | 16/26 [00:02<00:01,  6.63it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  62%|██████▏   | 16/26 [00:02<00:01,  6.63it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  65%|██████▌   | 17/26 [00:02<00:01,  7.02it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  65%|██████▌   | 17/26 [00:02<00:01,  7.02it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  69%|██████▉   | 18/26 [00:02<00:01,  7.41it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  69%|██████▉   | 18/26 [00:02<00:01,  7.41it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  73%|███████▎  | 19/26 [00:02<00:00,  7.80it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  73%|███████▎  | 19/26 [00:02<00:00,  7.79it/s, v_num=8, loss/train=11.30, loss/val=11.40]    Epoch 4:  77%|███████▋  | 20/26 [00:02<00:00,  8.18it/s, v_num=8, loss/train=11.30, loss/val=11.40]    Epoch 4:  77%|███████▋  | 20/26 [00:02<00:00,  8.18it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  81%|████████  | 21/26 [00:02<00:00,  8.54it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  81%|████████  | 21/26 [00:02<00:00,  8.54it/s, v_num=8, loss/train=11.30, loss/val=11.40]    Epoch 4:  85%|████████▍ | 22/26 [00:02<00:00,  8.91it/s, v_num=8, loss/train=11.30, loss/val=11.40]    Epoch 4:  85%|████████▍ | 22/26 [00:02<00:00,  8.91it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  88%|████████▊ | 23/26 [00:02<00:00,  9.29it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  88%|████████▊ | 23/26 [00:02<00:00,  9.29it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  92%|█████████▏| 24/26 [00:02<00:00,  9.67it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4:  92%|█████████▏| 24/26 [00:02<00:00,  9.66it/s, v_num=8, loss/train=11.30, loss/val=11.40]    Epoch 4:  96%|█████████▌| 25/26 [00:02<00:00, 10.03it/s, v_num=8, loss/train=11.30, loss/val=11.40]    Epoch 4:  96%|█████████▌| 25/26 [00:02<00:00, 10.03it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4: 100%|██████████| 26/26 [00:02<00:00, 10.40it/s, v_num=8, loss/train=11.40, loss/val=11.40]    Epoch 4: 100%|██████████| 26/26 [00:02<00:00, 10.40it/s, v_num=8, loss/train=8.100, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 12.61it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 22.43it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 29.62it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 37.54it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 44.93it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 49.61it/s]
                                                                              Epoch 4: 100%|██████████| 26/26 [00:03<00:00,  7.76it/s, v_num=8, loss/train=8.100, loss/val=11.30]    Epoch 4: 100%|██████████| 26/26 [00:03<00:00,  7.76it/s, v_num=8, loss/train=8.100, loss/val=11.30]    Epoch 4:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.100, loss/val=11.30]             Epoch 5:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.100, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 5:   4%|▍         | 1/26 [00:02<00:52,  0.48it/s, v_num=8, loss/train=8.100, loss/val=11.30]    Epoch 5:   4%|▍         | 1/26 [00:02<00:52,  0.48it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:   8%|▊         | 2/26 [00:02<00:25,  0.94it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:   8%|▊         | 2/26 [00:02<00:25,  0.94it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  12%|█▏        | 3/26 [00:02<00:16,  1.39it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  12%|█▏        | 3/26 [00:02<00:16,  1.39it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  15%|█▌        | 4/26 [00:02<00:12,  1.82it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  15%|█▌        | 4/26 [00:02<00:12,  1.82it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  19%|█▉        | 5/26 [00:02<00:09,  2.24it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  19%|█▉        | 5/26 [00:02<00:09,  2.24it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  23%|██▎       | 6/26 [00:02<00:07,  2.59it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  23%|██▎       | 6/26 [00:02<00:07,  2.59it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  27%|██▋       | 7/26 [00:02<00:06,  3.00it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  27%|██▋       | 7/26 [00:02<00:06,  3.00it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  31%|███       | 8/26 [00:02<00:05,  3.40it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  31%|███       | 8/26 [00:02<00:05,  3.40it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  35%|███▍      | 9/26 [00:02<00:04,  3.80it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  35%|███▍      | 9/26 [00:02<00:04,  3.80it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  38%|███▊      | 10/26 [00:02<00:03,  4.19it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  38%|███▊      | 10/26 [00:02<00:03,  4.19it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  42%|████▏     | 11/26 [00:02<00:03,  4.53it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  42%|████▏     | 11/26 [00:02<00:03,  4.53it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  46%|████▌     | 12/26 [00:02<00:02,  4.91it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  46%|████▌     | 12/26 [00:02<00:02,  4.91it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  50%|█████     | 13/26 [00:02<00:02,  5.30it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  50%|█████     | 13/26 [00:02<00:02,  5.30it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  54%|█████▍    | 14/26 [00:02<00:02,  5.69it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  54%|█████▍    | 14/26 [00:02<00:02,  5.69it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  58%|█████▊    | 15/26 [00:02<00:01,  6.08it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  58%|█████▊    | 15/26 [00:02<00:01,  6.08it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  62%|██████▏   | 16/26 [00:02<00:01,  6.46it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  62%|██████▏   | 16/26 [00:02<00:01,  6.46it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  65%|██████▌   | 17/26 [00:02<00:01,  6.85it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  65%|██████▌   | 17/26 [00:02<00:01,  6.84it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  69%|██████▉   | 18/26 [00:02<00:01,  7.22it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  69%|██████▉   | 18/26 [00:02<00:01,  7.22it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  73%|███████▎  | 19/26 [00:02<00:00,  7.60it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  73%|███████▎  | 19/26 [00:02<00:00,  7.60it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  77%|███████▋  | 20/26 [00:02<00:00,  7.98it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  77%|███████▋  | 20/26 [00:02<00:00,  7.98it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  81%|████████  | 21/26 [00:02<00:00,  8.33it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  81%|████████  | 21/26 [00:02<00:00,  8.33it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  85%|████████▍ | 22/26 [00:02<00:00,  8.69it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  85%|████████▍ | 22/26 [00:02<00:00,  8.69it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  88%|████████▊ | 23/26 [00:02<00:00,  9.06it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  88%|████████▊ | 23/26 [00:02<00:00,  9.05it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  92%|█████████▏| 24/26 [00:02<00:00,  9.41it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5:  92%|█████████▏| 24/26 [00:02<00:00,  9.41it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  96%|█████████▌| 25/26 [00:02<00:00,  9.78it/s, v_num=8, loss/train=11.40, loss/val=11.30]    Epoch 5:  96%|█████████▌| 25/26 [00:02<00:00,  9.77it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5: 100%|██████████| 26/26 [00:02<00:00, 10.14it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 5: 100%|██████████| 26/26 [00:02<00:00, 10.14it/s, v_num=8, loss/train=8.140, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 45.72it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 16.96it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 24.51it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 31.57it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 38.20it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 43.42it/s]
                                                                              Epoch 5: 100%|██████████| 26/26 [00:03<00:00,  7.51it/s, v_num=8, loss/train=8.140, loss/val=11.30]    Epoch 5: 100%|██████████| 26/26 [00:03<00:00,  7.51it/s, v_num=8, loss/train=8.140, loss/val=11.30]    Epoch 5:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.140, loss/val=11.30]             Epoch 6:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.140, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 6:   4%|▍         | 1/26 [00:01<00:46,  0.54it/s, v_num=8, loss/train=8.140, loss/val=11.30]    Epoch 6:   4%|▍         | 1/26 [00:01<00:46,  0.54it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:   8%|▊         | 2/26 [00:01<00:23,  1.00it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:   8%|▊         | 2/26 [00:01<00:23,  1.00it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  12%|█▏        | 3/26 [00:02<00:15,  1.44it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  12%|█▏        | 3/26 [00:02<00:15,  1.44it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  15%|█▌        | 4/26 [00:02<00:11,  1.88it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  15%|█▌        | 4/26 [00:02<00:11,  1.88it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  19%|█▉        | 5/26 [00:02<00:09,  2.26it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  19%|█▉        | 5/26 [00:02<00:09,  2.26it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  23%|██▎       | 6/26 [00:02<00:07,  2.68it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  23%|██▎       | 6/26 [00:02<00:07,  2.68it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  27%|██▋       | 7/26 [00:02<00:06,  3.11it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  27%|██▋       | 7/26 [00:02<00:06,  3.11it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  31%|███       | 8/26 [00:02<00:05,  3.53it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  31%|███       | 8/26 [00:02<00:05,  3.53it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  35%|███▍      | 9/26 [00:02<00:04,  3.94it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  35%|███▍      | 9/26 [00:02<00:04,  3.94it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  38%|███▊      | 10/26 [00:02<00:03,  4.34it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  38%|███▊      | 10/26 [00:02<00:03,  4.34it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  42%|████▏     | 11/26 [00:02<00:03,  4.75it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  42%|████▏     | 11/26 [00:02<00:03,  4.75it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  46%|████▌     | 12/26 [00:02<00:02,  5.15it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  46%|████▌     | 12/26 [00:02<00:02,  5.15it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  50%|█████     | 13/26 [00:02<00:02,  5.55it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  50%|█████     | 13/26 [00:02<00:02,  5.55it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  54%|█████▍    | 14/26 [00:02<00:02,  5.96it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  54%|█████▍    | 14/26 [00:02<00:02,  5.96it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  58%|█████▊    | 15/26 [00:02<00:01,  6.35it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  58%|█████▊    | 15/26 [00:02<00:01,  6.35it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  62%|██████▏   | 16/26 [00:02<00:01,  6.75it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  62%|██████▏   | 16/26 [00:02<00:01,  6.75it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  65%|██████▌   | 17/26 [00:02<00:01,  7.14it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  65%|██████▌   | 17/26 [00:02<00:01,  7.14it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  69%|██████▉   | 18/26 [00:02<00:01,  7.53it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  69%|██████▉   | 18/26 [00:02<00:01,  7.53it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  73%|███████▎  | 19/26 [00:02<00:00,  7.92it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  73%|███████▎  | 19/26 [00:02<00:00,  7.92it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  77%|███████▋  | 20/26 [00:02<00:00,  8.30it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  77%|███████▋  | 20/26 [00:02<00:00,  8.30it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  81%|████████  | 21/26 [00:02<00:00,  8.69it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  81%|████████  | 21/26 [00:02<00:00,  8.69it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  85%|████████▍ | 22/26 [00:02<00:00,  9.05it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  85%|████████▍ | 22/26 [00:02<00:00,  9.04it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  88%|████████▊ | 23/26 [00:02<00:00,  9.43it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  88%|████████▊ | 23/26 [00:02<00:00,  9.43it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  92%|█████████▏| 24/26 [00:02<00:00,  9.81it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  92%|█████████▏| 24/26 [00:02<00:00,  9.80it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  96%|█████████▌| 25/26 [00:02<00:00, 10.17it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6:  96%|█████████▌| 25/26 [00:02<00:00, 10.17it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6: 100%|██████████| 26/26 [00:02<00:00, 10.55it/s, v_num=8, loss/train=11.30, loss/val=11.30]    Epoch 6: 100%|██████████| 26/26 [00:02<00:00, 10.55it/s, v_num=8, loss/train=8.210, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 64.24it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 91.38it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 109.05it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 121.29it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 129.85it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 125.88it/s]
                                                                               Epoch 6: 100%|██████████| 26/26 [00:03<00:00,  7.81it/s, v_num=8, loss/train=8.210, loss/val=11.20]    Epoch 6: 100%|██████████| 26/26 [00:03<00:00,  7.81it/s, v_num=8, loss/train=8.210, loss/val=11.20]    Epoch 6:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.210, loss/val=11.20]             Epoch 7:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.210, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 7:   4%|▍         | 1/26 [00:01<00:42,  0.59it/s, v_num=8, loss/train=8.210, loss/val=11.20]    Epoch 7:   4%|▍         | 1/26 [00:01<00:42,  0.59it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:   8%|▊         | 2/26 [00:02<00:24,  0.99it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:   8%|▊         | 2/26 [00:02<00:24,  0.99it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  12%|█▏        | 3/26 [00:02<00:16,  1.43it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  12%|█▏        | 3/26 [00:02<00:16,  1.43it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  15%|█▌        | 4/26 [00:02<00:11,  1.88it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  15%|█▌        | 4/26 [00:02<00:11,  1.88it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  19%|█▉        | 5/26 [00:02<00:09,  2.31it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  19%|█▉        | 5/26 [00:02<00:09,  2.31it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  23%|██▎       | 6/26 [00:02<00:07,  2.74it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  23%|██▎       | 6/26 [00:02<00:07,  2.73it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  27%|██▋       | 7/26 [00:02<00:06,  3.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  27%|██▋       | 7/26 [00:02<00:06,  3.12it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  31%|███       | 8/26 [00:02<00:05,  3.53it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  31%|███       | 8/26 [00:02<00:05,  3.53it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  35%|███▍      | 9/26 [00:02<00:04,  3.94it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  35%|███▍      | 9/26 [00:02<00:04,  3.94it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  38%|███▊      | 10/26 [00:02<00:03,  4.36it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  38%|███▊      | 10/26 [00:02<00:03,  4.35it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  42%|████▏     | 11/26 [00:02<00:03,  4.76it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  42%|████▏     | 11/26 [00:02<00:03,  4.76it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  46%|████▌     | 12/26 [00:02<00:02,  5.09it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  46%|████▌     | 12/26 [00:02<00:02,  5.09it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  50%|█████     | 13/26 [00:02<00:02,  5.49it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  50%|█████     | 13/26 [00:02<00:02,  5.49it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  54%|█████▍    | 14/26 [00:02<00:02,  5.89it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  54%|█████▍    | 14/26 [00:02<00:02,  5.89it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  58%|█████▊    | 15/26 [00:02<00:01,  6.28it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  58%|█████▊    | 15/26 [00:02<00:01,  6.28it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  62%|██████▏   | 16/26 [00:02<00:01,  6.68it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  62%|██████▏   | 16/26 [00:02<00:01,  6.68it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  65%|██████▌   | 17/26 [00:02<00:01,  7.07it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  65%|██████▌   | 17/26 [00:02<00:01,  7.07it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  69%|██████▉   | 18/26 [00:02<00:01,  7.46it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  69%|██████▉   | 18/26 [00:02<00:01,  7.46it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  73%|███████▎  | 19/26 [00:02<00:00,  7.85it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  73%|███████▎  | 19/26 [00:02<00:00,  7.84it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  77%|███████▋  | 20/26 [00:02<00:00,  8.23it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  77%|███████▋  | 20/26 [00:02<00:00,  8.23it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  81%|████████  | 21/26 [00:02<00:00,  8.62it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  81%|████████  | 21/26 [00:02<00:00,  8.62it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  85%|████████▍ | 22/26 [00:02<00:00,  8.98it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  85%|████████▍ | 22/26 [00:02<00:00,  8.97it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  88%|████████▊ | 23/26 [00:02<00:00,  9.35it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  88%|████████▊ | 23/26 [00:02<00:00,  9.35it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  92%|█████████▏| 24/26 [00:02<00:00,  9.73it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  92%|█████████▏| 24/26 [00:02<00:00,  9.73it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  96%|█████████▌| 25/26 [00:02<00:00, 10.10it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7:  96%|█████████▌| 25/26 [00:02<00:00, 10.10it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7: 100%|██████████| 26/26 [00:02<00:00, 10.48it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 7: 100%|██████████| 26/26 [00:02<00:00, 10.48it/s, v_num=8, loss/train=8.160, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 48.00it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 64.59it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 37.90it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 47.52it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 56.17it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 62.22it/s]
                                                                              Epoch 7: 100%|██████████| 26/26 [00:03<00:00,  7.78it/s, v_num=8, loss/train=8.160, loss/val=11.20]    Epoch 7: 100%|██████████| 26/26 [00:03<00:00,  7.78it/s, v_num=8, loss/train=8.160, loss/val=11.20]    Epoch 7:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.160, loss/val=11.20]             Epoch 8:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.160, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 8:   4%|▍         | 1/26 [00:01<00:43,  0.58it/s, v_num=8, loss/train=8.160, loss/val=11.20]    Epoch 8:   4%|▍         | 1/26 [00:01<00:43,  0.58it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:   8%|▊         | 2/26 [00:01<00:22,  1.07it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:   8%|▊         | 2/26 [00:01<00:22,  1.07it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  12%|█▏        | 3/26 [00:01<00:14,  1.57it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  12%|█▏        | 3/26 [00:01<00:14,  1.56it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  15%|█▌        | 4/26 [00:02<00:11,  1.94it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  15%|█▌        | 4/26 [00:02<00:11,  1.94it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  19%|█▉        | 5/26 [00:02<00:08,  2.35it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  19%|█▉        | 5/26 [00:02<00:08,  2.35it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  23%|██▎       | 6/26 [00:02<00:07,  2.74it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  23%|██▎       | 6/26 [00:02<00:07,  2.74it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  27%|██▋       | 7/26 [00:02<00:06,  3.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  27%|██▋       | 7/26 [00:02<00:06,  3.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  31%|███       | 8/26 [00:02<00:05,  3.56it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  31%|███       | 8/26 [00:02<00:05,  3.56it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  35%|███▍      | 9/26 [00:02<00:04,  3.99it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  35%|███▍      | 9/26 [00:02<00:04,  3.99it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  38%|███▊      | 10/26 [00:02<00:03,  4.41it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  38%|███▊      | 10/26 [00:02<00:03,  4.41it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  42%|████▏     | 11/26 [00:02<00:03,  4.83it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  42%|████▏     | 11/26 [00:02<00:03,  4.83it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  46%|████▌     | 12/26 [00:02<00:02,  5.22it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  46%|████▌     | 12/26 [00:02<00:02,  5.22it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  50%|█████     | 13/26 [00:02<00:02,  5.62it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  50%|█████     | 13/26 [00:02<00:02,  5.62it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  54%|█████▍    | 14/26 [00:02<00:01,  6.01it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  54%|█████▍    | 14/26 [00:02<00:01,  6.01it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  58%|█████▊    | 15/26 [00:02<00:01,  6.39it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  58%|█████▊    | 15/26 [00:02<00:01,  6.38it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  62%|██████▏   | 16/26 [00:02<00:01,  6.78it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  62%|██████▏   | 16/26 [00:02<00:01,  6.78it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  65%|██████▌   | 17/26 [00:02<00:01,  7.18it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  65%|██████▌   | 17/26 [00:02<00:01,  7.17it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  69%|██████▉   | 18/26 [00:02<00:01,  7.57it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  69%|██████▉   | 18/26 [00:02<00:01,  7.57it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  73%|███████▎  | 19/26 [00:02<00:00,  7.97it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  73%|███████▎  | 19/26 [00:02<00:00,  7.97it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  77%|███████▋  | 20/26 [00:02<00:00,  8.36it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  77%|███████▋  | 20/26 [00:02<00:00,  8.36it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  81%|████████  | 21/26 [00:02<00:00,  8.75it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  81%|████████  | 21/26 [00:02<00:00,  8.75it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  85%|████████▍ | 22/26 [00:02<00:00,  9.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  85%|████████▍ | 22/26 [00:02<00:00,  9.12it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  88%|████████▊ | 23/26 [00:02<00:00,  9.51it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 8:  88%|████████▊ | 23/26 [00:02<00:00,  9.51it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  92%|█████████▏| 24/26 [00:02<00:00,  9.89it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  92%|█████████▏| 24/26 [00:02<00:00,  9.89it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  96%|█████████▌| 25/26 [00:02<00:00, 10.26it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8:  96%|█████████▌| 25/26 [00:02<00:00, 10.26it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8: 100%|██████████| 26/26 [00:02<00:00, 10.64it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 8: 100%|██████████| 26/26 [00:02<00:00, 10.64it/s, v_num=8, loss/train=8.130, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 26.12it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 29.59it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 41.09it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 51.44it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 60.85it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 66.87it/s]
                                                                              Epoch 8: 100%|██████████| 26/26 [00:03<00:00,  7.84it/s, v_num=8, loss/train=8.130, loss/val=11.20]    Epoch 8: 100%|██████████| 26/26 [00:03<00:00,  7.83it/s, v_num=8, loss/train=8.130, loss/val=11.20]    Epoch 8:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.130, loss/val=11.20]             Epoch 9:   0%|          | 0/26 [00:00<?, ?it/s, v_num=8, loss/train=8.130, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 9:   4%|▍         | 1/26 [00:02<00:50,  0.49it/s, v_num=8, loss/train=8.130, loss/val=11.20]    Epoch 9:   4%|▍         | 1/26 [00:02<00:50,  0.49it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:   8%|▊         | 2/26 [00:02<00:26,  0.90it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:   8%|▊         | 2/26 [00:02<00:26,  0.90it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  12%|█▏        | 3/26 [00:02<00:17,  1.34it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  12%|█▏        | 3/26 [00:02<00:17,  1.34it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  15%|█▌        | 4/26 [00:02<00:12,  1.76it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  15%|█▌        | 4/26 [00:02<00:12,  1.76it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  19%|█▉        | 5/26 [00:02<00:09,  2.18it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  19%|█▉        | 5/26 [00:02<00:09,  2.18it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  23%|██▎       | 6/26 [00:02<00:07,  2.58it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  23%|██▎       | 6/26 [00:02<00:07,  2.58it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  27%|██▋       | 7/26 [00:02<00:06,  2.98it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  27%|██▋       | 7/26 [00:02<00:06,  2.97it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  31%|███       | 8/26 [00:02<00:05,  3.36it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  31%|███       | 8/26 [00:02<00:05,  3.35it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  35%|███▍      | 9/26 [00:02<00:04,  3.75it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  35%|███▍      | 9/26 [00:02<00:04,  3.75it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  38%|███▊      | 10/26 [00:02<00:03,  4.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  38%|███▊      | 10/26 [00:02<00:03,  4.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  42%|████▏     | 11/26 [00:02<00:03,  4.51it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  42%|████▏     | 11/26 [00:02<00:03,  4.51it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  46%|████▌     | 12/26 [00:02<00:02,  4.90it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  46%|████▌     | 12/26 [00:02<00:02,  4.90it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  50%|█████     | 13/26 [00:02<00:02,  5.29it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  50%|█████     | 13/26 [00:02<00:02,  5.29it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  54%|█████▍    | 14/26 [00:02<00:02,  5.67it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  54%|█████▍    | 14/26 [00:02<00:02,  5.67it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  58%|█████▊    | 15/26 [00:02<00:01,  6.06it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  58%|█████▊    | 15/26 [00:02<00:01,  6.05it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  62%|██████▏   | 16/26 [00:02<00:01,  6.44it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  62%|██████▏   | 16/26 [00:02<00:01,  6.44it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  65%|██████▌   | 17/26 [00:02<00:01,  6.81it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  65%|██████▌   | 17/26 [00:02<00:01,  6.81it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  69%|██████▉   | 18/26 [00:02<00:01,  7.17it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  69%|██████▉   | 18/26 [00:02<00:01,  7.17it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  73%|███████▎  | 19/26 [00:02<00:00,  7.54it/s, v_num=8, loss/train=11.30, loss/val=11.20]    Epoch 9:  73%|███████▎  | 19/26 [00:02<00:00,  7.54it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  77%|███████▋  | 20/26 [00:02<00:00,  7.92it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  77%|███████▋  | 20/26 [00:02<00:00,  7.91it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  81%|████████  | 21/26 [00:02<00:00,  8.29it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  81%|████████  | 21/26 [00:02<00:00,  8.28it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  85%|████████▍ | 22/26 [00:02<00:00,  8.64it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  85%|████████▍ | 22/26 [00:02<00:00,  8.64it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  88%|████████▊ | 23/26 [00:02<00:00,  8.98it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  88%|████████▊ | 23/26 [00:02<00:00,  8.98it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  92%|█████████▏| 24/26 [00:02<00:00,  9.34it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  92%|█████████▏| 24/26 [00:02<00:00,  9.34it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  96%|█████████▌| 25/26 [00:02<00:00,  9.70it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9:  96%|█████████▌| 25/26 [00:02<00:00,  9.70it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9: 100%|██████████| 26/26 [00:02<00:00, 10.06it/s, v_num=8, loss/train=11.20, loss/val=11.20]    Epoch 9: 100%|██████████| 26/26 [00:02<00:00, 10.06it/s, v_num=8, loss/train=8.080, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 70.69it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 102.41it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 123.21it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 137.28it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 147.45it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 143.85it/s]
                                                                               Epoch 9: 100%|██████████| 26/26 [00:03<00:00,  7.48it/s, v_num=8, loss/train=8.080, loss/val=11.20]    Epoch 9: 100%|██████████| 26/26 [00:03<00:00,  7.48it/s, v_num=8, loss/train=8.080, loss/val=11.20]    Epoch 9: 100%|██████████| 26/26 [00:03<00:00,  7.48it/s, v_num=8, loss/train=8.080, loss/val=11.20]
    Sanity Checking: |          | 0/? [00:00<?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Sanity Checking: |          | 0/? [00:00<?, ?it/s]    Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]    Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 44.43it/s]    Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 20.63it/s]                                                                               /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
    Training: |          | 0/? [00:00<?, ?it/s]    Training: |          | 0/? [00:00<?, ?it/s]    Epoch 0:   0%|          | 0/26 [00:00<?, ?it/s]    Epoch 0:   4%|▍         | 1/26 [00:01<00:37,  0.67it/s]    Epoch 0:   4%|▍         | 1/26 [00:01<00:37,  0.67it/s, v_num=9, loss/train=12.90]    Epoch 0:   8%|▊         | 2/26 [00:01<00:18,  1.29it/s, v_num=9, loss/train=12.90]    Epoch 0:   8%|▊         | 2/26 [00:01<00:18,  1.29it/s, v_num=9, loss/train=12.90]    Epoch 0:  12%|█▏        | 3/26 [00:01<00:12,  1.90it/s, v_num=9, loss/train=12.90]    Epoch 0:  12%|█▏        | 3/26 [00:01<00:12,  1.90it/s, v_num=9, loss/train=13.00]    Epoch 0:  15%|█▌        | 4/26 [00:01<00:08,  2.49it/s, v_num=9, loss/train=13.00]    Epoch 0:  15%|█▌        | 4/26 [00:01<00:08,  2.49it/s, v_num=9, loss/train=13.00]    Epoch 0:  19%|█▉        | 5/26 [00:01<00:06,  3.06it/s, v_num=9, loss/train=13.00]    Epoch 0:  19%|█▉        | 5/26 [00:01<00:06,  3.06it/s, v_num=9, loss/train=12.90]    Epoch 0:  23%|██▎       | 6/26 [00:01<00:05,  3.52it/s, v_num=9, loss/train=12.90]    Epoch 0:  23%|██▎       | 6/26 [00:01<00:05,  3.52it/s, v_num=9, loss/train=12.90]    Epoch 0:  27%|██▋       | 7/26 [00:01<00:04,  4.04it/s, v_num=9, loss/train=12.90]    Epoch 0:  27%|██▋       | 7/26 [00:01<00:04,  4.04it/s, v_num=9, loss/train=12.80]    Epoch 0:  31%|███       | 8/26 [00:01<00:03,  4.57it/s, v_num=9, loss/train=12.80]    Epoch 0:  31%|███       | 8/26 [00:01<00:03,  4.57it/s, v_num=9, loss/train=12.80]    Epoch 0:  35%|███▍      | 9/26 [00:01<00:03,  5.11it/s, v_num=9, loss/train=12.80]    Epoch 0:  35%|███▍      | 9/26 [00:01<00:03,  5.11it/s, v_num=9, loss/train=12.80]    Epoch 0:  38%|███▊      | 10/26 [00:01<00:02,  5.64it/s, v_num=9, loss/train=12.80]    Epoch 0:  38%|███▊      | 10/26 [00:01<00:02,  5.64it/s, v_num=9, loss/train=12.90]    Epoch 0:  42%|████▏     | 11/26 [00:01<00:02,  6.13it/s, v_num=9, loss/train=12.90]    Epoch 0:  42%|████▏     | 11/26 [00:01<00:02,  6.13it/s, v_num=9, loss/train=12.80]    Epoch 0:  46%|████▌     | 12/26 [00:01<00:02,  6.64it/s, v_num=9, loss/train=12.80]    Epoch 0:  46%|████▌     | 12/26 [00:01<00:02,  6.64it/s, v_num=9, loss/train=12.90]    Epoch 0:  50%|█████     | 13/26 [00:01<00:01,  7.17it/s, v_num=9, loss/train=12.90]    Epoch 0:  50%|█████     | 13/26 [00:01<00:01,  7.17it/s, v_num=9, loss/train=12.60]    Epoch 0:  54%|█████▍    | 14/26 [00:01<00:01,  7.69it/s, v_num=9, loss/train=12.60]    Epoch 0:  54%|█████▍    | 14/26 [00:01<00:01,  7.69it/s, v_num=9, loss/train=12.80]    Epoch 0:  58%|█████▊    | 15/26 [00:01<00:01,  8.20it/s, v_num=9, loss/train=12.80]    Epoch 0:  58%|█████▊    | 15/26 [00:01<00:01,  8.20it/s, v_num=9, loss/train=12.60]    Epoch 0:  62%|██████▏   | 16/26 [00:01<00:01,  8.71it/s, v_num=9, loss/train=12.60]    Epoch 0:  62%|██████▏   | 16/26 [00:01<00:01,  8.71it/s, v_num=9, loss/train=12.70]    Epoch 0:  65%|██████▌   | 17/26 [00:01<00:00,  9.22it/s, v_num=9, loss/train=12.70]    Epoch 0:  65%|██████▌   | 17/26 [00:01<00:00,  9.22it/s, v_num=9, loss/train=12.60]    Epoch 0:  69%|██████▉   | 18/26 [00:01<00:00,  9.73it/s, v_num=9, loss/train=12.60]    Epoch 0:  69%|██████▉   | 18/26 [00:01<00:00,  9.72it/s, v_num=9, loss/train=12.60]    Epoch 0:  73%|███████▎  | 19/26 [00:01<00:00, 10.22it/s, v_num=9, loss/train=12.60]    Epoch 0:  73%|███████▎  | 19/26 [00:01<00:00, 10.22it/s, v_num=9, loss/train=12.80]    Epoch 0:  77%|███████▋  | 20/26 [00:01<00:00, 10.72it/s, v_num=9, loss/train=12.80]    Epoch 0:  77%|███████▋  | 20/26 [00:01<00:00, 10.71it/s, v_num=9, loss/train=12.70]    Epoch 0:  81%|████████  | 21/26 [00:01<00:00, 11.18it/s, v_num=9, loss/train=12.70]    Epoch 0:  81%|████████  | 21/26 [00:01<00:00, 11.18it/s, v_num=9, loss/train=12.80]    Epoch 0:  85%|████████▍ | 22/26 [00:01<00:00, 11.67it/s, v_num=9, loss/train=12.80]    Epoch 0:  85%|████████▍ | 22/26 [00:01<00:00, 11.67it/s, v_num=9, loss/train=12.80]    Epoch 0:  88%|████████▊ | 23/26 [00:01<00:00, 12.15it/s, v_num=9, loss/train=12.80]    Epoch 0:  88%|████████▊ | 23/26 [00:01<00:00, 12.14it/s, v_num=9, loss/train=12.70]    Epoch 0:  92%|█████████▏| 24/26 [00:01<00:00, 12.61it/s, v_num=9, loss/train=12.70]    Epoch 0:  92%|█████████▏| 24/26 [00:01<00:00, 12.61it/s, v_num=9, loss/train=12.60]    Epoch 0:  96%|█████████▌| 25/26 [00:01<00:00, 13.08it/s, v_num=9, loss/train=12.60]    Epoch 0:  96%|█████████▌| 25/26 [00:01<00:00, 13.07it/s, v_num=9, loss/train=12.60]    Epoch 0: 100%|██████████| 26/26 [00:01<00:00, 13.56it/s, v_num=9, loss/train=12.60]    Epoch 0: 100%|██████████| 26/26 [00:01<00:00, 13.55it/s, v_num=9, loss/train=9.700]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 77.90it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 111.24it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 132.34it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 145.82it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 156.14it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 152.18it/s]
                                                                               Epoch 0: 100%|██████████| 26/26 [00:02<00:00,  9.38it/s, v_num=9, loss/train=9.700, loss/val=12.60]    Epoch 0: 100%|██████████| 26/26 [00:02<00:00,  9.37it/s, v_num=9, loss/train=9.700, loss/val=12.60]    Epoch 0:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=9.700, loss/val=12.60]             Epoch 1:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=9.700, loss/val=12.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 1:   4%|▍         | 1/26 [00:01<00:42,  0.59it/s, v_num=9, loss/train=9.700, loss/val=12.60]    Epoch 1:   4%|▍         | 1/26 [00:01<00:42,  0.58it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:   8%|▊         | 2/26 [00:02<00:24,  0.98it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:   8%|▊         | 2/26 [00:02<00:24,  0.98it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  12%|█▏        | 3/26 [00:02<00:15,  1.46it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  12%|█▏        | 3/26 [00:02<00:15,  1.46it/s, v_num=9, loss/train=12.70, loss/val=12.60]    Epoch 1:  15%|█▌        | 4/26 [00:02<00:11,  1.93it/s, v_num=9, loss/train=12.70, loss/val=12.60]    Epoch 1:  15%|█▌        | 4/26 [00:02<00:11,  1.93it/s, v_num=9, loss/train=12.70, loss/val=12.60]    Epoch 1:  19%|█▉        | 5/26 [00:02<00:08,  2.38it/s, v_num=9, loss/train=12.70, loss/val=12.60]    Epoch 1:  19%|█▉        | 5/26 [00:02<00:08,  2.38it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  23%|██▎       | 6/26 [00:02<00:07,  2.83it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  23%|██▎       | 6/26 [00:02<00:07,  2.83it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  27%|██▋       | 7/26 [00:02<00:05,  3.18it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  27%|██▋       | 7/26 [00:02<00:05,  3.18it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  31%|███       | 8/26 [00:02<00:04,  3.61it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  31%|███       | 8/26 [00:02<00:04,  3.61it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  35%|███▍      | 9/26 [00:02<00:04,  4.05it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  35%|███▍      | 9/26 [00:02<00:04,  4.05it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  38%|███▊      | 10/26 [00:02<00:03,  4.48it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  38%|███▊      | 10/26 [00:02<00:03,  4.48it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  42%|████▏     | 11/26 [00:02<00:03,  4.90it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  42%|████▏     | 11/26 [00:02<00:03,  4.90it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  46%|████▌     | 12/26 [00:02<00:02,  5.31it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  46%|████▌     | 12/26 [00:02<00:02,  5.31it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  50%|█████     | 13/26 [00:02<00:02,  5.73it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  50%|█████     | 13/26 [00:02<00:02,  5.73it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  54%|█████▍    | 14/26 [00:02<00:01,  6.15it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  54%|█████▍    | 14/26 [00:02<00:01,  6.15it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  58%|█████▊    | 15/26 [00:02<00:01,  6.55it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  58%|█████▊    | 15/26 [00:02<00:01,  6.55it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  62%|██████▏   | 16/26 [00:02<00:01,  6.96it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  62%|██████▏   | 16/26 [00:02<00:01,  6.96it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  65%|██████▌   | 17/26 [00:02<00:01,  7.37it/s, v_num=9, loss/train=12.60, loss/val=12.60]    Epoch 1:  65%|██████▌   | 17/26 [00:02<00:01,  7.37it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  69%|██████▉   | 18/26 [00:02<00:01,  7.78it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  69%|██████▉   | 18/26 [00:02<00:01,  7.78it/s, v_num=9, loss/train=12.30, loss/val=12.60]    Epoch 1:  73%|███████▎  | 19/26 [00:02<00:00,  8.19it/s, v_num=9, loss/train=12.30, loss/val=12.60]    Epoch 1:  73%|███████▎  | 19/26 [00:02<00:00,  8.19it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  77%|███████▋  | 20/26 [00:02<00:00,  8.59it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  77%|███████▋  | 20/26 [00:02<00:00,  8.59it/s, v_num=9, loss/train=12.30, loss/val=12.60]    Epoch 1:  81%|████████  | 21/26 [00:02<00:00,  8.99it/s, v_num=9, loss/train=12.30, loss/val=12.60]    Epoch 1:  81%|████████  | 21/26 [00:02<00:00,  8.99it/s, v_num=9, loss/train=12.40, loss/val=12.60]    Epoch 1:  85%|████████▍ | 22/26 [00:02<00:00,  9.38it/s, v_num=9, loss/train=12.40, loss/val=12.60]    Epoch 1:  85%|████████▍ | 22/26 [00:02<00:00,  9.38it/s, v_num=9, loss/train=12.30, loss/val=12.60]    Epoch 1:  88%|████████▊ | 23/26 [00:02<00:00,  9.76it/s, v_num=9, loss/train=12.30, loss/val=12.60]    Epoch 1:  88%|████████▊ | 23/26 [00:02<00:00,  9.76it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  92%|█████████▏| 24/26 [00:02<00:00, 10.15it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  92%|█████████▏| 24/26 [00:02<00:00, 10.15it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  96%|█████████▌| 25/26 [00:02<00:00, 10.53it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1:  96%|█████████▌| 25/26 [00:02<00:00, 10.52it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1: 100%|██████████| 26/26 [00:02<00:00, 10.92it/s, v_num=9, loss/train=12.50, loss/val=12.60]    Epoch 1: 100%|██████████| 26/26 [00:02<00:00, 10.92it/s, v_num=9, loss/train=9.300, loss/val=12.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 23.37it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 17.35it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 25.02it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 31.58it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 38.14it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 43.36it/s]
                                                                              Epoch 1: 100%|██████████| 26/26 [00:03<00:00,  8.02it/s, v_num=9, loss/train=9.300, loss/val=12.40]    Epoch 1: 100%|██████████| 26/26 [00:03<00:00,  8.02it/s, v_num=9, loss/train=9.300, loss/val=12.40]    Epoch 1:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=9.300, loss/val=12.40]             Epoch 2:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=9.300, loss/val=12.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 2:   4%|▍         | 1/26 [00:01<00:36,  0.69it/s, v_num=9, loss/train=9.300, loss/val=12.40]    Epoch 2:   4%|▍         | 1/26 [00:01<00:36,  0.69it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:   8%|▊         | 2/26 [00:01<00:23,  1.04it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:   8%|▊         | 2/26 [00:01<00:23,  1.04it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  12%|█▏        | 3/26 [00:01<00:15,  1.51it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  12%|█▏        | 3/26 [00:01<00:15,  1.51it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  15%|█▌        | 4/26 [00:02<00:11,  1.97it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  15%|█▌        | 4/26 [00:02<00:11,  1.97it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  19%|█▉        | 5/26 [00:02<00:08,  2.42it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  19%|█▉        | 5/26 [00:02<00:08,  2.42it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  23%|██▎       | 6/26 [00:02<00:06,  2.86it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  23%|██▎       | 6/26 [00:02<00:06,  2.86it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  27%|██▋       | 7/26 [00:02<00:05,  3.29it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  27%|██▋       | 7/26 [00:02<00:05,  3.29it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  31%|███       | 8/26 [00:02<00:04,  3.71it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  31%|███       | 8/26 [00:02<00:04,  3.71it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  35%|███▍      | 9/26 [00:02<00:04,  4.15it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  35%|███▍      | 9/26 [00:02<00:04,  4.15it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  38%|███▊      | 10/26 [00:02<00:03,  4.58it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  38%|███▊      | 10/26 [00:02<00:03,  4.58it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  42%|████▏     | 11/26 [00:02<00:02,  5.01it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  42%|████▏     | 11/26 [00:02<00:02,  5.01it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  46%|████▌     | 12/26 [00:02<00:02,  5.39it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  46%|████▌     | 12/26 [00:02<00:02,  5.39it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  50%|█████     | 13/26 [00:02<00:02,  5.81it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  50%|█████     | 13/26 [00:02<00:02,  5.81it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  54%|█████▍    | 14/26 [00:02<00:01,  6.24it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  54%|█████▍    | 14/26 [00:02<00:01,  6.24it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  58%|█████▊    | 15/26 [00:02<00:01,  6.66it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2:  58%|█████▊    | 15/26 [00:02<00:01,  6.66it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  62%|██████▏   | 16/26 [00:02<00:01,  7.08it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  62%|██████▏   | 16/26 [00:02<00:01,  7.08it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  65%|██████▌   | 17/26 [00:02<00:01,  7.50it/s, v_num=9, loss/train=12.50, loss/val=12.40]    Epoch 2:  65%|██████▌   | 17/26 [00:02<00:01,  7.50it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  69%|██████▉   | 18/26 [00:02<00:01,  7.92it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  69%|██████▉   | 18/26 [00:02<00:01,  7.92it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  73%|███████▎  | 19/26 [00:02<00:00,  8.33it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  73%|███████▎  | 19/26 [00:02<00:00,  8.33it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  77%|███████▋  | 20/26 [00:02<00:00,  8.74it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  77%|███████▋  | 20/26 [00:02<00:00,  8.74it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  81%|████████  | 21/26 [00:02<00:00,  9.15it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  81%|████████  | 21/26 [00:02<00:00,  9.15it/s, v_num=9, loss/train=12.20, loss/val=12.40]    Epoch 2:  85%|████████▍ | 22/26 [00:02<00:00,  9.53it/s, v_num=9, loss/train=12.20, loss/val=12.40]    Epoch 2:  85%|████████▍ | 22/26 [00:02<00:00,  9.53it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  88%|████████▊ | 23/26 [00:02<00:00,  9.93it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  88%|████████▊ | 23/26 [00:02<00:00,  9.93it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  92%|█████████▏| 24/26 [00:02<00:00, 10.33it/s, v_num=9, loss/train=12.30, loss/val=12.40]    Epoch 2:  92%|█████████▏| 24/26 [00:02<00:00, 10.33it/s, v_num=9, loss/train=12.20, loss/val=12.40]    Epoch 2:  96%|█████████▌| 25/26 [00:02<00:00, 10.73it/s, v_num=9, loss/train=12.20, loss/val=12.40]    Epoch 2:  96%|█████████▌| 25/26 [00:02<00:00, 10.73it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2: 100%|██████████| 26/26 [00:02<00:00, 11.13it/s, v_num=9, loss/train=12.40, loss/val=12.40]    Epoch 2: 100%|██████████| 26/26 [00:02<00:00, 11.13it/s, v_num=9, loss/train=8.850, loss/val=12.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 78.31it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 117.02it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 142.20it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 160.15it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 173.87it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 168.75it/s]
                                                                               Epoch 2: 100%|██████████| 26/26 [00:03<00:00,  8.16it/s, v_num=9, loss/train=8.850, loss/val=12.20]    Epoch 2: 100%|██████████| 26/26 [00:03<00:00,  8.15it/s, v_num=9, loss/train=8.850, loss/val=12.20]    Epoch 2:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.850, loss/val=12.20]             Epoch 3:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.850, loss/val=12.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 3:   4%|▍         | 1/26 [00:01<00:46,  0.53it/s, v_num=9, loss/train=8.850, loss/val=12.20]    Epoch 3:   4%|▍         | 1/26 [00:01<00:47,  0.53it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:   8%|▊         | 2/26 [00:01<00:23,  1.02it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:   8%|▊         | 2/26 [00:01<00:23,  1.02it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  12%|█▏        | 3/26 [00:02<00:15,  1.48it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  12%|█▏        | 3/26 [00:02<00:15,  1.48it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  15%|█▌        | 4/26 [00:02<00:11,  1.95it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  15%|█▌        | 4/26 [00:02<00:11,  1.94it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  19%|█▉        | 5/26 [00:02<00:08,  2.38it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  19%|█▉        | 5/26 [00:02<00:08,  2.38it/s, v_num=9, loss/train=12.30, loss/val=12.20]    Epoch 3:  23%|██▎       | 6/26 [00:02<00:07,  2.81it/s, v_num=9, loss/train=12.30, loss/val=12.20]    Epoch 3:  23%|██▎       | 6/26 [00:02<00:07,  2.81it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  27%|██▋       | 7/26 [00:02<00:05,  3.23it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  27%|██▋       | 7/26 [00:02<00:05,  3.23it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  31%|███       | 8/26 [00:02<00:04,  3.64it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  31%|███       | 8/26 [00:02<00:04,  3.64it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  35%|███▍      | 9/26 [00:02<00:04,  4.08it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  35%|███▍      | 9/26 [00:02<00:04,  4.07it/s, v_num=9, loss/train=12.30, loss/val=12.20]    Epoch 3:  38%|███▊      | 10/26 [00:02<00:03,  4.51it/s, v_num=9, loss/train=12.30, loss/val=12.20]    Epoch 3:  38%|███▊      | 10/26 [00:02<00:03,  4.51it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  42%|████▏     | 11/26 [00:02<00:03,  4.94it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  42%|████▏     | 11/26 [00:02<00:03,  4.94it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  46%|████▌     | 12/26 [00:02<00:02,  5.34it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  46%|████▌     | 12/26 [00:02<00:02,  5.34it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  50%|█████     | 13/26 [00:02<00:02,  5.76it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  50%|█████     | 13/26 [00:02<00:02,  5.76it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  54%|█████▍    | 14/26 [00:02<00:01,  6.18it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  54%|█████▍    | 14/26 [00:02<00:01,  6.18it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  58%|█████▊    | 15/26 [00:02<00:01,  6.60it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  58%|█████▊    | 15/26 [00:02<00:01,  6.60it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  62%|██████▏   | 16/26 [00:02<00:01,  7.02it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  62%|██████▏   | 16/26 [00:02<00:01,  7.01it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  65%|██████▌   | 17/26 [00:02<00:01,  7.43it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  65%|██████▌   | 17/26 [00:02<00:01,  7.43it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  69%|██████▉   | 18/26 [00:02<00:01,  7.84it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  69%|██████▉   | 18/26 [00:02<00:01,  7.84it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  73%|███████▎  | 19/26 [00:02<00:00,  8.25it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  73%|███████▎  | 19/26 [00:02<00:00,  8.25it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  77%|███████▋  | 20/26 [00:02<00:00,  8.66it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  77%|███████▋  | 20/26 [00:02<00:00,  8.66it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  81%|████████  | 21/26 [00:02<00:00,  9.05it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  81%|████████  | 21/26 [00:02<00:00,  9.05it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  85%|████████▍ | 22/26 [00:02<00:00,  9.44it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  85%|████████▍ | 22/26 [00:02<00:00,  9.43it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  88%|████████▊ | 23/26 [00:02<00:00,  9.83it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3:  88%|████████▊ | 23/26 [00:02<00:00,  9.83it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  92%|█████████▏| 24/26 [00:02<00:00, 10.23it/s, v_num=9, loss/train=12.10, loss/val=12.20]    Epoch 3:  92%|█████████▏| 24/26 [00:02<00:00, 10.23it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  96%|█████████▌| 25/26 [00:02<00:00, 10.61it/s, v_num=9, loss/train=12.20, loss/val=12.20]    Epoch 3:  96%|█████████▌| 25/26 [00:02<00:00, 10.61it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3: 100%|██████████| 26/26 [00:02<00:00, 11.01it/s, v_num=9, loss/train=12.00, loss/val=12.20]    Epoch 3: 100%|██████████| 26/26 [00:02<00:00, 11.01it/s, v_num=9, loss/train=8.820, loss/val=12.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 37.98it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 14.63it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 21.04it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 26.81it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 32.08it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 36.78it/s]
                                                                              Epoch 3: 100%|██████████| 26/26 [00:03<00:00,  8.06it/s, v_num=9, loss/train=8.820, loss/val=12.00]    Epoch 3: 100%|██████████| 26/26 [00:03<00:00,  8.06it/s, v_num=9, loss/train=8.820, loss/val=12.00]    Epoch 3:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.820, loss/val=12.00]             Epoch 4:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.820, loss/val=12.00]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 4:   4%|▍         | 1/26 [00:01<00:48,  0.51it/s, v_num=9, loss/train=8.820, loss/val=12.00]    Epoch 4:   4%|▍         | 1/26 [00:01<00:48,  0.51it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:   8%|▊         | 2/26 [00:01<00:23,  1.01it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:   8%|▊         | 2/26 [00:01<00:23,  1.01it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  12%|█▏        | 3/26 [00:02<00:15,  1.48it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  12%|█▏        | 3/26 [00:02<00:15,  1.48it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  15%|█▌        | 4/26 [00:02<00:11,  1.96it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  15%|█▌        | 4/26 [00:02<00:11,  1.96it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  19%|█▉        | 5/26 [00:02<00:08,  2.39it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  19%|█▉        | 5/26 [00:02<00:08,  2.39it/s, v_num=9, loss/train=12.20, loss/val=12.00]    Epoch 4:  23%|██▎       | 6/26 [00:02<00:07,  2.83it/s, v_num=9, loss/train=12.20, loss/val=12.00]    Epoch 4:  23%|██▎       | 6/26 [00:02<00:07,  2.83it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  27%|██▋       | 7/26 [00:02<00:05,  3.23it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  27%|██▋       | 7/26 [00:02<00:05,  3.22it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  31%|███       | 8/26 [00:02<00:04,  3.66it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  31%|███       | 8/26 [00:02<00:04,  3.66it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  35%|███▍      | 9/26 [00:02<00:04,  4.09it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  35%|███▍      | 9/26 [00:02<00:04,  4.08it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  38%|███▊      | 10/26 [00:02<00:03,  4.50it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  38%|███▊      | 10/26 [00:02<00:03,  4.50it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  42%|████▏     | 11/26 [00:02<00:03,  4.86it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  42%|████▏     | 11/26 [00:02<00:03,  4.86it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  46%|████▌     | 12/26 [00:02<00:02,  5.28it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  46%|████▌     | 12/26 [00:02<00:02,  5.28it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  50%|█████     | 13/26 [00:02<00:02,  5.68it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  50%|█████     | 13/26 [00:02<00:02,  5.68it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  54%|█████▍    | 14/26 [00:02<00:01,  6.10it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  54%|█████▍    | 14/26 [00:02<00:01,  6.10it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  58%|█████▊    | 15/26 [00:02<00:01,  6.51it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  58%|█████▊    | 15/26 [00:02<00:01,  6.51it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  62%|██████▏   | 16/26 [00:02<00:01,  6.92it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  62%|██████▏   | 16/26 [00:02<00:01,  6.92it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  65%|██████▌   | 17/26 [00:02<00:01,  7.32it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  65%|██████▌   | 17/26 [00:02<00:01,  7.32it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  69%|██████▉   | 18/26 [00:02<00:01,  7.73it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4:  69%|██████▉   | 18/26 [00:02<00:01,  7.73it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  73%|███████▎  | 19/26 [00:02<00:00,  8.13it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  73%|███████▎  | 19/26 [00:02<00:00,  8.13it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  77%|███████▋  | 20/26 [00:02<00:00,  8.53it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  77%|███████▋  | 20/26 [00:02<00:00,  8.53it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  81%|████████  | 21/26 [00:02<00:00,  8.92it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  81%|████████  | 21/26 [00:02<00:00,  8.92it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  85%|████████▍ | 22/26 [00:02<00:00,  9.32it/s, v_num=9, loss/train=12.10, loss/val=12.00]    Epoch 4:  85%|████████▍ | 22/26 [00:02<00:00,  9.31it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  88%|████████▊ | 23/26 [00:02<00:00,  9.69it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  88%|████████▊ | 23/26 [00:02<00:00,  9.69it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  92%|█████████▏| 24/26 [00:02<00:00, 10.08it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  92%|█████████▏| 24/26 [00:02<00:00, 10.08it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  96%|█████████▌| 25/26 [00:02<00:00, 10.47it/s, v_num=9, loss/train=12.00, loss/val=12.00]    Epoch 4:  96%|█████████▌| 25/26 [00:02<00:00, 10.46it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4: 100%|██████████| 26/26 [00:02<00:00, 10.86it/s, v_num=9, loss/train=11.90, loss/val=12.00]    Epoch 4: 100%|██████████| 26/26 [00:02<00:00, 10.86it/s, v_num=9, loss/train=8.880, loss/val=12.00]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 80.66it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 99.34it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 124.84it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 143.45it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 157.64it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 155.72it/s]
                                                                               Epoch 4: 100%|██████████| 26/26 [00:03<00:00,  8.06it/s, v_num=9, loss/train=8.880, loss/val=11.90]    Epoch 4: 100%|██████████| 26/26 [00:03<00:00,  8.06it/s, v_num=9, loss/train=8.880, loss/val=11.90]    Epoch 4:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.880, loss/val=11.90]             Epoch 5:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.880, loss/val=11.90]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 5:   4%|▍         | 1/26 [00:01<00:45,  0.55it/s, v_num=9, loss/train=8.880, loss/val=11.90]    Epoch 5:   4%|▍         | 1/26 [00:01<00:45,  0.54it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:   8%|▊         | 2/26 [00:01<00:23,  1.04it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:   8%|▊         | 2/26 [00:01<00:23,  1.04it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  12%|█▏        | 3/26 [00:01<00:15,  1.53it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  12%|█▏        | 3/26 [00:01<00:15,  1.53it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  15%|█▌        | 4/26 [00:02<00:11,  1.98it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  15%|█▌        | 4/26 [00:02<00:11,  1.98it/s, v_num=9, loss/train=12.00, loss/val=11.90]    Epoch 5:  19%|█▉        | 5/26 [00:02<00:08,  2.43it/s, v_num=9, loss/train=12.00, loss/val=11.90]    Epoch 5:  19%|█▉        | 5/26 [00:02<00:08,  2.43it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  23%|██▎       | 6/26 [00:02<00:06,  2.89it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  23%|██▎       | 6/26 [00:02<00:06,  2.89it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  27%|██▋       | 7/26 [00:02<00:05,  3.33it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  27%|██▋       | 7/26 [00:02<00:05,  3.33it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  31%|███       | 8/26 [00:02<00:04,  3.77it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  31%|███       | 8/26 [00:02<00:04,  3.77it/s, v_num=9, loss/train=12.10, loss/val=11.90]    Epoch 5:  35%|███▍      | 9/26 [00:02<00:04,  4.21it/s, v_num=9, loss/train=12.10, loss/val=11.90]    Epoch 5:  35%|███▍      | 9/26 [00:02<00:04,  4.21it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  38%|███▊      | 10/26 [00:02<00:03,  4.63it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  38%|███▊      | 10/26 [00:02<00:03,  4.62it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  42%|████▏     | 11/26 [00:02<00:03,  4.95it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  42%|████▏     | 11/26 [00:02<00:03,  4.95it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  46%|████▌     | 12/26 [00:02<00:02,  5.37it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  46%|████▌     | 12/26 [00:02<00:02,  5.37it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  50%|█████     | 13/26 [00:02<00:02,  5.80it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  50%|█████     | 13/26 [00:02<00:02,  5.80it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  54%|█████▍    | 14/26 [00:02<00:01,  6.22it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  54%|█████▍    | 14/26 [00:02<00:01,  6.22it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  58%|█████▊    | 15/26 [00:02<00:01,  6.65it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  58%|█████▊    | 15/26 [00:02<00:01,  6.65it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  62%|██████▏   | 16/26 [00:02<00:01,  7.07it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  62%|██████▏   | 16/26 [00:02<00:01,  7.07it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  65%|██████▌   | 17/26 [00:02<00:01,  7.47it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  65%|██████▌   | 17/26 [00:02<00:01,  7.47it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  69%|██████▉   | 18/26 [00:02<00:01,  7.89it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  69%|██████▉   | 18/26 [00:02<00:01,  7.89it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  73%|███████▎  | 19/26 [00:02<00:00,  8.30it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  73%|███████▎  | 19/26 [00:02<00:00,  8.30it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  77%|███████▋  | 20/26 [00:02<00:00,  8.71it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  77%|███████▋  | 20/26 [00:02<00:00,  8.71it/s, v_num=9, loss/train=12.00, loss/val=11.90]    Epoch 5:  81%|████████  | 21/26 [00:02<00:00,  9.09it/s, v_num=9, loss/train=12.00, loss/val=11.90]    Epoch 5:  81%|████████  | 21/26 [00:02<00:00,  9.09it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  85%|████████▍ | 22/26 [00:02<00:00,  9.50it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  85%|████████▍ | 22/26 [00:02<00:00,  9.50it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  88%|████████▊ | 23/26 [00:02<00:00,  9.90it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  88%|████████▊ | 23/26 [00:02<00:00,  9.89it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  92%|█████████▏| 24/26 [00:02<00:00, 10.30it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5:  92%|█████████▏| 24/26 [00:02<00:00, 10.29it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  96%|█████████▌| 25/26 [00:02<00:00, 10.69it/s, v_num=9, loss/train=11.80, loss/val=11.90]    Epoch 5:  96%|█████████▌| 25/26 [00:02<00:00, 10.68it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5: 100%|██████████| 26/26 [00:02<00:00, 11.09it/s, v_num=9, loss/train=11.90, loss/val=11.90]    Epoch 5: 100%|██████████| 26/26 [00:02<00:00, 11.08it/s, v_num=9, loss/train=8.460, loss/val=11.90]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 34.38it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 13.51it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 19.57it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 25.27it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 30.68it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 35.03it/s]
                                                                              Epoch 5: 100%|██████████| 26/26 [00:03<00:00,  8.13it/s, v_num=9, loss/train=8.460, loss/val=11.80]    Epoch 5: 100%|██████████| 26/26 [00:03<00:00,  8.12it/s, v_num=9, loss/train=8.460, loss/val=11.80]    Epoch 5:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.460, loss/val=11.80]             Epoch 6:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.460, loss/val=11.80]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 6:   4%|▍         | 1/26 [00:01<00:45,  0.55it/s, v_num=9, loss/train=8.460, loss/val=11.80]    Epoch 6:   4%|▍         | 1/26 [00:01<00:45,  0.54it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:   8%|▊         | 2/26 [00:02<00:24,  0.99it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:   8%|▊         | 2/26 [00:02<00:24,  0.98it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  12%|█▏        | 3/26 [00:02<00:15,  1.44it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  12%|█▏        | 3/26 [00:02<00:15,  1.44it/s, v_num=9, loss/train=11.90, loss/val=11.80]    Epoch 6:  15%|█▌        | 4/26 [00:02<00:11,  1.90it/s, v_num=9, loss/train=11.90, loss/val=11.80]    Epoch 6:  15%|█▌        | 4/26 [00:02<00:11,  1.90it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  19%|█▉        | 5/26 [00:02<00:09,  2.33it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  19%|█▉        | 5/26 [00:02<00:09,  2.33it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  23%|██▎       | 6/26 [00:02<00:07,  2.74it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  23%|██▎       | 6/26 [00:02<00:07,  2.74it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  27%|██▋       | 7/26 [00:02<00:06,  3.16it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  27%|██▋       | 7/26 [00:02<00:06,  3.16it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  31%|███       | 8/26 [00:02<00:05,  3.55it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  31%|███       | 8/26 [00:02<00:05,  3.55it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  35%|███▍      | 9/26 [00:02<00:04,  3.98it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  35%|███▍      | 9/26 [00:02<00:04,  3.98it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  38%|███▊      | 10/26 [00:02<00:03,  4.40it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  38%|███▊      | 10/26 [00:02<00:03,  4.40it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  42%|████▏     | 11/26 [00:02<00:03,  4.82it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  42%|████▏     | 11/26 [00:02<00:03,  4.82it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  46%|████▌     | 12/26 [00:02<00:02,  5.22it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  46%|████▌     | 12/26 [00:02<00:02,  5.22it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  50%|█████     | 13/26 [00:02<00:02,  5.63it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  50%|█████     | 13/26 [00:02<00:02,  5.63it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  54%|█████▍    | 14/26 [00:02<00:01,  6.04it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  54%|█████▍    | 14/26 [00:02<00:01,  6.04it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  58%|█████▊    | 15/26 [00:02<00:01,  6.44it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  58%|█████▊    | 15/26 [00:02<00:01,  6.44it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  62%|██████▏   | 16/26 [00:02<00:01,  6.85it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  62%|██████▏   | 16/26 [00:02<00:01,  6.85it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  65%|██████▌   | 17/26 [00:02<00:01,  7.25it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  65%|██████▌   | 17/26 [00:02<00:01,  7.25it/s, v_num=9, loss/train=11.90, loss/val=11.80]    Epoch 6:  69%|██████▉   | 18/26 [00:02<00:01,  7.64it/s, v_num=9, loss/train=11.90, loss/val=11.80]    Epoch 6:  69%|██████▉   | 18/26 [00:02<00:01,  7.64it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  73%|███████▎  | 19/26 [00:02<00:00,  8.03it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  73%|███████▎  | 19/26 [00:02<00:00,  8.03it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  77%|███████▋  | 20/26 [00:02<00:00,  8.43it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  77%|███████▋  | 20/26 [00:02<00:00,  8.42it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  81%|████████  | 21/26 [00:02<00:00,  8.81it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  81%|████████  | 21/26 [00:02<00:00,  8.81it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  85%|████████▍ | 22/26 [00:02<00:00,  9.18it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  85%|████████▍ | 22/26 [00:02<00:00,  9.18it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  88%|████████▊ | 23/26 [00:02<00:00,  9.57it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  88%|████████▊ | 23/26 [00:02<00:00,  9.57it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  92%|█████████▏| 24/26 [00:02<00:00,  9.96it/s, v_num=9, loss/train=11.80, loss/val=11.80]    Epoch 6:  92%|█████████▏| 24/26 [00:02<00:00,  9.95it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  96%|█████████▌| 25/26 [00:02<00:00, 10.34it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6:  96%|█████████▌| 25/26 [00:02<00:00, 10.34it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6: 100%|██████████| 26/26 [00:02<00:00, 10.73it/s, v_num=9, loss/train=11.70, loss/val=11.80]    Epoch 6: 100%|██████████| 26/26 [00:02<00:00, 10.73it/s, v_num=9, loss/train=8.570, loss/val=11.80]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 58.80it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 34.41it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 44.87it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 53.22it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 61.60it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 66.26it/s]
                                                                              Epoch 6: 100%|██████████| 26/26 [00:03<00:00,  7.89it/s, v_num=9, loss/train=8.570, loss/val=11.70]    Epoch 6: 100%|██████████| 26/26 [00:03<00:00,  7.88it/s, v_num=9, loss/train=8.570, loss/val=11.70]    Epoch 6:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.70]             Epoch 7:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.70]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 7:   4%|▍         | 1/26 [00:01<00:49,  0.51it/s, v_num=9, loss/train=8.570, loss/val=11.70]    Epoch 7:   4%|▍         | 1/26 [00:01<00:49,  0.51it/s, v_num=9, loss/train=11.80, loss/val=11.70]    Epoch 7:   8%|▊         | 2/26 [00:01<00:23,  1.01it/s, v_num=9, loss/train=11.80, loss/val=11.70]    Epoch 7:   8%|▊         | 2/26 [00:01<00:23,  1.01it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  12%|█▏        | 3/26 [00:02<00:15,  1.50it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  12%|█▏        | 3/26 [00:02<00:15,  1.49it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  15%|█▌        | 4/26 [00:02<00:11,  1.96it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  15%|█▌        | 4/26 [00:02<00:11,  1.95it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  19%|█▉        | 5/26 [00:02<00:08,  2.41it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  19%|█▉        | 5/26 [00:02<00:08,  2.40it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  23%|██▎       | 6/26 [00:02<00:07,  2.81it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  23%|██▎       | 6/26 [00:02<00:07,  2.81it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  27%|██▋       | 7/26 [00:02<00:05,  3.24it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  27%|██▋       | 7/26 [00:02<00:05,  3.24it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  31%|███       | 8/26 [00:02<00:04,  3.68it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  31%|███       | 8/26 [00:02<00:04,  3.68it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  35%|███▍      | 9/26 [00:02<00:04,  4.12it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  35%|███▍      | 9/26 [00:02<00:04,  4.12it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  38%|███▊      | 10/26 [00:02<00:03,  4.56it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  38%|███▊      | 10/26 [00:02<00:03,  4.56it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  42%|████▏     | 11/26 [00:02<00:03,  4.95it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  42%|████▏     | 11/26 [00:02<00:03,  4.94it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  46%|████▌     | 12/26 [00:02<00:02,  5.37it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  46%|████▌     | 12/26 [00:02<00:02,  5.37it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  50%|█████     | 13/26 [00:02<00:02,  5.80it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  50%|█████     | 13/26 [00:02<00:02,  5.79it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  54%|█████▍    | 14/26 [00:02<00:01,  6.22it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  54%|█████▍    | 14/26 [00:02<00:01,  6.21it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  58%|█████▊    | 15/26 [00:02<00:01,  6.63it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  58%|█████▊    | 15/26 [00:02<00:01,  6.63it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  62%|██████▏   | 16/26 [00:02<00:01,  7.04it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  62%|██████▏   | 16/26 [00:02<00:01,  7.04it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  65%|██████▌   | 17/26 [00:02<00:01,  7.46it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  65%|██████▌   | 17/26 [00:02<00:01,  7.45it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  69%|██████▉   | 18/26 [00:02<00:01,  7.86it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  69%|██████▉   | 18/26 [00:02<00:01,  7.86it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  73%|███████▎  | 19/26 [00:02<00:00,  8.26it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  73%|███████▎  | 19/26 [00:02<00:00,  8.25it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  77%|███████▋  | 20/26 [00:02<00:00,  8.66it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  77%|███████▋  | 20/26 [00:02<00:00,  8.65it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  81%|████████  | 21/26 [00:02<00:00,  9.04it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  81%|████████  | 21/26 [00:02<00:00,  9.04it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  85%|████████▍ | 22/26 [00:02<00:00,  9.42it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  85%|████████▍ | 22/26 [00:02<00:00,  9.42it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  88%|████████▊ | 23/26 [00:02<00:00,  9.82it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  88%|████████▊ | 23/26 [00:02<00:00,  9.82it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  92%|█████████▏| 24/26 [00:02<00:00, 10.21it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7:  92%|█████████▏| 24/26 [00:02<00:00, 10.21it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  96%|█████████▌| 25/26 [00:02<00:00, 10.59it/s, v_num=9, loss/train=11.60, loss/val=11.70]    Epoch 7:  96%|█████████▌| 25/26 [00:02<00:00, 10.58it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7: 100%|██████████| 26/26 [00:02<00:00, 10.98it/s, v_num=9, loss/train=11.70, loss/val=11.70]    Epoch 7: 100%|██████████| 26/26 [00:02<00:00, 10.97it/s, v_num=9, loss/train=8.550, loss/val=11.70]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 53.93it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 32.42it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 44.95it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 55.99it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 65.53it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 70.70it/s]
                                                                              Epoch 7: 100%|██████████| 26/26 [00:03<00:00,  8.10it/s, v_num=9, loss/train=8.550, loss/val=11.60]    Epoch 7: 100%|██████████| 26/26 [00:03<00:00,  8.10it/s, v_num=9, loss/train=8.550, loss/val=11.60]    Epoch 7:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.550, loss/val=11.60]             Epoch 8:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.550, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 8:   4%|▍         | 1/26 [00:01<00:46,  0.54it/s, v_num=9, loss/train=8.550, loss/val=11.60]    Epoch 8:   4%|▍         | 1/26 [00:01<00:46,  0.54it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:   8%|▊         | 2/26 [00:01<00:23,  1.01it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:   8%|▊         | 2/26 [00:01<00:23,  1.01it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  12%|█▏        | 3/26 [00:02<00:15,  1.49it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  12%|█▏        | 3/26 [00:02<00:15,  1.49it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  15%|█▌        | 4/26 [00:02<00:11,  1.88it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  15%|█▌        | 4/26 [00:02<00:11,  1.88it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  19%|█▉        | 5/26 [00:02<00:09,  2.33it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  19%|█▉        | 5/26 [00:02<00:09,  2.32it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  23%|██▎       | 6/26 [00:02<00:07,  2.76it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  23%|██▎       | 6/26 [00:02<00:07,  2.76it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  27%|██▋       | 7/26 [00:02<00:05,  3.20it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  27%|██▋       | 7/26 [00:02<00:05,  3.20it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  31%|███       | 8/26 [00:02<00:04,  3.64it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  31%|███       | 8/26 [00:02<00:04,  3.64it/s, v_num=9, loss/train=11.70, loss/val=11.60]    Epoch 8:  35%|███▍      | 9/26 [00:02<00:04,  4.08it/s, v_num=9, loss/train=11.70, loss/val=11.60]    Epoch 8:  35%|███▍      | 9/26 [00:02<00:04,  4.08it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  38%|███▊      | 10/26 [00:02<00:03,  4.51it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  38%|███▊      | 10/26 [00:02<00:03,  4.50it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  42%|████▏     | 11/26 [00:02<00:03,  4.92it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  42%|████▏     | 11/26 [00:02<00:03,  4.92it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  46%|████▌     | 12/26 [00:02<00:02,  5.33it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  46%|████▌     | 12/26 [00:02<00:02,  5.33it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  50%|█████     | 13/26 [00:02<00:02,  5.76it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  50%|█████     | 13/26 [00:02<00:02,  5.76it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  54%|█████▍    | 14/26 [00:02<00:01,  6.19it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  54%|█████▍    | 14/26 [00:02<00:01,  6.19it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  58%|█████▊    | 15/26 [00:02<00:01,  6.61it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  58%|█████▊    | 15/26 [00:02<00:01,  6.61it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  62%|██████▏   | 16/26 [00:02<00:01,  7.02it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  62%|██████▏   | 16/26 [00:02<00:01,  7.02it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  65%|██████▌   | 17/26 [00:02<00:01,  7.44it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  65%|██████▌   | 17/26 [00:02<00:01,  7.44it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  69%|██████▉   | 18/26 [00:02<00:01,  7.85it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  69%|██████▉   | 18/26 [00:02<00:01,  7.85it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  73%|███████▎  | 19/26 [00:02<00:00,  8.26it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  73%|███████▎  | 19/26 [00:02<00:00,  8.26it/s, v_num=9, loss/train=11.70, loss/val=11.60]    Epoch 8:  77%|███████▋  | 20/26 [00:02<00:00,  8.68it/s, v_num=9, loss/train=11.70, loss/val=11.60]    Epoch 8:  77%|███████▋  | 20/26 [00:02<00:00,  8.67it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  81%|████████  | 21/26 [00:02<00:00,  9.08it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  81%|████████  | 21/26 [00:02<00:00,  9.08it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  85%|████████▍ | 22/26 [00:02<00:00,  9.48it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  85%|████████▍ | 22/26 [00:02<00:00,  9.48it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  88%|████████▊ | 23/26 [00:02<00:00,  9.86it/s, v_num=9, loss/train=11.50, loss/val=11.60]    Epoch 8:  88%|████████▊ | 23/26 [00:02<00:00,  9.86it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  92%|█████████▏| 24/26 [00:02<00:00, 10.26it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  92%|█████████▏| 24/26 [00:02<00:00, 10.26it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  96%|█████████▌| 25/26 [00:02<00:00, 10.65it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8:  96%|█████████▌| 25/26 [00:02<00:00, 10.65it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8: 100%|██████████| 26/26 [00:02<00:00, 11.05it/s, v_num=9, loss/train=11.60, loss/val=11.60]    Epoch 8: 100%|██████████| 26/26 [00:02<00:00, 11.05it/s, v_num=9, loss/train=8.570, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 17.91it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 21.61it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 31.04it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 39.80it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 47.87it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 54.02it/s]
                                                                              Epoch 8: 100%|██████████| 26/26 [00:03<00:00,  8.15it/s, v_num=9, loss/train=8.570, loss/val=11.50]    Epoch 8: 100%|██████████| 26/26 [00:03<00:00,  8.15it/s, v_num=9, loss/train=8.570, loss/val=11.50]    Epoch 8:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.50]             Epoch 9:   0%|          | 0/26 [00:00<?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Epoch 9:   4%|▍         | 1/26 [00:01<00:34,  0.73it/s, v_num=9, loss/train=8.570, loss/val=11.50]    Epoch 9:   4%|▍         | 1/26 [00:01<00:34,  0.73it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:   8%|▊         | 2/26 [00:01<00:20,  1.18it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:   8%|▊         | 2/26 [00:01<00:20,  1.17it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  12%|█▏        | 3/26 [00:01<00:13,  1.64it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  12%|█▏        | 3/26 [00:01<00:13,  1.64it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  15%|█▌        | 4/26 [00:01<00:10,  2.01it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  15%|█▌        | 4/26 [00:01<00:10,  2.01it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  19%|█▉        | 5/26 [00:02<00:08,  2.47it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  19%|█▉        | 5/26 [00:02<00:08,  2.47it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  23%|██▎       | 6/26 [00:02<00:07,  2.85it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  23%|██▎       | 6/26 [00:02<00:07,  2.85it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  27%|██▋       | 7/26 [00:02<00:05,  3.29it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  27%|██▋       | 7/26 [00:02<00:05,  3.29it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  31%|███       | 8/26 [00:02<00:04,  3.73it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  31%|███       | 8/26 [00:02<00:04,  3.73it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  35%|███▍      | 9/26 [00:02<00:04,  4.17it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  35%|███▍      | 9/26 [00:02<00:04,  4.17it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  38%|███▊      | 10/26 [00:02<00:03,  4.61it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  38%|███▊      | 10/26 [00:02<00:03,  4.61it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  42%|████▏     | 11/26 [00:02<00:02,  5.06it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  42%|████▏     | 11/26 [00:02<00:02,  5.05it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  46%|████▌     | 12/26 [00:02<00:02,  5.49it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  46%|████▌     | 12/26 [00:02<00:02,  5.49it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  50%|█████     | 13/26 [00:02<00:02,  5.89it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  50%|█████     | 13/26 [00:02<00:02,  5.89it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  54%|█████▍    | 14/26 [00:02<00:01,  6.30it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  54%|█████▍    | 14/26 [00:02<00:01,  6.29it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  58%|█████▊    | 15/26 [00:02<00:01,  6.72it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  58%|█████▊    | 15/26 [00:02<00:01,  6.72it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  62%|██████▏   | 16/26 [00:02<00:01,  7.14it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  62%|██████▏   | 16/26 [00:02<00:01,  7.14it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  65%|██████▌   | 17/26 [00:02<00:01,  7.56it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  65%|██████▌   | 17/26 [00:02<00:01,  7.56it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  69%|██████▉   | 18/26 [00:02<00:01,  7.97it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  69%|██████▉   | 18/26 [00:02<00:01,  7.97it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  73%|███████▎  | 19/26 [00:02<00:00,  8.39it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  73%|███████▎  | 19/26 [00:02<00:00,  8.39it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  77%|███████▋  | 20/26 [00:02<00:00,  8.81it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  77%|███████▋  | 20/26 [00:02<00:00,  8.81it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  81%|████████  | 21/26 [00:02<00:00,  9.22it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  81%|████████  | 21/26 [00:02<00:00,  9.22it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  85%|████████▍ | 22/26 [00:02<00:00,  9.63it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  85%|████████▍ | 22/26 [00:02<00:00,  9.63it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  88%|████████▊ | 23/26 [00:02<00:00, 10.02it/s, v_num=9, loss/train=11.60, loss/val=11.50]    Epoch 9:  88%|████████▊ | 23/26 [00:02<00:00, 10.02it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  92%|█████████▏| 24/26 [00:02<00:00, 10.42it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  92%|█████████▏| 24/26 [00:02<00:00, 10.42it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  96%|█████████▌| 25/26 [00:02<00:00, 10.83it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9:  96%|█████████▌| 25/26 [00:02<00:00, 10.83it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9: 100%|██████████| 26/26 [00:02<00:00, 11.24it/s, v_num=9, loss/train=11.50, loss/val=11.50]    Epoch 9: 100%|██████████| 26/26 [00:02<00:00, 11.24it/s, v_num=9, loss/train=8.460, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(

    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation: |          | 0/? [00:00<?, ?it/s]
    Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]
    Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 41.67it/s]
    Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 27.39it/s]
    Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 35.30it/s]
    Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 29.13it/s]
    Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 35.04it/s]
    Validation DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 39.68it/s]
                                                                              Epoch 9: 100%|██████████| 26/26 [00:03<00:00,  8.29it/s, v_num=9, loss/train=8.460, loss/val=11.50]    Epoch 9: 100%|██████████| 26/26 [00:03<00:00,  8.29it/s, v_num=9, loss/train=8.460, loss/val=11.50]    Epoch 9: 100%|██████████| 26/26 [00:03<00:00,  8.28it/s, v_num=9, loss/train=8.460, loss/val=11.50]

    YAwareContrastiveLearning(
      (encoder): MLP(
        (0): Linear(in_features=272, out_features=64, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=64, out_features=32, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
      (projection_head): YAwareProjectionHead(
        (layers): Sequential(
          (0): Linear(in_features=32, out_features=64, bias=True)
          (1): ReLU()
          (2): Linear(in_features=64, out_features=32, bias=True)
        )
      )
      (loss): YAwareInfoNCE(
        (sim_metric): PairwiseCosineSimilarity()
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 303-312

Visualization and evaluation of the learned representations
--------------------------------------------------------------

In order to visualize the learned representations of both models, we apply
a widely used dimensionality reduction technique: Multi-Dimensional Scaling
(MDS). This technique project the points in a lower-dimensional space such
that the pairwise distances between points are preserved as much as possible.
Then, we evaluate the learned representations on age prediction using linear
regression and KNN regression.

.. GENERATED FROM PYTHON SOURCE LINES 314-316

We first extract the embeddings of the training and test sets for both VBM
and SBM data.

.. GENERATED FROM PYTHON SOURCE LINES 316-321

.. code-block:: Python

    Z_train_vbm = vbm_model.transform(dataloader_vbm_train).cpu()
    Z_test_vbm = vbm_model.transform(dataloader_vbm_test).cpu()
    Z_train_sbm = sbm_model.transform(dataloader_sbm_train).cpu()
    Z_test_sbm = sbm_model.transform(dataloader_sbm_test).cpu()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(
    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting DataLoader 0:   0%|          | 0/26 [00:00<?, ?it/s]    Predicting DataLoader 0:   4%|▍         | 1/26 [00:00<00:00, 144.44it/s]    Predicting DataLoader 0:   8%|▊         | 2/26 [00:00<00:04,  5.00it/s]     Predicting DataLoader 0:  12%|█▏        | 3/26 [00:00<00:03,  7.04it/s]    Predicting DataLoader 0:  15%|█▌        | 4/26 [00:00<00:02,  8.54it/s]    Predicting DataLoader 0:  19%|█▉        | 5/26 [00:00<00:02, 10.33it/s]    Predicting DataLoader 0:  23%|██▎       | 6/26 [00:00<00:01, 11.18it/s]    Predicting DataLoader 0:  27%|██▋       | 7/26 [00:00<00:01, 11.84it/s]    Predicting DataLoader 0:  31%|███       | 8/26 [00:00<00:01, 12.93it/s]    Predicting DataLoader 0:  35%|███▍      | 9/26 [00:00<00:01, 13.70it/s]    Predicting DataLoader 0:  38%|███▊      | 10/26 [00:00<00:01, 15.19it/s]    Predicting DataLoader 0:  42%|████▏     | 11/26 [00:00<00:00, 16.47it/s]    Predicting DataLoader 0:  46%|████▌     | 12/26 [00:00<00:00, 17.70it/s]    Predicting DataLoader 0:  50%|█████     | 13/26 [00:00<00:00, 18.71it/s]    Predicting DataLoader 0:  54%|█████▍    | 14/26 [00:00<00:00, 19.81it/s]    Predicting DataLoader 0:  58%|█████▊    | 15/26 [00:00<00:00, 20.70it/s]    Predicting DataLoader 0:  62%|██████▏   | 16/26 [00:00<00:00, 21.23it/s]    Predicting DataLoader 0:  65%|██████▌   | 17/26 [00:00<00:00, 22.21it/s]    Predicting DataLoader 0:  69%|██████▉   | 18/26 [00:00<00:00, 23.38it/s]    Predicting DataLoader 0:  73%|███████▎  | 19/26 [00:00<00:00, 24.14it/s]    Predicting DataLoader 0:  77%|███████▋  | 20/26 [00:00<00:00, 25.35it/s]    Predicting DataLoader 0:  81%|████████  | 21/26 [00:00<00:00, 26.56it/s]    Predicting DataLoader 0:  85%|████████▍ | 22/26 [00:00<00:00, 27.77it/s]    Predicting DataLoader 0:  88%|████████▊ | 23/26 [00:00<00:00, 28.96it/s]    Predicting DataLoader 0:  92%|█████████▏| 24/26 [00:00<00:00, 29.38it/s]    Predicting DataLoader 0:  96%|█████████▌| 25/26 [00:00<00:00, 30.08it/s]    Predicting DataLoader 0: 100%|██████████| 26/26 [00:00<00:00, 31.25it/s]    Predicting DataLoader 0: 100%|██████████| 26/26 [00:00<00:00, 31.24it/s]
    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]    Predicting DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 93.92it/s]    Predicting DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 29.76it/s]    Predicting DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 44.04it/s]    Predicting DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 58.03it/s]    Predicting DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 71.73it/s]    Predicting DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 85.01it/s]    Predicting DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 84.55it/s]
    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting DataLoader 0:   0%|          | 0/26 [00:00<?, ?it/s]    Predicting DataLoader 0:   4%|▍         | 1/26 [00:00<00:00, 108.01it/s]    Predicting DataLoader 0:   8%|▊         | 2/26 [00:00<00:01, 23.11it/s]     Predicting DataLoader 0:  12%|█▏        | 3/26 [00:00<00:01, 18.79it/s]    Predicting DataLoader 0:  15%|█▌        | 4/26 [00:00<00:01, 17.12it/s]    Predicting DataLoader 0:  19%|█▉        | 5/26 [00:00<00:01, 20.43it/s]    Predicting DataLoader 0:  23%|██▎       | 6/26 [00:00<00:00, 23.65it/s]    Predicting DataLoader 0:  27%|██▋       | 7/26 [00:00<00:00, 26.64it/s]    Predicting DataLoader 0:  31%|███       | 8/26 [00:00<00:00, 29.44it/s]    Predicting DataLoader 0:  35%|███▍      | 9/26 [00:00<00:00, 32.17it/s]    Predicting DataLoader 0:  38%|███▊      | 10/26 [00:00<00:00, 35.57it/s]    Predicting DataLoader 0:  42%|████▏     | 11/26 [00:00<00:00, 37.98it/s]    Predicting DataLoader 0:  46%|████▌     | 12/26 [00:00<00:00, 27.68it/s]    Predicting DataLoader 0:  50%|█████     | 13/26 [00:00<00:00, 29.57it/s]    Predicting DataLoader 0:  54%|█████▍    | 14/26 [00:00<00:00, 31.30it/s]    Predicting DataLoader 0:  58%|█████▊    | 15/26 [00:00<00:00, 33.13it/s]    Predicting DataLoader 0:  62%|██████▏   | 16/26 [00:00<00:00, 35.17it/s]    Predicting DataLoader 0:  65%|██████▌   | 17/26 [00:00<00:00, 37.07it/s]    Predicting DataLoader 0:  69%|██████▉   | 18/26 [00:00<00:00, 39.06it/s]    Predicting DataLoader 0:  73%|███████▎  | 19/26 [00:00<00:00, 40.89it/s]    Predicting DataLoader 0:  77%|███████▋  | 20/26 [00:00<00:00, 42.86it/s]    Predicting DataLoader 0:  81%|████████  | 21/26 [00:00<00:00, 44.62it/s]    Predicting DataLoader 0:  85%|████████▍ | 22/26 [00:00<00:00, 40.92it/s]    Predicting DataLoader 0:  88%|████████▊ | 23/26 [00:00<00:00, 42.71it/s]    Predicting DataLoader 0:  92%|█████████▏| 24/26 [00:00<00:00, 44.50it/s]    Predicting DataLoader 0:  96%|█████████▌| 25/26 [00:00<00:00, 46.28it/s]    Predicting DataLoader 0: 100%|██████████| 26/26 [00:00<00:00, 48.06it/s]    Predicting DataLoader 0: 100%|██████████| 26/26 [00:00<00:00, 48.02it/s]
    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting: |          | 0/? [00:00<?, ?it/s]    Predicting DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]    Predicting DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 249.26it/s]    Predicting DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00, 31.01it/s]     Predicting DataLoader 0:  50%|█████     | 3/6 [00:00<00:00, 45.49it/s]    Predicting DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00, 44.91it/s]    Predicting DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00, 55.50it/s]    Predicting DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 65.93it/s]    Predicting DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 65.62it/s]




.. GENERATED FROM PYTHON SOURCE LINES 322-324

We also extract the ages of the subjects for coloring the points in the
visualizations and for evaluating the representations on age prediction.

.. GENERATED FROM PYTHON SOURCE LINES 324-330

.. code-block:: Python

    y_train_vbm = [y for (_, y) in dataloader_vbm_train.dataset.samples]
    y_test_vbm = [y for (_, y) in dataloader_vbm_test.dataset.samples]
    y_train_sbm = [y for (_, y) in dataloader_sbm_train.dataset.samples]
    y_test_sbm = [y for (_, y) in dataloader_sbm_test.dataset.samples]









.. GENERATED FROM PYTHON SOURCE LINES 331-333

We then apply MDS on the test set and visualize the results. The
points are colored according to the age of the subjects.

.. GENERATED FROM PYTHON SOURCE LINES 333-370

.. code-block:: Python



    def plot_mds_side_by_side(Z_vbm, Z_sbm, y_vbm, y_sbm):
        """Run MDS on VBM and SBM embeddings and plot side-by-side scatter
        plots."""
        mds = MDS(n_components=2, n_init=4, max_iter=300)

        # Fit-transform embeddings
        Z_vbm_mds = mds.fit_transform(Z_vbm)
        Z_sbm_mds = mds.fit_transform(Z_sbm)

        # Side-by-side plots
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        sc1 = axes[0].scatter(
            Z_vbm_mds[:, 0], Z_vbm_mds[:, 1], c=y_vbm, cmap="viridis", alpha=0.8
        )
        axes[0].set_title("VBM - MDS projection")
        axes[0].set_xlabel("Dim 1")
        axes[0].set_ylabel("Dim 2")
        plt.colorbar(sc1, ax=axes[0], label="Age")

        sc2 = axes[1].scatter(
            Z_sbm_mds[:, 0], Z_sbm_mds[:, 1], c=y_sbm, cmap="viridis", alpha=0.8
        )
        axes[1].set_title("SBM - MDS projection")
        axes[1].set_xlabel("Dim 1")
        axes[1].set_ylabel("Dim 2")
        plt.colorbar(sc2, ax=axes[1], label="Age")

        plt.suptitle("MDS projections of test embeddings", fontsize=14)
        plt.tight_layout()
        plt.show()


    plot_mds_side_by_side(Z_test_vbm, Z_test_sbm, y_test_vbm, y_test_sbm)




.. image-sg:: /auto_examples/images/sphx_glr_plot_yaware_openbhb_001.png
   :alt: MDS projections of test embeddings, VBM - MDS projection, SBM - MDS projection
   :srcset: /auto_examples/images/sphx_glr_plot_yaware_openbhb_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 371-375

Finally, we evaluate the learned representations on age prediction using
linear regression and KNN regression. We report the mean absolute error and
the R^2 coefficient between the true and predicted ages on the test set for
each model.

.. GENERATED FROM PYTHON SOURCE LINES 375-452

.. code-block:: Python



    def evaluate_and_predict(model, Z_train, Z_test, y_train, y_test):
        """Train model and return predictions + metrics."""
        model.fit(Z_train, y_train)
        y_pred = model.predict(Z_test)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        return y_pred, mae, r2


    def plot_comparison(models, embeddings):
        """
        Plot side-by-side scatter plots for each model and modality.
        models: dict of {name: model}
        embeddings: dict of {modality: (Z_train, Z_test, y_train, y_test)}
        """
        n_models = len(models)
        n_modalities = len(embeddings)

        fig, axes = plt.subplots(
            n_models,
            n_modalities,
            figsize=(6 * n_modalities, 5 * n_models),
            sharex=True,
            sharey=True,
        )
        for row, (model_name, model) in enumerate(models.items()):
            for col, (modality, (Z_train, Z_test, y_train, y_test)) in enumerate(
                embeddings.items()
            ):
                y_pred, mae, r2 = evaluate_and_predict(
                    model, Z_train, Z_test, y_train, y_test
                )

                ax = axes[row, col]
                ax.scatter(
                    y_test,
                    y_pred,
                    alpha=0.7,
                    color="orange" if modality == "SBM" else "steelblue",
                )
                ax.plot(
                    [np.min(y_test), np.max(y_test)],
                    [np.min(y_test), np.max(y_test)],
                    "r--",
                    lw=2,
                    label="Ideal",
                )
                ax.set_title(
                    f"{modality} - {model_name}\nMAE={mae:.2f}, R²={r2:.2f}"
                )
                ax.set_xlabel("True Age")
                if col == 0:
                    ax.set_ylabel("Predicted Age")
                ax.legend()
                ax.grid(True)

        plt.suptitle("Model Comparison: VBM vs SBM", fontsize=16, y=1.02)
        plt.tight_layout()
        plt.show()


    # Define models and embeddings
    models = {
        "Linear Regression": LinearRegression(),
        "KNN (k=5)": KNeighborsRegressor(n_neighbors=5),
    }

    embeddings = {
        "VBM": (Z_train_vbm, Z_test_vbm, y_train_vbm, y_test_vbm),
        "SBM": (Z_train_sbm, Z_test_sbm, y_train_sbm, y_test_sbm),
    }

    # Run comparison
    plot_comparison(models, embeddings)




.. image-sg:: /auto_examples/images/sphx_glr_plot_yaware_openbhb_002.png
   :alt: Model Comparison: VBM vs SBM, VBM - Linear Regression MAE=5.62, R²=0.66, SBM - Linear Regression MAE=7.51, R²=0.38, VBM - KNN (k=5) MAE=4.98, R²=0.64, SBM - KNN (k=5) MAE=7.19, R²=0.29
   :srcset: /auto_examples/images/sphx_glr_plot_yaware_openbhb_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 453-462

**Observations**: From the MDS visualizations, we can observe that both VBM
and SBM embeddings show a gradient of ages, indicating that the models have
learned to organize the data in a way that reflects age similarity. However,
the VBM embeddings appear to have a more continuous distribution of ages
compared to SBM. This suggests that VBM may capture age-related features
more effectively than SBM in this context. This is confirmed when looking at
the age prediction results, where VBM outperforms SBM for both linear
regression and KNN regression. However, the results can be improved by
working with the original 3d brain scans instead of the ROI-averaged data.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 31.106 seconds)

**Estimated memory usage:**  110 MB


.. _sphx_glr_download_auto_examples_plot_yaware_openbhb.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_yaware_openbhb.ipynb <plot_yaware_openbhb.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_yaware_openbhb.py <plot_yaware_openbhb.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_yaware_openbhb.zip <plot_yaware_openbhb.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
