{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Weakly Supervised Contrastive Learning with y-Aware\n\nThis tutorial will show you how to fit and evaluate a y-Aware Contrastive\nLearning model on the OpenBHB dataset using NIDL. As in the original paper\n[1]_, we will use age as a weak label to guide the contrastive learning\nprocess. The model will be trained to bring representations of samples with\nsimilar ages closer together in the feature space, while pushing apart samples\nwith dissimilar ages. This approach leverages the age information to enhance\nthe quality of the learned representations, making them more relevant for\ndownstream tasks such as age prediction or disease classification.\n\nWe will follow these steps using the NIDL library:\n\n1. Load the OpenBHB dataset.\n2. Define the data augmentations for weakly-supervised training.\n3. Define the y-Aware Contrastive Learning model.\n4. Train the model using the weak labels.\n5. Visualize the model's embedding using MDS and evaluate its\n   performance on age prediction using linear regression and KNN.\n\nAs for the neuroimaging data, we will investigate two input representations:\n\n- Voxel-based morphometry (VBM) maps, which are preprocessed gray matter\n  density maps.\n- Surface-based morphometry (SBM) maps, which are cortical thickness, mean\n  curvature, gray matter volume and surface area maps projected onto a\n  standard surface template.\n\n  Both representations are available in the OpenBHB dataset. To make the\n  training faster and reduce the memory footprint, we will consider regions\n  of interest (ROIs) instead of the whole brain. For VBM, we will\n  use the mean gray matter density averaged within each ROI of the\n  Neuromorphometrics atlas (284 regions). For SBM, we will use the cortical\n  thickness, mean curvature, gray matter volume and surface area averaged\n  within each ROI of the Desikan-Killiany atlas (68 regions).\n\n  The y-Aware Contrastive Learning model will be trained individually on both\n  representations and we will compare their performance on age prediction.\n\n.. [1] Dufumier et al., Contrastive learning with continuous proxy meta-data\n       for 3d mri classification, MICCAI 2021.\n\n## Setup\n\nThis notebook requires some packages besides nidl. Let's first start with\nimporting our standard libraries below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.manifold import MDS\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom torch.utils.data import DataLoader\nfrom torchvision.ops import MLP\n\nfrom nidl.datasets import OpenBHB\nfrom nidl.estimators.ssl import YAwareContrastiveLearning\nfrom nidl.transforms import MultiViewsTransform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define some global parameters that will be used throughout the notebook:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dir = \"/tmp/openbhb\"\nbatch_size = 128\nnum_workers = 10\nlatent_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenBHB datasets and data augmentations for Contrastive Learning\n\nWe will use the OpenBHB dataset for pre-training the models. We will focus\non the VBM ROI representation and the SBM ROI representation for this\ntutorial. Since they are tabular data, we will use random masking and\nadding Gaussian noise as data augmentation in contrastive learning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for data augmentations\nmask_prob = 0.8\nnoise_std = 0.5\ncontrast_transforms = transforms.Compose(\n    [\n        lambda x: x.flatten(),\n        lambda x: (np.random.rand(*x.shape) > mask_prob).astype(np.float32)\n        * x,  # random masking\n        lambda x: x\n        + (\n            (np.random.rand() > 0.5) * np.random.randn(*x.shape) * noise_std\n        ).astype(np.float32),  # random Gaussian noise\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first create the SSL dataloaders with VBM modality and age as weak label.\nWe use the previous contrastive transforms for data augmentation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataloader_ssl_vbm = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"vbm_roi\",\n        target=\"age\",\n        streaming=False,\n        transforms=MultiViewsTransform(contrast_transforms, n_views=2),\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=True,\n)\ndataloader_ssl_vbm_test = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"vbm_roi\",\n        target=\"age\",\n        split=\"val\",\n        streaming=False,\n        transforms=MultiViewsTransform(contrast_transforms, n_views=2),\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create the SSL dataloaders with SBM modality on the Desikan-Killiany\natlas and age as weak label. We only extract some surface features and we use\nthe same contrastive transforms as for VBM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Extract only surface area, GM volume, cortical thickness, mean curvature for\n# SBM maps\nsbm_channels = [0, 1, 2, 5]\n\n\ndef sbm_transform(x):\n    return x[sbm_channels].flatten()\n\n\ndef vbm_transform(x):\n    return x.flatten()\n\n\ndataloader_ssl_sbm = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"fs_desikan_roi\",\n        target=\"age\",\n        streaming=False,\n        transforms=MultiViewsTransform(\n            transforms.Compose([sbm_transform, contrast_transforms]), n_views=2\n        ),\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=True,\n)\ndataloader_ssl_sbm_test = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"fs_desikan_roi\",\n        target=\"age\",\n        split=\"val\",\n        streaming=False,\n        transforms=MultiViewsTransform(\n            transforms.Compose([sbm_transform, contrast_transforms]), n_views=2\n        ),\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create the dataloaders for evaluating the learned representations\non age prediction. We don't apply any data augmentation here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataloader_vbm_train = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"vbm_roi\",\n        target=\"age\",\n        split=\"train\",\n        transforms=vbm_transform,\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=False,\n)\n\ndataloader_vbm_test = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"vbm_roi\",\n        target=\"age\",\n        split=\"val\",\n        transforms=vbm_transform,\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=False,\n)\n\ndataloader_sbm_train = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"fs_desikan_roi\",\n        target=\"age\",\n        split=\"train\",\n        transforms=sbm_transform,\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=False,\n)\ndataloader_sbm_test = DataLoader(\n    OpenBHB(\n        data_dir,\n        modality=\"fs_desikan_roi\",\n        target=\"age\",\n        split=\"val\",\n        transforms=sbm_transform,\n    ),\n    batch_size=batch_size,\n    num_workers=num_workers,\n    shuffle=False,\n)\n\n# Small hack to avoid returning the target in the dataloaders since we aim\n# at transforming these datasets without their targets.\ndataloader_vbm_train.dataset.target = None\ndataloader_vbm_test.dataset.target = None\ndataloader_sbm_train.dataset.target = None\ndataloader_sbm_test.dataset.target = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training of y-Aware Contrastive Learning models\n\nWe can now instantiate and train two y-Aware Contrastive Learning models (one\nfor VBM and another for SBM). We use the age as weak label to impose similar\nrepresentations of samples with close age for both models.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we work with tabular data, we can use a simple MLP as encoder. For\nVBM data, the input dimension is 284 and we compress the data to a 32-d\nvector. SBM data is flattened to a 272-d vector (68 regions * 4 features)\nand we also compress it to a 32-d vector.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vbm_encoder = MLP(in_channels=284, hidden_channels=[64, latent_size])\nsbm_encoder = MLP(in_channels=272, hidden_channels=[64, latent_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We limit the training to 10 epochs for the sake of time and we use a small\nbandwidth for the Gaussian kernel in the y-Aware model compared to the\nvariance of the age in OpenBHB (sigma=4 vs std(age)=15).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = 4\nvbm_model = YAwareContrastiveLearning(\n    encoder=vbm_encoder,\n    projection_head_kwargs={\n        \"input_dim\": latent_size,\n        \"hidden_dim\": 2 * latent_size,\n        \"output_dim\": latent_size,\n    },\n    bandwidth=sigma**2,\n    random_state=42,\n    max_epochs=10,\n    temperature=0.1,\n    learning_rate=1e-5,\n    enable_checkpointing=False,\n)\n\nsbm_model = YAwareContrastiveLearning(\n    encoder=sbm_encoder,\n    projection_head_kwargs={\n        \"input_dim\": latent_size,\n        \"hidden_dim\": 2 * latent_size,\n        \"output_dim\": latent_size,\n    },\n    bandwidth=sigma**2,\n    random_state=42,\n    max_epochs=10,\n    temperature=0.1,\n    learning_rate=1e-5,\n    enable_checkpointing=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We train both models on their respective dataloaders.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vbm_model.fit(\n    dataloader_ssl_vbm,\n    dataloader_ssl_vbm_test,\n)\n\nsbm_model.fit(\n    dataloader_ssl_sbm,\n    dataloader_ssl_sbm_test,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization and evaluation of the learned representations\n\nIn order to visualize the learned representations of both models, we apply\na widely used dimensionality reduction technique: Multi-Dimensional Scaling\n(MDS). This technique project the points in a lower-dimensional space such\nthat the pairwise distances between points are preserved as much as possible.\nThen, we evaluate the learned representations on age prediction using linear\nregression and KNN regression.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first extract the embeddings of the training and test sets for both VBM\nand SBM data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Z_train_vbm = vbm_model.transform(dataloader_vbm_train).cpu()\nZ_test_vbm = vbm_model.transform(dataloader_vbm_test).cpu()\nZ_train_sbm = sbm_model.transform(dataloader_sbm_train).cpu()\nZ_test_sbm = sbm_model.transform(dataloader_sbm_test).cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also extract the ages of the subjects for coloring the points in the\nvisualizations and for evaluating the representations on age prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_train_vbm = [y for (_, y) in dataloader_vbm_train.dataset.samples]\ny_test_vbm = [y for (_, y) in dataloader_vbm_test.dataset.samples]\ny_train_sbm = [y for (_, y) in dataloader_sbm_train.dataset.samples]\ny_test_sbm = [y for (_, y) in dataloader_sbm_test.dataset.samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then apply MDS on the test set and visualize the results. The\npoints are colored according to the age of the subjects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_mds_side_by_side(Z_vbm, Z_sbm, y_vbm, y_sbm):\n    \"\"\"Run MDS on VBM and SBM embeddings and plot side-by-side scatter\n    plots.\"\"\"\n    mds = MDS(n_components=2, n_init=4, max_iter=300)\n\n    # Fit-transform embeddings\n    Z_vbm_mds = mds.fit_transform(Z_vbm)\n    Z_sbm_mds = mds.fit_transform(Z_sbm)\n\n    # Side-by-side plots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    sc1 = axes[0].scatter(\n        Z_vbm_mds[:, 0], Z_vbm_mds[:, 1], c=y_vbm, cmap=\"viridis\", alpha=0.8\n    )\n    axes[0].set_title(\"VBM - MDS projection\")\n    axes[0].set_xlabel(\"Dim 1\")\n    axes[0].set_ylabel(\"Dim 2\")\n    plt.colorbar(sc1, ax=axes[0], label=\"Age\")\n\n    sc2 = axes[1].scatter(\n        Z_sbm_mds[:, 0], Z_sbm_mds[:, 1], c=y_sbm, cmap=\"viridis\", alpha=0.8\n    )\n    axes[1].set_title(\"SBM - MDS projection\")\n    axes[1].set_xlabel(\"Dim 1\")\n    axes[1].set_ylabel(\"Dim 2\")\n    plt.colorbar(sc2, ax=axes[1], label=\"Age\")\n\n    plt.suptitle(\"MDS projections of test embeddings\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_mds_side_by_side(Z_test_vbm, Z_test_sbm, y_test_vbm, y_test_sbm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we evaluate the learned representations on age prediction using\nlinear regression and KNN regression. We report the mean absolute error and\nthe R^2 coefficient between the true and predicted ages on the test set for\neach model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_and_predict(model, Z_train, Z_test, y_train, y_test):\n    \"\"\"Train model and return predictions + metrics.\"\"\"\n    model.fit(Z_train, y_train)\n    y_pred = model.predict(Z_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    return y_pred, mae, r2\n\n\ndef plot_comparison(models, embeddings):\n    \"\"\"\n    Plot side-by-side scatter plots for each model and modality.\n    models: dict of {name: model}\n    embeddings: dict of {modality: (Z_train, Z_test, y_train, y_test)}\n    \"\"\"\n    n_models = len(models)\n    n_modalities = len(embeddings)\n\n    fig, axes = plt.subplots(\n        n_models,\n        n_modalities,\n        figsize=(6 * n_modalities, 5 * n_models),\n        sharex=True,\n        sharey=True,\n    )\n    for row, (model_name, model) in enumerate(models.items()):\n        for col, (modality, (Z_train, Z_test, y_train, y_test)) in enumerate(\n            embeddings.items()\n        ):\n            y_pred, mae, r2 = evaluate_and_predict(\n                model, Z_train, Z_test, y_train, y_test\n            )\n\n            ax = axes[row, col]\n            ax.scatter(\n                y_test,\n                y_pred,\n                alpha=0.7,\n                color=\"orange\" if modality == \"SBM\" else \"steelblue\",\n            )\n            ax.plot(\n                [np.min(y_test), np.max(y_test)],\n                [np.min(y_test), np.max(y_test)],\n                \"r--\",\n                lw=2,\n                label=\"Ideal\",\n            )\n            ax.set_title(\n                f\"{modality} - {model_name}\\nMAE={mae:.2f}, R\u00b2={r2:.2f}\"\n            )\n            ax.set_xlabel(\"True Age\")\n            if col == 0:\n                ax.set_ylabel(\"Predicted Age\")\n            ax.legend()\n            ax.grid(True)\n\n    plt.suptitle(\"Model Comparison: VBM vs SBM\", fontsize=16, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n\n# Define models and embeddings\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"KNN (k=5)\": KNeighborsRegressor(n_neighbors=5),\n}\n\nembeddings = {\n    \"VBM\": (Z_train_vbm, Z_test_vbm, y_train_vbm, y_test_vbm),\n    \"SBM\": (Z_train_sbm, Z_test_sbm, y_train_sbm, y_test_sbm),\n}\n\n# Run comparison\nplot_comparison(models, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations**: From the MDS visualizations, we can observe that both VBM\nand SBM embeddings show a gradient of ages, indicating that the models have\nlearned to organize the data in a way that reflects age similarity. However,\nthe VBM embeddings appear to have a more continuous distribution of ages\ncompared to SBM. This suggests that VBM may capture age-related features\nmore effectively than SBM in this context. This is confirmed when looking at\nthe age prediction results, where VBM outperforms SBM for both linear\nregression and KNN regression. However, the results can be improved by\nworking with the original 3d brain scans instead of the ROI-averaged data.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}