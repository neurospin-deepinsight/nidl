<!doctype html>
<html lang="en">

    <head>

		<!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <title>nidl</title>
        
        <!-- CSS -->
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500&display=swap">
		<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
        <link rel="stylesheet" href="../../../../_static/css/jquery.mCustomScrollbar.min.css">
        <link rel="stylesheet" href="../../../../_static/css/animate.css">
        <link rel="stylesheet" href="../../../../_static/css/style.css">
        <link rel="stylesheet" href="../../../../_static/css/jquery.mosaic.css">
        <link rel="stylesheet" href="../../../../_static/sg_gallery.css">
        <link rel="stylesheet" href="../../../../_static/css/media-queries.css">
        <link rel="stylesheet" href="../../../../_static/css/pygment.css">

        <!-- Favicon and touch icons -->
        <link rel="shortcut icon" href="../../../../_static/ico/favicon.png">
        <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../../../_static/ico/apple-touch-icon-144-precomposed.png">
        <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../../../../_static/ico/apple-touch-icon-114-precomposed.png">
        <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../../../../_static/ico/apple-touch-icon-72-precomposed.png">
        <link rel="apple-touch-icon-precomposed" href="../../../../_static/ico/apple-touch-icon-57-precomposed.png">

    </head>

    <body>

		<!-- Wrapper -->
    	<div class="wrapper">

			<!-- Sidebar -->
			<nav class="sidebar">
				
				<!-- close sidebar menu -->
				<div class="dismiss">
					<i class="fas fa-arrow-left"></i>
				</div>
				
				<!-- <div class="logo"">
					<h3><a href="../../../../index.html">Sidebar Menu</a></h3>
				</div> -->

                <!-- info setup -->
                    <p class="doc-version">
                        This documentation is for nidl <strong>version 0.0.0</strong>
                    </p>
                <p class="citing">
                    If you use the software, please do not hesitate to 
                    <a &mdash; <a href="https://github.com/neurospin-deepinsight/nidl">
                    Report a Bug</a>.
                </p>
				
                <!-- links -->
                
                
				<ul class="list-unstyled menu-elements">
					<li class="active">
						<a href="../../../../index.html"><i class="fas fa-home"></i> Home</a>
					</li>
					<li>
						<a href="../../../../generated/installation.html"><i class="fas fa-cog"></i> Installation</a>
					</li>
					<li>
						<a href="../../../../auto_gallery/index.html"><i class="fas fa-eye"></i> Gallery</a>
					</li>
					<li>
						<a href="../../../../generated/documentation.html"><i class="fas fa-pencil-alt"></i> API documentation</a>
					</li>
					<li>
						<a href="../../../../generated/search.html"><i class="fas fa-search"></i> Search</a>
					</li>
					<!-- <li>
						<a href="https://github.com/AGrigis/pysphinxdoc"><i class="fas fa-external-link-alt"></i> PYSPHINXDOC</a>
					</li> -->
					<!-- <li>
						<a href="#otherSections" data-toggle="collapse" aria-expanded="false" class="dropdown-toggle" role="button" aria-controls="otherSections">
							<i class="fas fa-sync"></i>Sections Shortcuts
						</a>
						<ul class="collapse list-unstyled" id="otherSections">
                            <li>LINKS</li><li><a href='https://github.com/neurospin-deepinsight/surfify'>surfify</a></li>
                            
                            <li>API</li>
                            <li><a href="../../../../generated/nidl.html">nidl</a></li><li><a href="../../../../generated/nidl.callbacks.html">nidl.callbacks</a></li><li><a href="../../../../generated/nidl.datasets.html">nidl.datasets</a></li><li><a href="../../../../generated/nidl.estimators.html">nidl.estimators</a></li><li><a href="../../../../generated/nidl.estimators.autoencoders.html">nidl.estimators.autoencoders</a></li><li><a href="../../../../generated/nidl.estimators.linear.html">nidl.estimators.linear</a></li><li><a href="../../../../generated/nidl.estimators.ssl.html">nidl.estimators.ssl</a></li><li><a href="../../../../generated/nidl.estimators.ssl.utils.html">nidl.estimators.ssl.utils</a></li><li><a href="../../../../generated/nidl.losses.html">nidl.losses</a></li><li><a href="../../../../generated/nidl.metrics.html">nidl.metrics</a></li><li><a href="../../../../generated/nidl.utils.html">nidl.utils</a></li><li><a href="../../../../generated/nidl.volume.html">nidl.volume</a></li><li><a href="../../../../generated/nidl.volume.backbones.html">nidl.volume.backbones</a></li><li><a href="../../../../generated/nidl.volume.transforms.html">nidl.volume.transforms</a></li><li><a href="../../../../generated/nidl.volume.transforms.augmentation.html">nidl.volume.transforms.augmentation</a></li><li><a href="../../../../generated/nidl.volume.transforms.augmentation.intensity.html">nidl.volume.transforms.augmentation.intensity</a></li><li><a href="../../../../generated/nidl.volume.transforms.augmentation.spatial.html">nidl.volume.transforms.augmentation.spatial</a></li><li><a href="../../../../generated/nidl.volume.transforms.preprocessing.html">nidl.volume.transforms.preprocessing</a></li><li><a href="../../../../generated/nidl.volume.transforms.preprocessing.intensity.html">nidl.volume.transforms.preprocessing.intensity</a></li><li><a href="../../../../generated/nidl.volume.transforms.preprocessing.spatial.html">nidl.volume.transforms.preprocessing.spatial</a></li><li><a href="../../../../generated/surfify.html">surfify</a></li><li><a href="../../../../generated/surfify.augmentation.html">surfify.augmentation</a></li><li><a href="../../../../generated/surfify.datasets.html">surfify.datasets</a></li><li><a href="../../../../generated/surfify.losses.html">surfify.losses</a></li><li><a href="../../../../generated/surfify.models.html">surfify.models</a></li><li><a href="../../../../generated/surfify.nn.html">surfify.nn</a></li><li><a href="../../../../generated/surfify.plotting.html">surfify.plotting</a></li><li><a href="../../../../generated/surfify.utils.html">surfify.utils</a></li>
						</ul>
					</li> -->
				</ul>
				
                <!-- go top page -->
				<!-- <div class="to-top">
					<a class="btn btn-primary btn-customized-3" href="#" role="button">
	                    <i class="fas fa-arrow-up"></i> Top
	                </a>
				</div> -->
			
                <!-- change color -->
				<!-- <div class="dark-light-buttons">
					<a class="btn btn-primary btn-customized-4 btn-customized-dark" href="#" role="button">Dark</a>
					<a class="btn btn-primary btn-customized-4 btn-customized-light" href="#" role="button">Light</a>
				</div> -->
			
			</nav>
			<!-- End sidebar -->
			
			<!-- Dark overlay -->
    		<div class="overlay"></div>

			<!-- Content -->
			<div class="content">
			
				<!-- open sidebar menu -->
				<a class="btn btn-primary btn-customized open-menu" href="#" role="button">
                    <i class="fas fa-align-left"></i> <span>Menu</span>
                </a>

		        <!-- Top content -->
		        <div class="top-content section-container" id="top-content">
			        <div class="container">
			            <div class="row">
                            <div class="col-md-3 section-5-box banner-logo">
                                <img alt="Logo" src="../../../../_static/nidl.png">
                            </div>
			                <div class="col-md-7 section-5-box">
			                	<h1 class="wow fadeIn">    <p>Deep learning for NeuroImaging in Python.</p></h1>
			                </div>
			            </div>
			        </div>
		        </div>
                    
                    <div class="document">
                        <h1>Source code for nidl.estimators.autoencoders.vae</h1><div class='divider-1 wow fadeInUp' style='margin-top: -20px;'><span></span></div><div class="highlight"><pre>
<span></span><span class="c1">##########################################################################</span>
<span class="c1"># NSAp - Copyright (C) CEA, 2025</span>
<span class="c1"># Distributed under the terms of the CeCILL-B license, as published by</span>
<span class="c1"># the CEA-CNRS-INRIA. Refer to the LICENSE file or to</span>
<span class="c1"># http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html</span>
<span class="c1"># for details.</span>
<span class="c1">##########################################################################</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">...losses</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetaVAELoss</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..base</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>


<div class="viewcode-block" id="VAE">
<a class="viewcode-back" href="../../../../generated/nidl.estimators.autoencoders.vae.VAE.html#nidl.estimators.autoencoders.VAE">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VAE</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; Variational Auto-Encoder (VAE) [1]_ [2]_.</span>

<span class="sd">    See Also: :class:`~nidl.lossess.beta_vae.BetaVAELoss`</span>

<span class="sd">    A VAE is a probabilistic generative model that learns a latent</span>
<span class="sd">    representation of input data and reconstructs it. It implements `fit` and</span>
<span class="sd">    `transform` methods to respectively train the model and obtain the latent</span>
<span class="sd">    embeddings.</span>

<span class="sd">    The VAE consists of three main components:</span>

<span class="sd">    - Encoder: maps input `x` to latent mean :math:`\mu` and log-variance</span>
<span class="sd">      :math:`\log \sigma^2`</span>
<span class="sd">    - Reparameterization trick: samples latent vector</span>
<span class="sd">      :math:`z \sim q(z | x) = \mathcal{N}(\mu, \sigma^2 I)`</span>
<span class="sd">    - Decoder: reconstructs input :math:`\hat{x}` from latent vector :math:`z`</span>

<span class="sd">    The model is trained by minimizing the sum of two components:</span>

<span class="sd">    - **Reconstruction loss**: Measures how well the decoder reconstructs the</span>
<span class="sd">      input.</span>

<span class="sd">        * For binary data: Binary Cross-Entropy (BCE) loss</span>
<span class="sd">        * For continuous data: Mean Squared Error (MSE) loss</span>

<span class="sd">        .. math::</span>
<span class="sd">            \mathcal{L}_{recon} = - \mathbb{E}_{q(z|x)} [ \log p(x|z) ]</span>
<span class="sd">    - **KL Divergence loss**: Encourages the latent distribution :math:`q(z|x)`</span>
<span class="sd">      to be close to the prior :math:`p(z) = \mathcal{N}(0, I)`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            \mathcal{L}_{KL} = D_{KL}(q(z|x) | p(z))</span>

<span class="sd">    The total loss is a weighted sum of these two components:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathcal{L}_{total} = \mathcal{L}_{recon} + \beta \mathcal{L}_{KL}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: class:`~torch.nn.Module`</span>
<span class="sd">        The encoder mapping input ``x`` to the representation space. The mean</span>
<span class="sd">        :math:`\mu` and log-variance  :math:`\log \sigma^2` layers</span>
<span class="sd">        are automatically added according to the ``latent_dim`` parameter.</span>
<span class="sd">    decoder: class:`~torch.nn.Module`</span>
<span class="sd">        The decoder backbone outputting :math:`p(x | z)` as a</span>
<span class="sd">        `torch.distributions` or a `torch.Tensor` representing the mean of a</span>
<span class="sd">        Normal (default) or Laplace distribution.</span>
<span class="sd">    encoder_out_dim: int</span>
<span class="sd">        The output size of the encoder.</span>
<span class="sd">    latent_dim: int</span>
<span class="sd">        The number of latent dimensions (which is the size of the mean and</span>
<span class="sd">        variance of the posterior distribution).</span>
<span class="sd">    beta: float, default=1.</span>
<span class="sd">        Scaling factor for Kullback-Leibler distance (beta-VAE).</span>
<span class="sd">    default_dist: str, default=&quot;normal&quot;</span>
<span class="sd">        Default decoder distribution. It defines the reconstruction loss</span>
<span class="sd">        (L2 for Normal, L1 for Laplace, cross-entropy for Bernoulli).</span>
<span class="sd">    stochastic_transform: bool, default=True</span>
<span class="sd">        If True (default), the transformed data are obtained by sampling</span>
<span class="sd">        according to the posterior distribution :math:`q(z | x)`.If False,</span>
<span class="sd">        the mean of the posterior distribution is returned.</span>
<span class="sd">    lr: float</span>
<span class="sd">        the learning rate.</span>
<span class="sd">    weight_decay: float</span>
<span class="sd">        the Adam optimizer weight decay parameter.</span>
<span class="sd">    random_state: int, default=None</span>
<span class="sd">        setting a seed for reproducibility.</span>
<span class="sd">    kwargs: dict</span>
<span class="sd">        trainer parameters.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: :class:`~torch.nn.Module`</span>
<span class="sd">        The encoder network.</span>
<span class="sd">    decoder: :class:`~torch.nn.Module`</span>
<span class="sd">        The decoder network.</span>
<span class="sd">    fc_mu: :class:`~torch.nn.Module`</span>
<span class="sd">        The linear layer mapping the encoder output to the mean :math:`\mu` of</span>
<span class="sd">        the posterior distribution.</span>
<span class="sd">    fc_logvar: :class:`~torch.nn.Module`</span>
<span class="sd">        The linear layer mapping the encoder output to the log-variance</span>
<span class="sd">        :math:`\log \sigma^2` of the posterior distribution.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Diederik P. Kingma, Max Welling, &quot;Auto-Encoding Variational Bayes&quot;,</span>
<span class="sd">       ICLR 2014.</span>
<span class="sd">    .. [2] Irina Higgins et al., &quot;beta-VAE: Learning Basic Visual Concepts with</span>
<span class="sd">       a Constrained Variational Framework&quot;, ICLR 2017.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">decoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">encoder_out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
            <span class="n">default_dist</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span>
            <span class="n">stochastic_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
            <span class="n">random_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">ignore</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">,</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;decoder&quot;</span><span class="p">],</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_out_dim</span> <span class="o">=</span> <span class="n">encoder_out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span> <span class="o">=</span> <span class="n">latent_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_transform</span> <span class="o">=</span> <span class="n">stochastic_transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">BetaVAELoss</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">default_dist</span><span class="o">=</span><span class="n">default_dist</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">)</span>

<div class="viewcode-block" id="VAE.forward">
<a class="viewcode-back" href="../../../../generated/nidl.estimators.autoencoders.vae.VAE.html#nidl.estimators.autoencoders.VAE.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Encode the input and sample from the posterior distribution q(z|x).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x: torch.Tensor</span>
<span class="sd">            Input data given to the encoder.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        z: torch.Tensor, shape (batch_size, latent_dim)</span>
<span class="sd">            Latent vector sampled from the posterior distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">log_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span></div>


<div class="viewcode-block" id="VAE.training_step">
<a class="viewcode-back" href="../../../../generated/nidl.estimators.autoencoders.vae.VAE.html#nidl.estimators.autoencoders.VAE.training_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dataloader_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Perform one training step and computes and logs training losses.</span>

<span class="sd">        Three losses are logged: the beta-VAE loss (&quot;loss&quot;), the reconstruction</span>
<span class="sd">        loss (&quot;rec_loss&quot;) and the KL divergence loss (&quot;kl_loss&quot;).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch: torch.Tensor</span>
<span class="sd">            The input data given to the encoder.</span>
<span class="sd">        batch_idx: int</span>
<span class="sd">            Ignored.</span>
<span class="sd">        dataloader_idx: Optional[int], default=0</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        losses: dict</span>
<span class="sd">            Dictionary with &quot;loss&quot;, &quot;rec_loss&quot;, &quot;kl_loss&quot; as keys.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span>
            <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;train/</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">losses</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
            <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">on_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">losses</span></div>


<div class="viewcode-block" id="VAE.validation_step">
<a class="viewcode-back" href="../../../../generated/nidl.estimators.autoencoders.vae.VAE.html#nidl.estimators.autoencoders.VAE.validation_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dataloader_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Perform one validation step and computes and logs validation</span>
<span class="sd">        losses.</span>

<span class="sd">        Three losses are logged: the beta-VAE loss (&quot;loss&quot;), the reconstruction</span>
<span class="sd">        loss (&quot;rec_loss&quot;) and the KL divergence loss (&quot;kl_loss&quot;).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch: torch.Tensor</span>
<span class="sd">            The input data given to the encoder.</span>
<span class="sd">        batch_idx: int</span>
<span class="sd">            Ignored.</span>
<span class="sd">        dataloader_idx: Optional[int], default=0</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        losses: dict</span>
<span class="sd">            Dictionary with &quot;loss&quot;, &quot;rec_loss&quot;, &quot;kl_loss&quot; as keys.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_step</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;val/</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">losses</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
        <span class="k">return</span> <span class="n">losses</span></div>


<div class="viewcode-block" id="VAE.transform_step">
<a class="viewcode-back" href="../../../../generated/nidl.estimators.autoencoders.vae.VAE.html#nidl.estimators.autoencoders.VAE.transform_step">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">transform_step</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dataloader_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Transform the input data to the latent space.</span>

<span class="sd">        By default, the latent vector is obtained by sampling according to the</span>
<span class="sd">        posterior distribution :math:`q(z | x)`. It is just the mean of the</span>
<span class="sd">        distribution if ``stochastic_transform`` is False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch: torch.Tensor</span>
<span class="sd">            The input data given to the encoder.</span>
<span class="sd">        batch_idx: int</span>
<span class="sd">            Ignored.</span>
<span class="sd">        dataloader_idx: Optional[int], default=0</span>
<span class="sd">            Ignored.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        z: torch.Tensor, shape (batch_size, latent_dim)</span>
<span class="sd">            The latent vector.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">log_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_transform</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">z</span>
        <span class="k">return</span> <span class="n">mu</span></div>


<div class="viewcode-block" id="VAE.configure_optimizers">
<a class="viewcode-back" href="../../../../generated/nidl.estimators.autoencoders.vae.VAE.html#nidl.estimators.autoencoders.VAE.configure_optimizers">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Declare an :class:`~torch.optim.AdamW` optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">]</span></div>


<div class="viewcode-block" id="VAE.sample">
<a class="viewcode-back" href="../../../../generated/nidl.estimators.autoencoders.vae.VAE.html#nidl.estimators.autoencoders.VAE.sample">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate `n_samples` by sampling from the latent space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        nsamples: int</span>
<span class="sd">            Number of samples to generate.</span>

<span class="sd">        Returns</span>
<span class="sd">        ----------</span>
<span class="sd">        x: torch.Tensor</span>
<span class="sd">            Generated samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_run_step</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Encode the input and sample from the posterior distribution q(z|x).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x: torch.Tensor</span>
<span class="sd">            Input data given to the encoder.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z: torch.Tensor, shape (batch_size, latent_dim)</span>
<span class="sd">            Latent vector sampled from the posterior distribution.</span>
<span class="sd">        q: torch.distributions</span>
<span class="sd">            Probabilistic encoder.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">log_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">q</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">log_var</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Reparameterization trick: samples latent vector.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">z</span></div>

</pre></div>
                    </div>
                <div class="spacer"></div>
		        
		        <!-- Footer -->
		        <div class="section-6-container section-container section-container-image-bg" id="section-6">
			        <div class="container">
			            <div class="row">
		                    <div class="col-md-5 offset-md-1 section-6-box wow fadeInDown">
                                <div class="section-6-title">
		                    	    <p>Follow us</p>
                                </div>
		                    	<div class="section-6-social">
			                    	<a href="https://www.facebook.com/pages/NeuroSpin/171075046414933"><i class="fab fa-facebook-f"></i></a>
									<a href="https://www.youtube.com/CEASaclay"><i class="fab fa-youtube"></i></a>
									<a href="https://twitter.com/neurospin_91"><i class="fab fa-twitter"></i></a>
									<a href="https://gaia.neurospin.fr"><i class="fa fa-link"></i></a>
                                    <p>&copy; 2025, 
nidl developers
 <antoine.grigis@cea.fr></p>
		                    	</div>
		                    </div>
			            </div>
			        </div>
                </div>
	        
	        </div>
	        <!-- End content -->
        
        </div>
        <!-- End wrapper -->

        <!-- Javascript -->
		<script src="../../../../_static/js/jquery-3.3.1.min.js"></script>
		<script src="../../../../_static/js/jquery-migrate-3.0.0.min.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
		<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <script src="../../../../_static/js/jquery.backstretch.min.js"></script>
        <script src="../../../../_static/js/wow.min.js"></script>
        <script src="../../../../_static/js/jquery.waypoints.min.js"></script>
        <script src="../../../../_static/js/jquery.mCustomScrollbar.concat.min.js"></script>
        <script src="../../../../_static/js/scripts.js"></script>
        <script src="../../../../_static/js/jquery.mosaic.js"></script>
        <script src="../../../../_static/js/search.js"></script>
        <script type="text/javascript">
	        $('.top-content').backstretch("../../../../_static/img/backgrounds/banner1.png");
            $('.section-6-container').backstretch("../../../../_static/img/backgrounds/footer1.png");
        </script>

    </body>

</html>